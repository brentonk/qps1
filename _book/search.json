[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Political Science I",
    "section": "",
    "text": "Preface\nThese are the course notes for Prof. Brenton Kenkel’s course PSCI 2300 (Quantitative Political Science I: Computing) at Vanderbilt University.\nThese notes are written in Quarto and published on GitHub Pages. You can find the Quarto source files on GitHub.\nAcknowledgments: I’ve taken a lot from lecture notes written by my colleague Josh Clinton for prior iterations of PSCI 2300. My colleague Jim Bisbee’s notes for his offering of DS 1000 have been helpful too. I’m also grateful to students from the fall 2024 offering of PSCI 2300 for their suggestions, and to Nguyen Ha who gave lots of helpful feedback in her capacity as the TA.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro_r.html",
    "href": "intro_r.html",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1 Installing the software for this course\nThis first lecture is designed to take you from zero to the first step in data analysis — being able to load a dataset into R and look at it. The most important concepts and skills we’ll hit are the following:\nThe notes for this course are written using Quarto. For this lecture and future ones, I encourage you to download the Quarto source code from https://github.com/brentonk/qps1. You can even take your own notes in your copy of the relevant Quarto file (e.g., for today, intro_r.qmd).\nWe won’t go through this part in class, as I’m assuming you’ve already followed all of these steps. Be sure to contact me (brenton.kenkel@gmail.com) or the TA (fall 2025: Mason Auten, mason.auten@vanderbilt.edu) ASAP if you have trouble installing any of these pieces of software — all of them are critical for you to be able to complete the work in PSCI 2300.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "intro_r.html#installing-the-software-for-this-course",
    "href": "intro_r.html#installing-the-software-for-this-course",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1.1 R\nR is a programming language that originated in the 1990s and became popular among political scientists in the 2000s. While you could theoretically use R for a variety of programming tasks, in practice it is almost exclusively used for statistical analysis and data science, as it was developed specifically for these tasks.\nTo install R, just download the program from https://cran.r-project.org.\n\n\n1.1.2 RStudio\nThe R that you installed in the last step just comes with very basic utilities for running R and writing R code. If you’re not used to programming, these programs will probably not be very helpful for you.\nRStudio is a separate program that runs R in a more friendly and useful environment. It is called an IDE, which stands for “integrated development environment”, a fancy way of saying that it runs R and gives you a space to write R code while providing tools to check your work and make the process easier.\nTo install RStudio, just download the program from https://posit.co/download/rstudio-desktop/.\n\n\n1.1.3 tidyverse package\nR comes with some built-in tools for data management and analysis, but R users typically use packages that extend R’s default capabilities.\nYou will end up installing and using a number of packages during PSCI 2300, but there are two that are absolutely essential to have from the start.\n\ntidyverse, which provides a huge set of helpful functions for data management and visualization. We’ll talk about the tidyverse tools and use them virtually every day in this class.\ntinytex, a background utility that is needed to compile your assignments to PDF. You’ll install this once, and — hopefully — never have to think about it again.\n\n\n\nBrightspace, the terrible course management software that Vanderbilt makes us use, doesn’t allow annotations on HTML assignment submissions. By default, Quarto files can only be rendered as HTML, hence we need the extension to allow PDF output.\nTo install these packages, open up RStudio on your computer. Figure 1.1 shows what the program will probably look like when you first open it.\n\n\n\nFigure 1.1: What RStudio looks like when you first open it.\n\n\n\n\n\n\n\n\n\n\n\n\nRearranging the RStudio panels\n\n\n\n\n\nI’m picky about my code setup. When you see me on RStudio in class, you might notice that I’m working with a different layout than the default illustrated in Figure 1.1. I prefer to have my source code on the left and any output — calculations that show up in the R console, plots that show up in the plotting window, or rendered documents that show up in the preview window — on the right.\nWhen I’m coding in RStudio, my setup looks like this:\n\n\nProfessor Kenkel’s standard RStudio setup.\n\n\n\nYou can change the RStudio layout to suit your own needs by going to the Tools menu, selecting Global Options, and then selecting Pane Layout.\n\n\nSetting the RStudio pane layout.\n\n\n\n\n\n\nThe console is the window on the left labeled with the version of R you’re running (4.5.1 in my case). You’ll see that the console contains a prompt, which looks like:\n&gt;\nWhen you type a command into the prompt and hit enter, R will run your command and display the results. The simplest type of command is to just use R as a calculator.\n&gt; 3.14 * 7^2\n[1] 153.86\nTo install new packages, use the install.packages() command. For example, to install the tidyverse package, you’ll run install.packages(\"tidyverse\"). After you hit enter, R will show you information about the download and installation process.\n&gt; install.packages(\"tidyverse\")\nInstalling package into ‘/home/brenton/R/x86_64-pc-linux-gnu-library/4.5’\n(as ‘lib’ is unspecified)\ntrying URL 'https://cloud.r-project.org/src/contrib/tidyverse_2.0.0.tar.gz'\n\n[... omitting a bunch more output here ...]\n\n* DONE (tidyverse)\n\n\n\n\n\n\nEven small typos lead to failure\n\n\n\nYou need to run R commands exactly as they’re written. For example, what if you forgot the quotation marks and ran install.packages(tidyverse)?\n&gt; install.packages(tidyverse)\nError: object 'tidyverse' not found\nWhen you run this, R thinks that you’re asking it to look for a variable named tidyverse that stores the names of the package or packages you want to install. When it sees that you didn’t actually create a variable named tidyverse, it stops with an error.\nThis seems stupid — shouldn’t it infer that you wanted it to install the package named tidyverse? Indeed, R is stupid in this way, doing no more or no less than you ask it to, and failing whenever you make any kind of minor typo or error. The problem is that a “smarter” program would also be a more unpredictable one, and we want our programming languages to operate predictably.\n\n\nOnce you’ve installed a package, you don’t need to install it again unless you upgrade the version of R on your computer. To use a package that you’ve installed, call it with the library() command:\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n1.1.4 tinytex package\nThe tinytex installation requires two steps. This is truly specific to tinytex — as far as I know, none of the other packages we use in this class will require this second step.\nTo install tinytex, run these two commands in the R console:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nThe first command installs the tinytex R package. The second command downloads supplemental files used for PDF rendering and installs them to your computer. After running these two commands, you should never need to directly interface with tinytex again — Quarto will handle that for you.\n\n\n1.1.5 Quarto\nIf you wanted to code like a pure programmer, you would work in scripts that contain R code and nothing else. These would be plain text files with a .r or .R extension.\nBut we’re not trying to become pure programmers in PSCI 2300. The real goal is to learn how to analyze data to answer questions about politics. For that purpose, it makes more sense to work in Quarto files, which typically have a .qmd extension.\nA Quarto file typically contains both regular language and code. RStudio makes it easy to check the code you’re writing as you go along, to make sure it’s doing what you expect and that there aren’t any errors. You will ultimately compile, or “render”, the Quarto file into a document much like this one. The document will not only display the code you wrote, but also run that code and show the output. This makes Quarto a great tool to communicate the results of data analyses.\nTo install Quarto, just download the program from https://quarto.org/docs/get-started/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "intro_r.html#sec-working-directories",
    "href": "intro_r.html#sec-working-directories",
    "title": "1  Introduction to R",
    "section": "1.2 Working with working directories",
    "text": "1.2 Working with working directories\nOne of the conveniences of contemporary computing is that you usually don’t have to think about the details of where or how files are stored on your computer. You can just open up File Explorer or Finder, and it’ll show you everything you recently accessed or downloaded. And if what you’re looking for isn’t there, you can run a search and it’ll come up quick enough.\nR, like any programming language, is powerful — but that power comes at a cost. You have to be very explicit with R about everything, including where to look for files. R will do exactly what you ask, no more, no less.\nBy default, R looks for files in the working directory. You can run the getwd() command (as in: get the working directory) in the R console to see what your current working directory is.\n\ngetwd()\n\n[1] \"/home/brenton/Dropbox/courses/qps1/notes\"\n\n\nIf you’re on Windows, the default working directory is probably C:/Users/username/Documents. On a Mac, it’s probably /Users/username.\nYou can use the setwd() command in the R console to change your working directory. Alternatively, it’s probably easier to use the RStudio file browser to set the working directory, especially if you’re not a middle-aged MS-DOS veteran who has their computer’s directory hierarchy memorized.\n\n\n\nFigure 1.2: Setting the working directory in RStudio.\n\n\n\n\n\n\nSometimes you’ll see people use setwd() to set the working directory within their R scripts or Quarto files. Don’t do this. Everyone’s computer is different, with a different directory structure. So if someone else — say, the TA or professor who is grading your problem set — downloads your file and tries to run the code, it’ll likely stop with an error when setwd() looks for a directory that doesn’t exist.\nFor this class, I would recommend creating a psci2300 directory somewhere easy to access, then create new directories inside of there for each set of lecture notes and each problem set. What’s most important, though, is that you keep the files for this course somewhere that you can remember, and that you set R’s working directory accordingly.\ntl;dr:\n\nPut your files in a directory where you can find them.\nsetwd() to that directory when doing data analysis in R.\nIf R isn’t finding the files you need, use getwd() to see where it’s looking for them. Then either put the files there, or setwd() to wherever they are.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "intro_r.html#sec-quarto",
    "href": "intro_r.html#sec-quarto",
    "title": "1  Introduction to R",
    "section": "1.3 Writing documents in Quarto",
    "text": "1.3 Writing documents in Quarto\nWe will do all of our work within Quarto files. A Quarto file is a plain text file: if you open it in a bare-bones editor like Notepad (Windows) or TextEdit (Mac), you will see exactly the same content that you see in RStudio, except without the pretty colors used for syntax highlighting.\nThere are three components to a Quarto file:\n\nThe header at the top, containing metadata about the document like the title and author. It also contains instructions for the Quarto engine on how to render it.\nText written in Markdown format. This contains the, like, words that you would use to say, um, stuff.\nCode chunks containing R code. When you render the Quarto document, it runs these code chunks in R. The output is then displayed in the document, like how above in the “Working with working directories” section it showed the output of getwd() when I compiled these notes on my computer.\n\nWe will only cover the bare bones of writing files in Quarto. For truly comprehensive information, see the documentation on Quarto’s website.\n\n1.3.1 The header\n\nThe very first thing in a Quarto file is a header. A typical header might look something like this:\n---\ntitle: \"Introduction to R\"\nauthor: \"Brenton Kenkel\"\ndate: 2025-08-26\nformat:\n  pdf:\n    fontsize: 12pt\n    highlight-style: tango\n---\nThe title line specifies the title of the document. Everything inside the quotation marks is considered the title; the quotation marks themselves are not treated as part of the title. If you needed to include quotation marks in your title for some reason, you would “escape” them with a backslash like this:\ntitle: \"Yeah, I \\\"Needed\\\" Quotation Marks in the Title\"\nThe author block specifies the author. It works just like the title block.\nThe date block specifies the date. Programming languages like R typically best manage dates in YYYY-MM-DD format, so that’s how I write the date. But you can just write it however you want it to show up in the output document, being careful to use quotation marks if there are commas or colons or the like.\ndate: \"The 21st day of August, Year of the Depend Adult Undergarment\"\nThe format block specifies the default format that the document should be rendered as. The most common formats are pdf, which you’ll use for your problem sets in this class, and html. The fontsize and highlight-style options within the pdf block in the example header control the look of the PDF that’s rendered. There are tons more options documented on the Quarto website. But don’t worry too much about having to work through these — for the problem sets, I’ll distribute a template with a premade header that you just have to plug your name into.\n\n\n1.3.2 Ordinary text in Markdown format\nYou write a Quarto document in plain text. There’s some special syntax you can use to mark up that text with formatting (bold, italics), to include headings and hyperlinks, place images, and so on. One of the best ways to learn the syntax is to just download the .qmd source for these lecture notes and compare to the output. And here’s a quick guide to the most useful pieces of syntax:\n# Big heading\n\n## Medium heading\n\n### Small heading\n\nYou can make things *italic* or **bold**.  You can even put things in `code font`.\n\n*   this is\n*   a bulleted\n*   list\n\n&lt;!--\nThis is a comment!\nIt won't show up in the rendered output!\n--&gt;\n\nYou can include [hyperlinks](https://vanderbilt.edu).\n\n1.  this is\n2.  a numbered\n3.  list\n\nFinally, you can include images:\n\n![](https://i.redd.it/tospo6k2u9l81.png)\n\n\n1.3.3 Code blocks\nThe special thing about Quarto is that you can include blocks of R code. (Or Python, if you’re into snakes.) When you render the document, the code you put into each block will be run in R, and the output will be included in the document.\nAs a quick example, we can have R do some basic arithmetic for us.\n\n7 * 6\n\n[1] 42\n\n\nTo create the above, all I entered in my Quarto source file was the following:\n```{r}\n7 * 6\n```\n\nThe triple backticks tell Quarto: “Everything after this is code, until the next set of triple backticks.” The {r} then tells Quarto: “Not only is this code, but this is R code that you should run.”\nIf you don’t want to remember this goofy syntax yourself, you can run Code -&gt; Insert Chunk from the RStudio menu. Or even better, for the efficiency aficionados, hit Ctrl-Alt-I (Windows) or Command-Option-I (Mac).\nYou usually won’t need to use anything other than the default code chunk. But sometimes you will want to customize. For example, as we saw when running library(\"tidyverse\") at the end of Section 1.1.3, loading the tidyverse package spits out a bunch of annoying messages that I usually don’t want cluttering up my Quarto output. To keep these out, I can use the message: false option for the code block where I load tidyverse:\n```{r}\n#| message: false\nlibrary(\"tidyverse\")\n```\nOr if I were creating a figure, I might use the fig-cap option to add a caption to it:\n```{r}\n#| fig-cap: \"Popularity of polar bears over time\"\ndf_polar_bears |&gt;\n  ggplot(aes(x = year, y = popularity)) +\n  geom_line()\n```\nSee the Quarto documentation for the comprehensive list of chunk options. FYI, I have used Quarto and its predecessor R Markdown for a decade and have never used 90% of these, so don’t worry about trying to figure them all out.\n\n\n1.3.4 Rendering\nWhen you’re done writing your Quarto file and want to turn it into a PDF, hit the “Render” button in the RStudio interface. Assuming that there aren’t any errors in your R code chunks that prevent the document from compiling, you’ll see the PDF output in RStudio’s Preview window (or perhaps in an external browser or PDF viewer, depending on RStudio’s settings).\nBut you really shouldn’t wait until you’re all done to render. As you write, you should regularly take a look at the rendered output to make sure there are no errors and that it looks how you expect. When I’m writing in Quarto (including these notes), at a minimum I re-render and check the output every time I finish writing a paragraph of text or a full code block. Even more aggressively, you can check the “Render on Save” box to automatically render the document every time you save.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "intro_r.html#pulling-data-into-r",
    "href": "intro_r.html#pulling-data-into-r",
    "title": "1  Introduction to R",
    "section": "1.4 Pulling data into R",
    "text": "1.4 Pulling data into R\nWe are finally ready for the fun part — data!\nOnce you’ve opened RStudio, make sure you’ve loaded the tidyverse package if you haven’t done so already.\n\nlibrary(\"tidyverse\")  # tells R to put the tidyverse functions into its memory\nWe will use the tidyverse package basically every day in this class. R has built-in functions for loading and manipulating data, but these got old and creaky over time. The tidyverse provides a more modern and full-featured set of functions to load and work with data. You should probably just put library(\"tidyverse\") in an R code chunk at the start of all your Quarto documents so that you don’t need to worry about which functions come from the tidyverse and which ones are built into R.\n\n\n\n\n\n\nComments in R code\n\n\n\nThe hashtag character # begins a comment in R. Once you put # in a line of R code, the R console will ignore everything after it until the end of the line. So both of these lines of code are the same, as far as R is concerned:\n\n3 + 8\n\n[1] 11\n\n3 + 8 # + 1000 * 92 - infinity\n\n[1] 11\n\n\nComments are useful to leave notes to yourself, or to anyone else who might read your code, to explain what you’re doing or why you did it. They’re a bit less important in Quarto documents than in raw R scripts, since you can include full-text explanations in Quarto, but you’ll still see me use comments pretty frequently within code chunks too.\n\n\n\n1.4.1 Reading a CSV\nThe data file we’ll work with today is called turnout.csv. It’s stored on the website for these lecture notes, at https://bkenkel.com/qps1/data/turnout.csv.\nCSV stands for comma-separated values. It is the most common way that political scientists distribute datasets, and all of the datasets I give you in this course will be in CSV format. The first row contains the names of the variables, each separated by a comma. Each subsequent row contains an observation, with the value of each variable again separated by commas. If you open up turnout.csv in a plain text editor (like Notepad on Windows or TextEdit on a Mac), the first few lines look like this:\nyear,votes_counted,voting_age_pop,voting_eligible_pop,ineligible_felons,ineligible_noncitizens,eligible_overseas\n1980,86.515221,164.445475,159.690927,0.801977,5.755591625000001,1.803021\n1982,67.615576,166.027633,160.408786,0.959637,6.64110532,1.981895\n1984,92.65268,173.99461,167.708463,1.165246,7.481768229999999,2.360867\n1986,64.991128,177.92233,170.408916,1.367117,8.36234951,2.216053\n1988,91.594691,181.955484,173.609148,1.593976,9.279729683999998,2.52737\nThis data comes from the Election Lab at the University of Florida. They track important statistics about turnout in federal elections: how many people were old enough to vote, how many of those were actually eligible to vote (i.e., excluding noncitizens and felons), and how many ballots were actually cast. Here’s a guide to the variables that are included here:\n\n\nSee Appendix B for more detail on all data files used in these notes.\n\n\n\n\n\n\n\nVariable name\nDefinition\n\n\n\n\nyear\nElection year\n\n\nvotes_counted\nNumber of votes cast that election year (in millions)\n\n\nvoting_age_pop\nVoting-age population (in millions)\n\n\nvoting_eligible_pop\nVoting-eligible population (in millions)\n\n\nineligible_felons\nCitizens ineligible to vote due to felonies (in millions)\n\n\nineligible_noncitizens\nResidents ineligible to vote because not citizens (in millions)\n\n\neligible_overseas\nNumber of voting-eligible citizens living outside the US (in millions)\n\n\n\nWe need to load this into R as a data frame, which is kind of like the equivalent of a spreadsheet within R. To that end, we’ll plug the URL into the read_csv() function. (Make sure you type the URL exactly right — it’s probably easiest to just copy-paste the following line.)\n\ndf_turnout &lt;- read_csv(\"https://bkenkel.com/qps1/data/turnout.csv\")\n\nLet me break down that line for you, because there’s a lot going on here:\n\ndf_turnout is a variable name, which we will use to store and retrieve the data. The choice of variable name is totally up to you — but you’ll find yourself less confused and making fewer mistakes if you give your variables descriptive, memorable names. I usually name data frames df_topic, where df stands for “data frame” (you’ll create plenty of other variables that aren’t data frames) and where topic is whatever the data is about.\n&lt;- is the assignment operator. When we write varname &lt;- value, what we are telling R is: “Calculate the value on the right-hand side. Then stuff the result of that calculation into varname, so that when I type varname into the R console later, it’s treated as the value of the calculation.”\nread_csv(\"https://bkenkel.com/qps1/data/turnout.csv\") tells R to visit the URL, take the contents of the file there, and put the data from it into a data frame. Notice that the URL goes into quotation marks.\n\nWhat if we were to run read_csv(...) alone, without assigning to a variable name? R would read the data, put it in a data frame, print the first few rows of the data frame, and then promptly forget that the data was ever there. Since the goal is to analyze the data, we want to store it as a variable to perform further calculations.\n\n\n\n\n\n\nread_csv versus read.csv\n\n\n\nIn this class, I’ll always read data files using the read_csv() function. Sometimes you’ll see people using read.csv() instead, with a dot instead of an underscore. For the most part, read.csv() works the same way as read_csv(). However, I prefer read_csv() because it works faster on large datasets and has better default options.\n\n\n\n\n\n\n\n\nReading data from a file on your computer\n\n\n\nSometimes you can’t read data directly from a URL. Maybe you’re about to get on an airplane and won’t have wi-fi, or maybe someone emailed you a file you want to analyze. Either way, once you’ve downloaded the file to your computer, how do you get it into R?\nIn this situation, you need to be especially mindful of your working directory (see Section 1.2). You want R’s working directory to match the location of the downloaded file, which means doing one of two things:\n\nBefore downloading the file, run getwd() in R to see where its working directory is. When you download the file, make sure to save it to that directory.\n\ngetwd()\n\n[1] \"/home/brenton/Dropbox/courses/qps1/notes\"\n\n\nFor example, I would need to save the file to (deep breath) the notes directory inside the qps1 directory inside the courses directory in my Dropbox.\nAlternatively, after downloading the file, change the working directory in R to be wherever you put it. You can do this by giving the full directory path to setwd(), but it’s probably easier to use the file navigator in the RStudio interface, as illustrated in Figure 1.2.\n\nAfter following either of these methods, you can use the dir() command to look at the list of files that R “sees” in the current working directory.\n\ndir()\n\n [1] \"_book\"                   \"_freeze\"                \n [3] \"_quarto.yml\"             \"_raw\"                   \n [5] \"cover.png\"               \"data\"                   \n [7] \"data_sources_cache\"      \"data_sources.qmd\"       \n [9] \"data_visualization.qmd\"  \"data_wrangling.qmd\"     \n[11] \"img\"                     \"index.html\"             \n[13] \"index.qmd\"               \"intro_r.qmd\"            \n[15] \"intro_r.rmarkdown\"       \"qps1.scss\"              \n[17] \"references.bib\"          \"references.qmd\"         \n[19] \"site_libs\"               \"univariate_analysis.qmd\"\n[21] \"using_chatgpt.qmd\"      \n\n\nFinally, once you’ve confirmed that the data file you want to analyze shows up here, you can load it into R by plugging the filename into read_csv(). As always, the filename will need go inside quotation marks. For example, if the file is called pre_election_polls.csv, you would run:\ndf_polls &lt;- read_csv(\"pre_election_polls.csv\")\n\n\n\n\n1.4.2 Looking at the data\nIf in the bottom of your heart you long for the comfort of Microsoft Excel, you can get a spreadsheet-type view of the data using the View() command in RStudio:\nView(df_turnout)\nOr you can see info in the R console by typing the variable name into the prompt and hitting enter:\n\ndf_turnout\n\n# A tibble: 22 × 7\n    year votes_counted voting_age_pop voting_eligible_pop ineligible_felons\n   &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n 1  1980          86.5           164.                160.             0.802\n 2  1982          67.6           166.                160.             0.960\n 3  1984          92.7           174.                168.             1.17 \n 4  1986          65.0           178.                170.             1.37 \n 5  1988          91.6           182.                174.             1.59 \n 6  1990          67.9           186.                177.             1.90 \n 7  1992         104.            191.                180.             2.18 \n 8  1994          75.1           195.                183.             2.44 \n 9  1996          96.3           200.                186.             2.59 \n10  1998          74.8           205.                190.             2.92 \n# ℹ 12 more rows\n# ℹ 2 more variables: ineligible_noncitizens &lt;dbl&gt;, eligible_overseas &lt;dbl&gt;\n\n\n“tibble” is just tidyverse’s cutesy name for data frames. I will use “tibble” and “data frame” interchangeably, though I’ll usually say “data frame” because “tibble” sounds more like a name for a stray cat. The &lt;dbl&gt; under each variable name indicates the type of variable. &lt;dbl&gt; just means the variable is a number (it comes from “double precision”, which is a particular way numbers are stored in computers). The other most common categorization you’ll see is &lt;chr&gt;, for variables that are “characters” rather than numbers.\nUnless your data frame has unusually few columns, this view will wind up with some of them being cut off. To see the first few entries of every single column, use glimpse():\n\nglimpse(df_turnout)\n\nRows: 22\nColumns: 7\n$ year                   &lt;dbl&gt; 1980, 1982, 1984, 1986, 1988, 1990, 1992, 1994,…\n$ votes_counted          &lt;dbl&gt; 86.51522, 67.61558, 92.65268, 64.99113, 91.5946…\n$ voting_age_pop         &lt;dbl&gt; 164.4455, 166.0276, 173.9946, 177.9223, 181.955…\n$ voting_eligible_pop    &lt;dbl&gt; 159.6909, 160.4088, 167.7085, 170.4089, 173.609…\n$ ineligible_felons      &lt;dbl&gt; 0.801977, 0.959637, 1.165246, 1.367117, 1.59397…\n$ ineligible_noncitizens &lt;dbl&gt; 5.755592, 6.641105, 7.481768, 8.362350, 9.27973…\n$ eligible_overseas      &lt;dbl&gt; 1.803021, 1.981895, 2.360867, 2.216053, 2.52737…\n\n\nYou can extract an individual row from a data frame using square brackets:\n\ndf_turnout[3, ]\n\n# A tibble: 1 × 7\n   year votes_counted voting_age_pop voting_eligible_pop ineligible_felons\n  &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n1  1984          92.7           174.                168.              1.17\n# ℹ 2 more variables: ineligible_noncitizens &lt;dbl&gt;, eligible_overseas &lt;dbl&gt;\n\n\nTo retrieve a consecutive sequence of rows, use a colon:\n\ndf_turnout[3:6, ]\n\n# A tibble: 4 × 7\n   year votes_counted voting_age_pop voting_eligible_pop ineligible_felons\n  &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n1  1984          92.7           174.                168.              1.17\n2  1986          65.0           178.                170.              1.37\n3  1988          91.6           182.                174.              1.59\n4  1990          67.9           186.                177.              1.90\n# ℹ 2 more variables: ineligible_noncitizens &lt;dbl&gt;, eligible_overseas &lt;dbl&gt;\n\n\nYou can retrieve a column by numeric index within brackets…\n\ndf_turnout[, 4]\n\n# A tibble: 22 × 1\n   voting_eligible_pop\n                 &lt;dbl&gt;\n 1                160.\n 2                160.\n 3                168.\n 4                170.\n 5                174.\n 6                177.\n 7                180.\n 8                183.\n 9                186.\n10                190.\n# ℹ 12 more rows\n\n\n…or by name (quoted) within brackets…\n\ndf_turnout[, \"voting_eligible_pop\"]\n\n# A tibble: 22 × 1\n   voting_eligible_pop\n                 &lt;dbl&gt;\n 1                160.\n 2                160.\n 3                168.\n 4                170.\n 5                174.\n 6                177.\n 7                180.\n 8                183.\n 9                186.\n10                190.\n# ℹ 12 more rows\n\n\n…or, the option I most prefer, by name (not quoted) with a dollar sign.\n\ndf_turnout$voting_eligible_pop\n\n [1] 159.6909 160.4088 167.7085 170.4089 173.6091 176.6783 179.5657 182.5494\n [9] 186.3277 190.3424 194.2597 198.3647 202.9729 207.1481 213.3062 221.7765\n[17] 222.4375 227.1371 230.7808 236.9181 242.0778 242.9077\n\n\nWhat do you think the following code does?\ndf_turnout[, 4:6]\n\n\n\n\n\n\nSelecting a column from a data frame\n\n\n\nAs you can see in the preceding code blocks, when you select a column using brackets (whether by number or by name), R gives you a data frame with a single column. When you use the dollar sign, it gives you a vector — which you can think of as just a list of numbers, without the row-column structure of a data frame.\nIf you’re planning to do other calculations like taking a sum or average, it’s usually more convenient to work with vectors than with single-column data frames. Hence my preference for df_turnout$voting_eligible_pop over df_turnout[, \"voting_eligible_pop\"].\n\n\n\n\n1.4.3 Simple analysis\nWhat has been the average voter turnout in federal elections since 1986? What about the highest and lowest? How has it varied over time?\nTo answer these questions, first we need to actually calculate turnout as a percentage: \\[\\text{turnout \\%} = \\frac{\\text{votes cast}}{\\text{eligible voters}} \\times 100.\\] Let’s implement this formula in R, storing the result in a variable called turnout_pct.\n\nturnout_pct &lt;- df_turnout$votes_counted / df_turnout$voting_eligible_pop\nturnout_pct &lt;- 100 * turnout_pct\n\nThe first line exploits a very convenient feature of R. If we have two vectors x and y with the same number of entries, then x / y will divide the first element of x by the first of y, the second element of x with the second of y, and so on. The same logic applies to x + y, x - y, x * y, x^y (x to the y’th power), and many more basic mathematical operations.\nThe second line shows how we can modify a variable “in place”. I often break up complex calculations into multiple lines this way, just to make the code as easy to follow and read as possible.\nWe can now calculate the lowest, highest, average, and median turnout.\n\nmin(turnout_pct)\n\n[1] 36.61048\n\nmax(turnout_pct)\n\n[1] 65.98637\n\nmean(turnout_pct)\n\n[1] 49.5346\n\nmedian(turnout_pct)\n\n[1] 50.85756\n\n\nThe summary() command does all this at once, and more:\n\nsummary(turnout_pct)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  36.61   41.02   50.86   49.53   57.43   65.99 \n\n\nThe first quartile is the 25th percentile: turnout is lower than 41% in a quarter of elections, and greater than 41% in the other three quarters of the elections. The third quartile is the mirror image of this: turnout is lower than 57% in three-quarters of elections, and greater than 57% in the other quarter.\n\n\n1.4.4 Simple data visualization\nWe’ll use ggplot() for data visualization throughout this course. The syntax for it is kind of weird, so don’t worry if it’s not all clear at first. You’ll get used to it over time, and we’ll talk about it in much more detail in the coming weeks.\nThe first thing we need to do is put our turnout percentage calculation into our data frame as a column.\n\ndf_turnout$turnout_pct &lt;- turnout_pct\n\nglimpse(df_turnout)  # now includes an 8th column containing turnout\n\nRows: 22\nColumns: 8\n$ year                   &lt;dbl&gt; 1980, 1982, 1984, 1986, 1988, 1990, 1992, 1994,…\n$ votes_counted          &lt;dbl&gt; 86.51522, 67.61558, 92.65268, 64.99113, 91.5946…\n$ voting_age_pop         &lt;dbl&gt; 164.4455, 166.0276, 173.9946, 177.9223, 181.955…\n$ voting_eligible_pop    &lt;dbl&gt; 159.6909, 160.4088, 167.7085, 170.4089, 173.609…\n$ ineligible_felons      &lt;dbl&gt; 0.801977, 0.959637, 1.165246, 1.367117, 1.59397…\n$ ineligible_noncitizens &lt;dbl&gt; 5.755592, 6.641105, 7.481768, 8.362350, 9.27973…\n$ eligible_overseas      &lt;dbl&gt; 1.803021, 1.981895, 2.360867, 2.216053, 2.52737…\n$ turnout_pct            &lt;dbl&gt; 54.17667, 42.15204, 55.24628, 38.13834, 52.7591…\n\n\nNow let’s make a very rough graph of how turnout has changed over time. Quarto will helpfully display the graph in the output once we render it!\n\nggplot(df_turnout, aes(x = year, y = turnout_pct)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWhat stands out to me about this graph is the cyclical pattern, with turnout typically being 10–15 percentage points lower in midterm years than in presidential election years.\n\n\n\n\n\n\n‘Percent’ versus ‘percentage point’\n\n\n\nVoter turnout was about 65% in 2020 and about 50% in 2018. So which of the following sentences is true, and which is false?\n\n“Turnout in 2018 was 15 percent lower than turnout in 2020.”\n“Turnout in 2018 was 15 percentage points lower than turnout in 2020.”\n\nYou might be surprised to learn that these sentences don’t mean the same thing. In fact, #2 is true, but #1 is false. If you found yourself surprised here, take a look at this very short article on the difference between percent and percentage point.\nHere’s my heuristic for deciding whether I’m calculating a “percent” or a “percentage point” change:\n\nIf I’m using division to make the comparison, then I’m calculating a percent. For example, because \\[\\frac{50}{65} \\approx 0.77 = 77\\%,\\] I would say that turnout in 2018 was about 23 percent lower than in 2020.\nIf I’m using subtraction to make the comparison, then I’m calculating a percentage point. For example, because \\(65 - 50 = 15\\), I would say that turnout in 2018 was about 15 percentage points lower than in 2020.\n\nIf you’re uncertain whether to report a percent difference or a percentage point difference, default to using the percentage point difference — it’s less confusing, especially to audiences that might not be finely attuned to the distinction. For example, imagine I said that jaywalking on 21st Ave raises your chance of being hit by a car by 50 percent. An inattentive or uninformed listener might incorrectly take that to mean that at least half of jaywalkers are getting hit. But if it’s actually just 2 out of every 10,000 non-jaywalkers who are getting hit, my statement would only mean that the rate is 3 out of 10,000 for jaywalkers.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html",
    "href": "data_wrangling.html",
    "title": "2  Data Wrangling",
    "section": "",
    "text": "2.1 Data: International crises\nIn Chapter 1, we saw how to get data into R and the basics of writing code in Quarto. Now we’re going to dig further into “wrangling” raw data into a form that’s useful for analysis.\nIn both academic research and industry data science, the hard part of data analysis usually isn’t the statistical modeling — it’s to acquire the data in the first place, and then to get it into a state that’s clean enough to plug into whatever statistical model you plan to use.\nThe main data wrangling skills you’re going to pick up here are:\nWe will work with data from the International Crisis Behavior Project on countries involved in international crises. The project’s operational definition of an international crisis is a situation in which there is\nTo put it more simply, countries are in a state of crisis when something happens that raises tensions between them to the point that there is a legitimate fear of war. Every war starts as a crisis, but not all crises lead to war (and in fact the vast majority do not).\nWe will work with the ICB Project’s “actor level” dataset, in which each observation (row) is a crisis participant. For example, in the Cuban Missile Crisis of 1962 (crisis #196 in the ICB data), the United States, Cuba, and the Soviet Union are each separate crisis participants. The data is on my website at https://bkenkel.com/qps1/data/crises.csv.\nlibrary(\"tidyverse\")\ndf_crises &lt;- read_csv(\"https://bkenkel.com/qps1/data/crises.csv\")\n\ndf_crises\n\n# A tibble: 1,131 × 95\n   icb2  crisno cracno cracid actor systrgyr systrgmo systrgda crisname   triggr\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 ICB2       1      1    365 RUS       1918        5       NA RUSSIAN C…      9\n 2 ICB2       2      2     93 NIC       1918        5       25 COSTA RIC…      7\n 3 ICB2       2      3     94 COS       1918        5       25 COSTA RIC…      4\n 4 ICB2       3      4    365 RUS       1918        6       23 RUSSIAN C…      7\n 5 ICB2       4      5    365 RUS       1918       11       18 BALTIC IN…      6\n 6 ICB2       4      6    366 EST       1918       11       18 BALTIC IN…      9\n 7 ICB2       4      7    368 LIT       1918       11       18 BALTIC IN…      9\n 8 ICB2       4      8    367 LAT       1918       11       18 BALTIC IN…      9\n 9 ICB2       5      9    315 CZE       1919        1       15 TESCHEN         2\n10 ICB2       5     10    290 POL       1919        1       15 TESCHEN         7\n# ℹ 1,121 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\nAs you can see, there are a ton of variables here. The ICB Project provides a full codebook describing each variable in the dataset. The most important variables you need to know from the start are:\nSo for example, the first crisis in the data (crisno = 1) is the Russian Civil War, which only involved Russia. The second (crisno = 2) is a 1918 crisis between Nicaragua and Costa Rica, each of which gets their own row in the data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html#data-international-crises",
    "href": "data_wrangling.html#data-international-crises",
    "title": "2  Data Wrangling",
    "section": "",
    "text": "… (1) a change in type and/or an increase in intensity of disruptive, that is, hostile verbal or physical interactions between two or more states, with a heightened probability of military hostilities; that, in turn, (2) destabilizes their relationship and challenges the structure of an international system—global, dominant, or subsystem.\n\n\n\nQuotation from Michael Brecher and Jonathan Wilkenfeld, A Study of Crisis, pages 4–5.\n\n\n\n\n\ncrisname and crisno: The name of the crisis and a numeric identifier assigned to each separate crisis.\nactor and cracid: A country involved in the crisis and a numeric identifier assigned to each different country (e.g., USA = 2, Russia/Soviet Union = 365).\nsystrgyr: The year the crisis began.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html#subsetting",
    "href": "data_wrangling.html#subsetting",
    "title": "2  Data Wrangling",
    "section": "2.2 Subsetting",
    "text": "2.2 Subsetting\n\n2.2.1 Subsetting by column: select()\nThe select() function lets us reduce the data frame just to have the columns we care about, or even just reorder the columns if you’re not trying to get rid of any data but want things to be arranged differently.\n\n2.2.1.1 Basic usage\nThe simplest way to use select() is to tell it what data frame you’re working with, then list the names of the columns you want to keep from it.\n\nselect(df_crises, crisname, actor, systrgyr)\n\n# A tibble: 1,131 × 3\n   crisname             actor systrgyr\n   &lt;chr&gt;                &lt;chr&gt;    &lt;dbl&gt;\n 1 RUSSIAN CIVIL WAR I  RUS       1918\n 2 COSTA RICAN COUP     NIC       1918\n 3 COSTA RICAN COUP     COS       1918\n 4 RUSSIAN CIVIL WAR II RUS       1918\n 5 BALTIC INDEPENDENCE  RUS       1918\n 6 BALTIC INDEPENDENCE  EST       1918\n 7 BALTIC INDEPENDENCE  LIT       1918\n 8 BALTIC INDEPENDENCE  LAT       1918\n 9 TESCHEN              CZE       1919\n10 TESCHEN              POL       1919\n# ℹ 1,121 more rows\n\n\nAn important thing to know about how R works: Just running a function like this won’t change your data frame permanently. To see this, after running select() like the above, go back and look at the data frame — nothing has changed:\n\ndf_crises\n\n# A tibble: 1,131 × 95\n   icb2  crisno cracno cracid actor systrgyr systrgmo systrgda crisname   triggr\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 ICB2       1      1    365 RUS       1918        5       NA RUSSIAN C…      9\n 2 ICB2       2      2     93 NIC       1918        5       25 COSTA RIC…      7\n 3 ICB2       2      3     94 COS       1918        5       25 COSTA RIC…      4\n 4 ICB2       3      4    365 RUS       1918        6       23 RUSSIAN C…      7\n 5 ICB2       4      5    365 RUS       1918       11       18 BALTIC IN…      6\n 6 ICB2       4      6    366 EST       1918       11       18 BALTIC IN…      9\n 7 ICB2       4      7    368 LIT       1918       11       18 BALTIC IN…      9\n 8 ICB2       4      8    367 LAT       1918       11       18 BALTIC IN…      9\n 9 ICB2       5      9    315 CZE       1919        1       15 TESCHEN         2\n10 ICB2       5     10    290 POL       1919        1       15 TESCHEN         7\n# ℹ 1,121 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n\nAll the original data is still there. Unlike some other programming languages, R essentially never modifies data “in place”. If you want to overwrite df_crises with the subsetted version, you need to explicitly tell R to overwrite it by assigning the select()’ed data frame to df_crises, as in:\n# I don't want to overwrite df_crises, so not actually running this\ndf_crises &lt;- select(df_crises, crisname, actor, systrgyr)\nJust as a matter of workflow, I usually try not to overwrite the raw data frame that I’ve read into R. That way, if I decide later on that I want more out of the original data, I don’t have to go back and load from the CSV file and clean it up again. So in this case, I would typically just create a new data frame with the subsetted data:\n\ndf_crises_reduced &lt;- select(df_crises, crisname, actor, systrgyr)\n\ndf_crises_reduced\n\n# A tibble: 1,131 × 3\n   crisname             actor systrgyr\n   &lt;chr&gt;                &lt;chr&gt;    &lt;dbl&gt;\n 1 RUSSIAN CIVIL WAR I  RUS       1918\n 2 COSTA RICAN COUP     NIC       1918\n 3 COSTA RICAN COUP     COS       1918\n 4 RUSSIAN CIVIL WAR II RUS       1918\n 5 BALTIC INDEPENDENCE  RUS       1918\n 6 BALTIC INDEPENDENCE  EST       1918\n 7 BALTIC INDEPENDENCE  LIT       1918\n 8 BALTIC INDEPENDENCE  LAT       1918\n 9 TESCHEN              CZE       1919\n10 TESCHEN              POL       1919\n# ℹ 1,121 more rows\n\n\n\n\n2.2.1.2 Selecting by range or pattern\nSuppose you wanted everything from the crisno column through the systrgyr column. Instead of typing out each column name explicitly, you can just use a colon (:) to tell select() to grab this range of variables:\n\nselect(df_crises, crisno:systrgyr)\n\n# A tibble: 1,131 × 5\n   crisno cracno cracid actor systrgyr\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1      1      1    365 RUS       1918\n 2      2      2     93 NIC       1918\n 3      2      3     94 COS       1918\n 4      3      4    365 RUS       1918\n 5      4      5    365 RUS       1918\n 6      4      6    366 EST       1918\n 7      4      7    368 LIT       1918\n 8      4      8    367 LAT       1918\n 9      5      9    315 CZE       1919\n10      5     10    290 POL       1919\n# ℹ 1,121 more rows\n\n\nIf you want everything except a particular variable, you can use the minus sign (-) to exclude it. Let’s see how to get rid of the useless icb2 column that just marks the dataset version:\n\nselect(df_crises, -icb2)\n\n# A tibble: 1,131 × 94\n   crisno cracno cracid actor systrgyr systrgmo systrgda crisname  triggr yrtrig\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1      1      1    365 RUS       1918        5       NA RUSSIAN …      9   1918\n 2      2      2     93 NIC       1918        5       25 COSTA RI…      7   1918\n 3      2      3     94 COS       1918        5       25 COSTA RI…      4   1919\n 4      3      4    365 RUS       1918        6       23 RUSSIAN …      7   1918\n 5      4      5    365 RUS       1918       11       18 BALTIC I…      6   1918\n 6      4      6    366 EST       1918       11       18 BALTIC I…      9   1918\n 7      4      7    368 LIT       1918       11       18 BALTIC I…      9   1918\n 8      4      8    367 LAT       1918       11       18 BALTIC I…      9   1918\n 9      5      9    315 CZE       1919        1       15 TESCHEN        2   1919\n10      5     10    290 POL       1919        1       15 TESCHEN        7   1919\n# ℹ 1,121 more rows\n# ℹ 84 more variables: motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nIn-class exercise\n\n\n\nWhat if you wanted to exclude the range of variables from actor through yrtrig? How would you do that without typing out each column name explicitly?\n\n#\n# [Write your answer here]\n#\n\n\n\nYou can use the starts_with() subfunction to select all of the columns whose name starts with “sys”, for example:\n\nselect(df_crises, starts_with(\"sys\"))\n\n# A tibble: 1,131 × 4\n   systrgyr systrgmo systrgda syslev\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1     1918        5       NA      2\n 2     1918        5       25      1\n 3     1918        5       25      1\n 4     1918        6       23      2\n 5     1918       11       18      2\n 6     1918       11       18      1\n 7     1918       11       18      1\n 8     1918       11       18      1\n 9     1919        1       15      1\n10     1919        1       15      1\n# ℹ 1,121 more rows\n\n\n\n\n\n\n\n\nIn-class exercise\n\n\n\nWhat happens if you instead run select(df_crises, starts_with(sys)) (without the quotation marks around sys)? Why doesn’t this work?\n\n[Write your answer here]\n\n\n\nThere are many other subfunctions you can use to select columns without naming them each explicitly. You can learn about them by looking at the help page for the select() function. To see the help page for any R function, just enter ?name_of_function at the R console, like the following:\n?select\n\n\n\n\n\n\nIn-class exercise\n\n\n\nSelect the columns whose name has \"fav\" anywhere in it. Look at the ?select help page to figure out the subfunction you need to do this.\n\n#\n# [Write your answer here]\n#\n\n\n\n\n\n2.2.1.3 Reordering and renaming\nAs you select variables, you can rename them by using the syntax newname = oldname:\n\nselect(df_crises, year = systrgyr, name = crisname)\n\n# A tibble: 1,131 × 2\n    year name                \n   &lt;dbl&gt; &lt;chr&gt;               \n 1  1918 RUSSIAN CIVIL WAR I \n 2  1918 COSTA RICAN COUP    \n 3  1918 COSTA RICAN COUP    \n 4  1918 RUSSIAN CIVIL WAR II\n 5  1918 BALTIC INDEPENDENCE \n 6  1918 BALTIC INDEPENDENCE \n 7  1918 BALTIC INDEPENDENCE \n 8  1918 BALTIC INDEPENDENCE \n 9  1919 TESCHEN             \n10  1919 TESCHEN             \n# ℹ 1,121 more rows\n\n\nTo rename individual columns without dropping or rearranging the other columns, use the rename() function instead of select().\n\nrename(df_crises, year = systrgyr, name = crisname)\n\n# A tibble: 1,131 × 95\n   icb2  crisno cracno cracid actor  year systrgmo systrgda name   triggr yrtrig\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 ICB2       1      1    365 RUS    1918        5       NA RUSSI…      9   1918\n 2 ICB2       2      2     93 NIC    1918        5       25 COSTA…      7   1918\n 3 ICB2       2      3     94 COS    1918        5       25 COSTA…      4   1919\n 4 ICB2       3      4    365 RUS    1918        6       23 RUSSI…      7   1918\n 5 ICB2       4      5    365 RUS    1918       11       18 BALTI…      6   1918\n 6 ICB2       4      6    366 EST    1918       11       18 BALTI…      9   1918\n 7 ICB2       4      7    368 LIT    1918       11       18 BALTI…      9   1918\n 8 ICB2       4      8    367 LAT    1918       11       18 BALTI…      9   1918\n 9 ICB2       5      9    315 CZE    1919        1       15 TESCH…      2   1919\n10 ICB2       5     10    290 POL    1919        1       15 TESCH…      7   1919\n# ℹ 1,121 more rows\n# ℹ 84 more variables: motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n\nIf you want certain columns to be placed first, use the relocate() function.\n\nrelocate(df_crises, crisname, actor)\n\n# A tibble: 1,131 × 95\n   crisname   actor icb2  crisno cracno cracid systrgyr systrgmo systrgda triggr\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 RUSSIAN C… RUS   ICB2       1      1    365     1918        5       NA      9\n 2 COSTA RIC… NIC   ICB2       2      2     93     1918        5       25      7\n 3 COSTA RIC… COS   ICB2       2      3     94     1918        5       25      4\n 4 RUSSIAN C… RUS   ICB2       3      4    365     1918        6       23      7\n 5 BALTIC IN… RUS   ICB2       4      5    365     1918       11       18      6\n 6 BALTIC IN… EST   ICB2       4      6    366     1918       11       18      9\n 7 BALTIC IN… LIT   ICB2       4      7    368     1918       11       18      9\n 8 BALTIC IN… LAT   ICB2       4      8    367     1918       11       18      9\n 9 TESCHEN    CZE   ICB2       5      9    315     1919        1       15      2\n10 TESCHEN    POL   ICB2       5     10    290     1919        1       15      7\n# ℹ 1,121 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n\nTo rearrange the rows of the data, you can use the arrange() function. By default, it sorts the data in ascending order of the column name(s) you feed it.\n\n# Arrange in ascending order of actor numeric ID\narrange(df_crises, cracid)\n\n# A tibble: 1,131 × 95\n   icb2  crisno cracno cracid actor systrgyr systrgmo systrgda crisname   triggr\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 ICB2      59    128      2 USA       1937       12       12 PANAY INC…      9\n 2 ICB2      88    224      2 USA       1941       11       26 PEARL HAR…      9\n 3 ICB2     104    251      2 USA       1945        5        1 TRIESTE I       7\n 4 ICB2     108    259      2 USA       1945        8       23 AZERBAIJAN      7\n 5 ICB2     111    265      2 USA       1946        8        7 TURKISH S…      2\n 6 ICB2     114    271      2 USA       1947        2       21 TRUMAN DO…      2\n 7 ICB2     123    293      2 USA       1948        6        7 BERLIN BL…      3\n 8 ICB2     125    295      2 USA       1948        9       23 CHINA CIV…      8\n 9 ICB2     132    308      2 USA       1950        6       25 KOREAN WA…      8\n10 ICB2     133    314      2 USA       1950        9       30 KOREAN WA…      8\n# ℹ 1,121 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n# Arrange by start year, then by actor ID within year\narrange(df_crises, systrgyr, cracid)\n\n# A tibble: 1,131 × 95\n   icb2  crisno cracno cracid actor systrgyr systrgmo systrgda crisname   triggr\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 ICB2       2      2     93 NIC       1918        5       25 COSTA RIC…      7\n 2 ICB2       2      3     94 COS       1918        5       25 COSTA RIC…      4\n 3 ICB2       1      1    365 RUS       1918        5       NA RUSSIAN C…      9\n 4 ICB2       3      4    365 RUS       1918        6       23 RUSSIAN C…      7\n 5 ICB2       4      5    365 RUS       1918       11       18 BALTIC IN…      6\n 6 ICB2       4      6    366 EST       1918       11       18 BALTIC IN…      9\n 7 ICB2       4      8    367 LAT       1918       11       18 BALTIC IN…      9\n 8 ICB2       4      7    368 LIT       1918       11       18 BALTIC IN…      9\n 9 ICB2       8     18    200 UKG       1919        4       15 THIRD AFG…      7\n10 ICB2      11     24    220 FRN       1919       11       NA CILICIAN …      9\n# ℹ 1,121 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n\nTo instead sort in descending order, wrap the variable name in desc().\n\n# Arrange so that the most recent crises come first\narrange(df_crises, desc(systrgyr))\n\n# A tibble: 1,131 × 95\n   icb2  crisno cracno cracid actor systrgyr systrgmo systrgda crisname   triggr\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 ICB2     506   1117    615 ALG       2021        1        9 US VISIT …      2\n 2 ICB2     507   1118    710 CHN       2021        1        9 US-TAIWAN…      2\n 3 ICB2     507   1119    713 TAW       2021        1        9 US-TAIWAN…      7\n 4 ICB2     508   1120    652 SYR       2021        1       13 ISRAEL-IR…      9\n 5 ICB2     508   1121    630 IRN       2021        1       13 ISRAEL-IR…      9\n 6 ICB2     509   1122    740 JPN       2021        1       22 CHINA COA…      5\n 7 ICB2     511   1128    703 KYR       2021        4       28 KOK-TASH …      9\n 8 ICB2     511   1129    702 TAJ       2021        4       28 KOK-TASH …      9\n 9 ICB2     512   1130    345 YUG       2021        9       20 KOSOVO LI…      7\n10 ICB2     512   1131    347 KOS       2021        9       20 KOSOVO LI…      7\n# ℹ 1,121 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n\n\n\n\n2.2.2 Subsetting by row: filter()\nWhat if we only want the crises that started during the Cold War? That involves selecting rows from the data rather than columns, which we’ll use the filter() function for. But before we can do that, we need to learn a bit about making logical statements in R.\n\n2.2.2.1 Comparisons and logical operators\nIf we make a comparison in R, it will spit out whether our statement is TRUE or FALSE. There are six logical comparisons we can make in R:\n\nx == y: Is x equal to y? (Notice that we use a double equals sign, not a single one!)\nx != y: Is x unequal to y?\nx &gt; y: Is x greater than y?\nx &gt;= y: Is x greater than or equal to y?\nx &lt; y: Is x less than y?\nx &lt;= y: Is x less than or equal to y?\n\nAs an example, let’s assign the value 1950 to the variable x.\n\nx &lt;- 1950\n\nNow let’s look at the values of a few logical comparisons.\n\nx == 1950\n\n[1] TRUE\n\nx == 1951\n\n[1] FALSE\n\nx != 1951\n\n[1] TRUE\n\nx &lt; 1950\n\n[1] FALSE\n\nx &lt;= 1950\n\n[1] TRUE\n\n\nIf we use these with a vector of numbers, R calculates the comparison for each individual element of the vector. So instead of just looking at 1950, let’s look at each number from 1948 to 1952.\n\nx &lt;- 1948:1952\nx\n\n[1] 1948 1949 1950 1951 1952\n\n\nWe’ve got a vector of five years here, so each of our logical comparisons will now return a vector of five TRUE/FALSE values.\n\nx == 1950\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\nx == 1951\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\nx != 1951\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE\n\nx &lt; 1950\n\n[1]  TRUE  TRUE FALSE FALSE FALSE\n\nx &lt;= 1950\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE\n\n\nYou can combine logical comparisons like this with square brackets to select elements from a vector on the basis of a logical comparison.\n\nx[x &lt; 1950]\n\n[1] 1948 1949\n\n\nAbove we’ve compared a vector to a single number. What if we compare a vector to another vector of the same length?\n\n\nThe last possibility is comparing a vector to another vector of a different length. Loosely speaking, R handles this situation by repeating the shorter vector until the two match up. R’s behavior in this situation can be hard to predict, so I recommend only doing logical comparisons against either a single number, or a vector of the same length. You can check the length of a vector using the length() function, FYI.\n\ny &lt;- c(1948, 1948, 1950, 1950, 1946)\ny\n\n[1] 1948 1948 1950 1950 1946\n\nx == y\n\n[1]  TRUE FALSE  TRUE FALSE FALSE\n\nx != y\n\n[1] FALSE  TRUE FALSE  TRUE  TRUE\n\nx &gt; y\n\n[1] FALSE  TRUE FALSE  TRUE  TRUE\n\nx &gt;= y\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\n\nThe exclamation point means “not”. You can use it to turn TRUE into FALSE and vice versa. Just make sure that you put parentheses around the whole statement that you’re trying to negate.\n\n!(x &gt;= y)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\n\nSometimes we will want to string together multiple comparisons. There are two ways to combine comparisons.\n\nand: a & b is TRUE when a is TRUE and b is also TRUE. If either or both are FALSE, then so is a & b.\nor: a | b is TRUE when a is TRUE, or b is TRUE, or both. The only way for a | b to be FALSE is for both a and b to be FALSE.\n\n\n\n\na\nb\na & b (and)\na | b (or)\n\n\n\n\nTRUE\nTRUE\nTRUE\nTRUE\n\n\nTRUE\nFALSE\nFALSE\nTRUE\n\n\nFALSE\nTRUE\nFALSE\nTRUE\n\n\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n\n\n# Reminder:\n# x = 1948, 1949, 1950, 1951, 1952\n# y = 1948, 1948, 1950, 1950, 1946\n\nx &gt; y & x &lt; 1950\n\n[1] FALSE  TRUE FALSE FALSE FALSE\n\nx &gt; y | x &lt; 1950\n\n[1]  TRUE  TRUE FALSE  TRUE  TRUE\n\n\nBe aware that & and | only operate on TRUE and FALSE values, not on other values. There are situations where you might use “and” or “or” in an English sentence, but where & and | can’t just be substituted into your R code. For example, think about trying to code up the logical statement “x or y is less than 100.”\n\nx &lt;- c(75, 100, 125, 150)\ny &lt;- c(100, 90, 150, 99)\n\n# Won't work correctly, because x and y aren't TRUE/FALSE\n(x | y) &lt; 100\n\n[1] TRUE TRUE TRUE TRUE\n\n# Will work correctly, because \"x &lt; 100\" and \"y &lt; 100\" are TRUE/FALSE\n(x &lt; 100) | (y &lt; 100)\n\n[1]  TRUE  TRUE FALSE  TRUE\n\n\n\n\nCompletely optional aside for those who are morbidly curious why we get all TRUE values when we run (x | y) &lt; 100: When you plug numbers into a logical operator, any non-zero number evaluates to TRUE. Since every element of x and y is nonzero, x | y evaluates to TRUE, TRUE, TRUE, TRUE. Then, when you plug logical values into a numerical comparison, TRUE values are treated like 1 and FALSE values like 0. So R converts each TRUE to 1, observes that 1 is less than 100, and again returns TRUE for each one.\nOne final minor note: If you look for R help on the Internet, you’ll sometimes see people use && and ||. What’s the difference between & and &&, or between | and ||? The doubled versions only work on a single TRUE/FALSE statement. This distinction is important in programming situations where you need to make sure you’re only working with a single value. That won’t typically come up in this course, so to make our lives easier we’ll just always use the single versions.\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(1, 1, 4, 4, 8)\n\nx &lt; y | x &gt; y   # returns c(FALSE, TRUE, TRUE, FALSE, TRUE)\n\n[1] FALSE  TRUE  TRUE FALSE  TRUE\n\nx &lt; y || x &gt; y  # won't run, will give error message\n\nError in x &lt; y || x &gt; y: 'length = 5' in coercion to 'logical(1)'\n\n\n\n\n2.2.2.2 The filter() function\nNow that we’re familiar with comparisons and logical operators, let’s return to the crisis data to reduce it down to Cold War–era observations.\nRemember that we can pull a single column from a data frame using the dollar sign. And we can use square brackets on a data frame to select particular observations from it. So we could subset to the Cold War observations that way.\n\ndf_crises[df_crises$systrgyr &gt;= 1946 & df_crises$systrgyr &lt;= 1989, ]\n\n# A tibble: 585 × 95\n   icb2  crisno cracno cracid actor systrgyr systrgmo systrgda crisname   triggr\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 ICB2     110    263    365 RUS       1946        6       30 COMMUNISM…      4\n 2 ICB2     111    264    640 TUR       1946        8        7 TURKISH S…      2\n 3 ICB2     111    265      2 USA       1946        8        7 TURKISH S…      2\n 4 ICB2     112    266    350 GRC       1946       11       13 GREEK CIV…      6\n 5 ICB2     113    267    365 RUS       1947        2       10 COMMUNISM…      4\n 6 ICB2     113    268    310 HUN       1947        2       10 COMMUNISM…      6\n 7 ICB2     114    269    350 GRC       1947        2       21 TRUMAN DO…      3\n 8 ICB2     114    270    640 TUR       1947        2       21 TRUMAN DO…      3\n 9 ICB2     114    271      2 USA       1947        2       21 TRUMAN DO…      2\n10 ICB2     115    272    365 RUS       1947        7        3 MARSHALL …      2\n# ℹ 575 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n\nThat works, but it’s a bit ugly and unwieldy. The filter() function gives us an easier way to do the same thing.\n\nfilter(df_crises, systrgyr &gt;= 1946 & systrgyr &lt;= 1989)\n\n# A tibble: 580 × 95\n   icb2  crisno cracno cracid actor systrgyr systrgmo systrgda crisname   triggr\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 ICB2     110    263    365 RUS       1946        6       30 COMMUNISM…      4\n 2 ICB2     111    264    640 TUR       1946        8        7 TURKISH S…      2\n 3 ICB2     111    265      2 USA       1946        8        7 TURKISH S…      2\n 4 ICB2     112    266    350 GRC       1946       11       13 GREEK CIV…      6\n 5 ICB2     113    267    365 RUS       1947        2       10 COMMUNISM…      4\n 6 ICB2     113    268    310 HUN       1947        2       10 COMMUNISM…      6\n 7 ICB2     114    269    350 GRC       1947        2       21 TRUMAN DO…      3\n 8 ICB2     114    270    640 TUR       1947        2       21 TRUMAN DO…      3\n 9 ICB2     114    271      2 USA       1947        2       21 TRUMAN DO…      2\n10 ICB2     115    272    365 RUS       1947        7        3 MARSHALL …      2\n# ℹ 570 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n\nWhen dealing with character columns, the greater/lesser comparisons no longer make sense, but we can still use the equality and inequality comparisons. Just make sure to use quotation marks around whatever you’re comparing to!\n\nfilter(df_crises, actor == \"USA\")\n\n# A tibble: 78 × 95\n   icb2  crisno cracno cracid actor systrgyr systrgmo systrgda crisname   triggr\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 ICB2      59    128      2 USA       1937       12       12 PANAY INC…      9\n 2 ICB2      88    224      2 USA       1941       11       26 PEARL HAR…      9\n 3 ICB2     104    251      2 USA       1945        5        1 TRIESTE I       7\n 4 ICB2     108    259      2 USA       1945        8       23 AZERBAIJAN      7\n 5 ICB2     111    265      2 USA       1946        8        7 TURKISH S…      2\n 6 ICB2     114    271      2 USA       1947        2       21 TRUMAN DO…      2\n 7 ICB2     123    293      2 USA       1948        6        7 BERLIN BL…      3\n 8 ICB2     125    295      2 USA       1948        9       23 CHINA CIV…      8\n 9 ICB2     132    308      2 USA       1950        6       25 KOREAN WA…      8\n10 ICB2     133    314      2 USA       1950        9       30 KOREAN WA…      8\n# ℹ 68 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\nfilter(df_crises, actor != \"RUS\")\n\n# A tibble: 1,079 × 95\n   icb2  crisno cracno cracid actor systrgyr systrgmo systrgda crisname   triggr\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 ICB2       2      2     93 NIC       1918        5       25 COSTA RIC…      7\n 2 ICB2       2      3     94 COS       1918        5       25 COSTA RIC…      4\n 3 ICB2       4      6    366 EST       1918       11       18 BALTIC IN…      9\n 4 ICB2       4      7    368 LIT       1918       11       18 BALTIC IN…      9\n 5 ICB2       4      8    367 LAT       1918       11       18 BALTIC IN…      9\n 6 ICB2       5      9    315 CZE       1919        1       15 TESCHEN         2\n 7 ICB2       5     10    290 POL       1919        1       15 TESCHEN         7\n 8 ICB2       6     11    310 HUN       1919        3       20 HUNGARIAN…      2\n 9 ICB2       6     12    315 CZE       1919        3       20 HUNGARIAN…      9\n10 ICB2       6     13    310 HUN       1919        3       20 HUNGARIAN…      2\n# ℹ 1,069 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n\nWhat if you wanted to get all of the observations involving the United States (USA), Russia (RUS), or the United Kingdom (UKG)? One kind of tedious way to go about it would be to string many “or” statements together.\n\nfilter(df_crises, actor == \"USA\" | actor == \"RUS\" | actor == \"UKG\")\n\n# A tibble: 178 × 95\n   icb2  crisno cracno cracid actor systrgyr systrgmo systrgda crisname   triggr\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 ICB2       1      1    365 RUS       1918        5       NA RUSSIAN C…      9\n 2 ICB2       3      4    365 RUS       1918        6       23 RUSSIAN C…      7\n 3 ICB2       4      5    365 RUS       1918       11       18 BALTIC IN…      6\n 4 ICB2       8     18    200 UKG       1919        4       15 THIRD AFG…      7\n 5 ICB2       9     19    365 RUS       1919        4       20 FINNISH/R…      9\n 6 ICB2      10     21    365 RUS       1919        4       30 BESSARABIA      2\n 7 ICB2      13     27    365 RUS       1920        4       25 POLISH/RU…      9\n 8 ICB2      14     29    365 RUS       1920        5       66 PERSIAN B…      7\n 9 ICB2      26     56    200 UKG       1922        9       23 CHANAK          7\n10 ICB2      31     66    200 UKG       1924        9       29 MOSUL LAN…      2\n# ℹ 168 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n\nThe easier way — especially if you’re matching a large number of values — is to use the %in% operator in R. When you run x %in% y, it gives you a TRUE or FALSE for each entry of x: TRUE if it matches at least one entry of y, and FALSE otherwise.\n\nfilter(df_crises, actor %in% c(\"USA\", \"RUS\", \"UKG\"))\n\n# A tibble: 178 × 95\n   icb2  crisno cracno cracid actor systrgyr systrgmo systrgda crisname   triggr\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 ICB2       1      1    365 RUS       1918        5       NA RUSSIAN C…      9\n 2 ICB2       3      4    365 RUS       1918        6       23 RUSSIAN C…      7\n 3 ICB2       4      5    365 RUS       1918       11       18 BALTIC IN…      6\n 4 ICB2       8     18    200 UKG       1919        4       15 THIRD AFG…      7\n 5 ICB2       9     19    365 RUS       1919        4       20 FINNISH/R…      9\n 6 ICB2      10     21    365 RUS       1919        4       30 BESSARABIA      2\n 7 ICB2      13     27    365 RUS       1920        4       25 POLISH/RU…      9\n 8 ICB2      14     29    365 RUS       1920        5       66 PERSIAN B…      7\n 9 ICB2      26     56    200 UKG       1922        9       23 CHANAK          7\n10 ICB2      31     66    200 UKG       1924        9       29 MOSUL LAN…      2\n# ℹ 168 more rows\n# ℹ 85 more variables: yrtrig &lt;dbl&gt;, motrig &lt;dbl&gt;, datrig &lt;dbl&gt;, trigent &lt;dbl&gt;,\n#   trigloc &lt;dbl&gt;, southv &lt;dbl&gt;, southpow &lt;dbl&gt;, sizedu &lt;dbl&gt;, strcdu &lt;dbl&gt;,\n#   comlev &lt;dbl&gt;, majres &lt;dbl&gt;, yerres &lt;dbl&gt;, monres &lt;dbl&gt;, dayres &lt;dbl&gt;,\n#   trgresra &lt;dbl&gt;, crismg &lt;dbl&gt;, cenvio &lt;dbl&gt;, sevvio &lt;dbl&gt;, usinv &lt;dbl&gt;,\n#   usfavr &lt;dbl&gt;, suinv &lt;dbl&gt;, sufavr &lt;dbl&gt;, gbinv &lt;dbl&gt;, gbfavr &lt;dbl&gt;,\n#   frinv &lt;dbl&gt;, frfavr &lt;dbl&gt;, itinv &lt;dbl&gt;, itfavr &lt;dbl&gt;, grinv &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nIn-class exercise\n\n\n\nHow can you filter down to the observations that are not those three countries?\n\n#\n# [Write your answer here]\n#\n\n\n\n\n\n\n2.2.3 “Piping” commands together\nHere’s a pretty common way to start off your data analysis workflow:\n\nLoad your raw data into R.\nReduce the columns down to the ones you will use in your analysis.\nFilter out the rows that aren’t relevant to your analysis.\n\nRemember that R doesn’t save what you do to a data frame unless you explicitly assign the result to a variable. This means that a data cleaning process might involve a lot of intermediate assignments that you never end up using again.\nImagine we want to extract the name of the crisis, the name and numeric ID of the country involved, the year the crisis began, the number of days it lasted (trgterra), and the outcome for the given actor (outcom) Along the way we’ll rename the variables to be less ugly. And furthermore imagine we only want Cold War–era observations, and only for the USA, Russia, and the United Kingdom. We could run the following sequence of commands:\n\ndf_crises_cw &lt;- select(\n  df_crises,\n  crisis_name = crisname,\n  actor_name = actor,\n  actor_id = cracid,\n  year = systrgyr,\n  duration = trgterra,\n  outcome = outcom\n)\ndf_crises_cw &lt;- filter(df_crises_cw, year &gt;= 1946 & year &lt;= 1989)\ndf_crises_cw &lt;- filter(df_crises_cw, actor_name %in% c(\"USA\", \"RUS\", \"UKG\"))\ndf_crises_cw\n\n# A tibble: 87 × 6\n   crisis_name          actor_name actor_id  year duration outcome\n   &lt;chr&gt;                &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 COMMUNISM IN POLAND  RUS             365  1946      204       1\n 2 TURKISH STRAITS      USA               2  1946       81       1\n 3 COMMUNISM IN HUNGARY RUS             365  1947      112       1\n 4 TRUMAN DOCTRINE      USA               2  1947       91       1\n 5 MARSHALL PLAN        RUS             365  1947        9       1\n 6 COMMUNISM IN CZECH.  RUS             365  1948       13       1\n 7 BERLIN BLOCKADE      RUS             365  1948      340       4\n 8 BERLIN BLOCKADE      UKG             200  1948      323       1\n 9 BERLIN BLOCKADE      USA               2  1948      323       1\n10 CHINA CIVIL WAR      USA               2  1948       34       4\n# ℹ 77 more rows\n\n\nThat certainly works fine to give us what we want, but it involved a lot of repetitive typing and retyping of df_crises_cw &lt;- function_name(df_crises_cw, ...). And that’s only with fairly simple subsetting operations.\nThe pipe operator in R lets us chain together commands like this much more succinctly. x |&gt; function_name(...) is equivalent to function_name(x, ...) in R.\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nmean(x)\n\n[1] 5.5\n\nx |&gt; mean()\n\n[1] 5.5\n\n\nOn its own the pipe might seem dumb, like just more typing for the same result. Weren’t we trying to reduce the amount of typing? The pipe is mainly useful when we are chaining many commands together.\n# Chained commands\ny &lt;- first_command(x, arg1, arg2)\ny &lt;- second_command(y, arg3)\ny &lt;- third_command(y, arg4, arg5)\nfourth_command(y, arg6)\n\n# Same thing, but piped\nx |&gt;\n  first_command(arg1, arg2) |&gt;\n  second_command(arg3) |&gt;\n  third_command(arg4, arg5) |&gt;\n  fourth_command(arg6)\nLet’s see the pipe in action in the context of our data cleaning example.\n\ndf_crises |&gt;\n  select(\n    crisis_name = crisname,\n    actor_name = actor,\n    actor_id = cracid,\n    year = systrgyr,\n    duration = trgterra,\n    outcome = outcom\n  ) |&gt;\n  filter(year &gt;= 1946 & year &lt;= 1989) |&gt;\n  filter(actor_name %in% c(\"USA\", \"RUS\", \"UKG\"))\n\n# A tibble: 87 × 6\n   crisis_name          actor_name actor_id  year duration outcome\n   &lt;chr&gt;                &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 COMMUNISM IN POLAND  RUS             365  1946      204       1\n 2 TURKISH STRAITS      USA               2  1946       81       1\n 3 COMMUNISM IN HUNGARY RUS             365  1947      112       1\n 4 TRUMAN DOCTRINE      USA               2  1947       91       1\n 5 MARSHALL PLAN        RUS             365  1947        9       1\n 6 COMMUNISM IN CZECH.  RUS             365  1948       13       1\n 7 BERLIN BLOCKADE      RUS             365  1948      340       4\n 8 BERLIN BLOCKADE      UKG             200  1948      323       1\n 9 BERLIN BLOCKADE      USA               2  1948      323       1\n10 CHINA CIVIL WAR      USA               2  1948       34       4\n# ℹ 77 more rows\n\n\nIt might take a bit getting used to, but I personally find it much easier to use the pipe when going through the data cleaning process. One thing to keep in mind, just like when you run these commands the regular way, they won’t change your data frame in place. You need to explicitly assign the output to a variable name.\n\ndf_crises_cw &lt;- df_crises |&gt;\n  select(\n    crisis_name = crisname,\n    actor_name = actor,\n    actor_id = cracid,\n    year = systrgyr,\n    duration = trgterra,\n    outcome = outcom\n  ) |&gt;\n  filter(year &gt;= 1946 & year &lt;= 1989) |&gt;\n  filter(actor_name %in% c(\"USA\", \"RUS\", \"UKG\"))\n\nFinal note on the pipe. Until pretty recently, the pipe was %&gt;% instead of |&gt;. So you’ll see a lot of code on the Internet that uses %&gt;%. Don’t worry, it works exactly the same as |&gt; (except in some edge cases that won’t come up for us).\n\n\n\n\n\n\nIn-class exercise\n\n\n\nUse the pipe to create a data frame called df_long_crises with the following specs:\n\nSame variables as in my example, except with the numeric crisis ID instead of the crisis name\nOnly observations where the actor was involved in the crisis for 100+ days\nExclude observations of Russia from 1990 onward\n\n\n#\n# [Write your answer here]\n#",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html#transforming",
    "href": "data_wrangling.html#transforming",
    "title": "2  Data Wrangling",
    "section": "2.3 Transforming",
    "text": "2.3 Transforming\n\n2.3.1 Changing variables and making new ones\nThe mutate() function lets you change a column in the dataframe, or create a new column using the values in existing columns. For example, right now the duration of each crisis is measured in days. We can add new columns to measure duration in weeks and years instead. As always, if we want our new columns to persist instead of vanishing into the ether, we need to assign the output to a variable—either overwriting df_crises_cw or creating a new data frame.\n\ndf_crises_cw &lt;- df_crises_cw |&gt;\n  mutate(\n    duration_weeks = duration / 7,\n    duration_years = duration / 365\n  )\n\ndf_crises_cw\n\n# A tibble: 87 × 8\n   crisis_name         actor_name actor_id  year duration outcome duration_weeks\n   &lt;chr&gt;               &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n 1 COMMUNISM IN POLAND RUS             365  1946      204       1          29.1 \n 2 TURKISH STRAITS     USA               2  1946       81       1          11.6 \n 3 COMMUNISM IN HUNGA… RUS             365  1947      112       1          16   \n 4 TRUMAN DOCTRINE     USA               2  1947       91       1          13   \n 5 MARSHALL PLAN       RUS             365  1947        9       1           1.29\n 6 COMMUNISM IN CZECH. RUS             365  1948       13       1           1.86\n 7 BERLIN BLOCKADE     RUS             365  1948      340       4          48.6 \n 8 BERLIN BLOCKADE     UKG             200  1948      323       1          46.1 \n 9 BERLIN BLOCKADE     USA               2  1948      323       1          46.1 \n10 CHINA CIVIL WAR     USA               2  1948       34       4           4.86\n# ℹ 77 more rows\n# ℹ 1 more variable: duration_years &lt;dbl&gt;\n\n\nThere’s a very useful function for mutating called if_else(). You invoke it as if_else(condition, a, b):\n\ncondition: vector of TRUE and FALSE values\na: value, or vector of values, to use for entries where condition is TRUE\nb: value, or vector of values, to use for entries where condition is FALSE\n\n\n# Simple if_else example\nx &lt;- c(1, 2, 3, 4)\ny &lt;- c(-100, -200, 300, 400)\nif_else(x &gt; y, x, y)\n\n[1]   1   2 300 400\n\nif_else(x &gt; y, \"ice cream\", \"pizza\")\n\n[1] \"ice cream\" \"ice cream\" \"pizza\"     \"pizza\"    \n\n\n\n\n\n\n\n\nIn-class exercise\n\n\n\nUse mutate() and if_else() together to add another column to df_crises_cw called crisis_length. This variable should say \"long\" if the crisis is 100 or more days, and \"short\" otherwise.\n\n#\n# [Write your answer here]\n#\n\n\n\nThe “outcome” variable here is recorded as numbers, which stand for different categories of crisis outcomes. The ICB codebook explains which category is associated with each numerical value.\n\nVictory: this country achieved its basic goals.\nCompromise: this country partly achieved its basic goals.\nStalemate: there was no major change in the situation.\nDefeat: this country did not achieve its basic goals, and instead yielded or surrendered.\nOther.\n\nSo for example, looking at the rows associated with the Berlin Blockade, we see that the USA and United Kingdom experienced victory, while the Soviet Union was defeated.\n\ndf_crises_cw |&gt;\n  filter(crisis_name == \"BERLIN BLOCKADE\") |&gt;\n  select(actor_name, outcome, everything())\n\n# A tibble: 3 × 8\n  actor_name outcome crisis_name     actor_id  year duration duration_weeks\n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n1 RUS              4 BERLIN BLOCKADE      365  1948      340           48.6\n2 UKG              1 BERLIN BLOCKADE      200  1948      323           46.1\n3 USA              1 BERLIN BLOCKADE        2  1948      323           46.1\n# ℹ 1 more variable: duration_years &lt;dbl&gt;\n\n\nWe could use a sequence of 5 if_else() statements to convert the outcome variable in terms of category names instead of (hard to remember) numeric identifiers. But it’s easier to use case_when():\n\ndf_crises_cw &lt;- df_crises_cw |&gt;\n  mutate(\n    outcome = case_when(\n      outcome == 1 ~ \"victory\",\n      outcome == 2 ~ \"compromise\",\n      outcome == 3 ~ \"stalemate\",\n      outcome == 4 ~ \"defeat\",\n      outcome == 5 ~ \"other\"\n    )\n  )\n\ndf_crises_cw\n\n# A tibble: 87 × 8\n   crisis_name         actor_name actor_id  year duration outcome duration_weeks\n   &lt;chr&gt;               &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 COMMUNISM IN POLAND RUS             365  1946      204 victory          29.1 \n 2 TURKISH STRAITS     USA               2  1946       81 victory          11.6 \n 3 COMMUNISM IN HUNGA… RUS             365  1947      112 victory          16   \n 4 TRUMAN DOCTRINE     USA               2  1947       91 victory          13   \n 5 MARSHALL PLAN       RUS             365  1947        9 victory           1.29\n 6 COMMUNISM IN CZECH. RUS             365  1948       13 victory           1.86\n 7 BERLIN BLOCKADE     RUS             365  1948      340 defeat           48.6 \n 8 BERLIN BLOCKADE     UKG             200  1948      323 victory          46.1 \n 9 BERLIN BLOCKADE     USA               2  1948      323 victory          46.1 \n10 CHINA CIVIL WAR     USA               2  1948       34 defeat            4.86\n# ℹ 77 more rows\n# ℹ 1 more variable: duration_years &lt;dbl&gt;\n\n\n\n\nThere’s another function called case_match() that would let you do this same thing with even less typing. Go ahead, look it up, and use it if you like! But case_when() is a bit more flexible, so to avoid confusion I’m only going to use it.\n\n\n2.3.2 Grouping and summarizing\nThe summarize() function lets us calculate summaries of columns in the data. For example, let’s find the shortest, longest, and average crisis durations among the USA/UK/USSR during the Cold War.\n\ndf_crises_cw |&gt;\n  summarize(\n    shortest = min(duration),\n    longest = max(duration),\n    average = mean(duration)\n  )\n\n# A tibble: 1 × 3\n  shortest longest average\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1        2     486    99.2\n\n\n\n# Seemingly easier way\nmin(df_crises_cw$duration)\n\n[1] 2\n\nmax(df_crises_cw$duration)\n\n[1] 486\n\nmean(df_crises_cw$duration)\n\n[1] 99.24138\n\n\nThe real power of summarize() comes in when you combine it with group_by() to calculate summaries for subcategories of the data. For example, say we wanted to calculate the shortest, longest, and average crisis duration separately for the USA, UK, and USSR. That would be tedious to do the “normal” way:\n\n# one of nine long expressions you'd have to type out\nmin(df_crises_cw$duration[df_crises_cw$actor_name == \"USA\"])\n\n[1] 2\n\n\ngroup_by |&gt; summarize makes this much easier.\n\ndf_crises_cw |&gt;\n  group_by(actor_name) |&gt;\n  summarize(\n    shortest = min(duration),\n    longest = max(duration),\n    average = mean(duration)\n  )\n\n# A tibble: 3 × 4\n  actor_name shortest longest average\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 RUS               3     486   124. \n2 UKG              12     323   106  \n3 USA               2     433    84.6\n\n\n\n\n\n\n\n\nIn-class exercise\n\n\n\nIn which year of the Cold War did the USA spend the most total days in crises? Use filter(), group_by(), summarize(), and arrange() together to figure it out.\n\n#\n# [Write your answer here]\n#\n\n\n\nWhat if you wanted to count the number of crises involving each country? There’s a special function called n() that counts the number of observations within each group.\n\ndf_crises_cw |&gt;\n  group_by(actor_name) |&gt;\n  summarize(number = n())\n\n# A tibble: 3 × 2\n  actor_name number\n  &lt;chr&gt;       &lt;int&gt;\n1 RUS            24\n2 UKG            15\n3 USA            48\n\n\nYou can group on multiple variables at once. The number of rows in the result will be the number of unique combinations of values across the grouping variables.\n\ndf_crises_cw |&gt;\n  group_by(actor_name, outcome) |&gt;\n  summarize(\n    number = n(),\n    shortest = min(duration),\n    longest = max(duration)\n  )\n\n`summarise()` has grouped output by 'actor_name'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 13 × 5\n# Groups:   actor_name [3]\n   actor_name outcome    number shortest longest\n   &lt;chr&gt;      &lt;chr&gt;       &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 RUS        compromise      3        8     222\n 2 RUS        defeat          5        3     340\n 3 RUS        stalemate       3        9     260\n 4 RUS        victory        13        9     486\n 5 UKG        compromise      2      183     192\n 6 UKG        defeat          2       66     104\n 7 UKG        other           1       17      17\n 8 UKG        stalemate       2      112     293\n 9 UKG        victory         8       12     323\n10 USA        compromise      7        4     433\n11 USA        defeat          6       10     336\n12 USA        stalemate       8        5     293\n13 USA        victory        27        2     323",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html#reshaping",
    "href": "data_wrangling.html#reshaping",
    "title": "2  Data Wrangling",
    "section": "2.4 Reshaping",
    "text": "2.4 Reshaping\nTo get practice with reshaping and merging data, we’re going to bring in a second dataset. military.csv, stored at https://bkenkel.com/qps1/data/military.csv, contains data on military spending and force sizes by country from 1816 to 2019.\n\ndf_military &lt;- read_csv(\"https://bkenkel.com/qps1/data/military.csv\")\n\ndf_military\n\n# A tibble: 31,902 × 5\n   ccode stateabb  year mil_indicator amount\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1     2 USA       1816 spending        3823\n 2     2 USA       1816 personnel         17\n 3     2 USA       1817 spending        2466\n 4     2 USA       1817 personnel         15\n 5     2 USA       1818 spending        1910\n 6     2 USA       1818 personnel         14\n 7     2 USA       1819 spending        2301\n 8     2 USA       1819 personnel         13\n 9     2 USA       1820 spending        1556\n10     2 USA       1820 personnel         15\n# ℹ 31,892 more rows\n\n\nThe amounts here are in thousands of dollars and people respectively. For example, looking at the first two rows, in 1816 the USA spent $3.8 million on the military, which consisted of 17,000 soldiers.\nHow can we reshape this data so that there’s a single row for the USA in 1816, a single row for the USA in 1817, and so on? As of now the data is “longer” than we want it to be, and we’d like it to be “wider”. Hence we’ll use the function pivot_wider(). Here’s how we use it.\n\ndf_military_wide &lt;- df_military |&gt;\n  pivot_wider(names_from = mil_indicator, values_from = amount)\n\ndf_military_wide\n\n# A tibble: 15,951 × 5\n   ccode stateabb  year spending personnel\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1     2 USA       1816     3823        17\n 2     2 USA       1817     2466        15\n 3     2 USA       1818     1910        14\n 4     2 USA       1819     2301        13\n 5     2 USA       1820     1556        15\n 6     2 USA       1821     1612        11\n 7     2 USA       1822     1079        10\n 8     2 USA       1823     1170        11\n 9     2 USA       1824     1261        11\n10     2 USA       1825     1336        11\n# ℹ 15,941 more rows\n\n\nMaking a data frame “wider” means taking data that’s currently stored row by row, and spreading it out across columns instead. The names_from argument tells pivot_wider() which column of the original data frame contains the names of the new columns we want to create. The values_from argument tells it which column contains the values that we want to put into each column.\n\n\n\n\n\n\nIn-class exercise\n\n\n\nGo back to our data on the USA, UK, and USSR in Cold War crises. Use pivot_wider() to create a new data frame where each row is a unique crisis name, and there are columns for the USA, UK, and USSR telling that country’s outcome from the given crisis. Were there any crises that ended in victory for both the USA and the USSR?\n\n#\n# [Write your answer here]\n#\n\n\n\nThe opposite of pivot_wider() is, naturally enough, pivot_longer(). For example, imagine your raw data had a single row for each country, then a separate column for its military spending each year.\n\ndf_too_wide &lt;- tibble(\n  ccode = c(2, 200, 365),\n  stateabb = c(\"USA\", \"UKG\", \"RUS\"),\n  mil1816 = c(17, 255, 800),\n  mil1817 = c(15, 190, 700),\n  mil1818 = c(14, 173, 600)\n)\n\ndf_too_wide\n\n# A tibble: 3 × 5\n  ccode stateabb mil1816 mil1817 mil1818\n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     2 USA           17      15      14\n2   200 UKG          255     190     173\n3   365 RUS          800     700     600\n\n\nTo use pivot_longer(), we have to tell it (1) the set of columns whose values we want to collapse into a single column, (2) what we want to name the column that stores the column names from the original data, and (3) what we want to name the column that stores the values.\n\ndf_too_wide |&gt;\n  pivot_longer(\n    starts_with(\"mil\"),\n    names_to = \"year\",\n    values_to = \"personnel\"\n  )\n\n# A tibble: 9 × 4\n  ccode stateabb year    personnel\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;\n1     2 USA      mil1816        17\n2     2 USA      mil1817        15\n3     2 USA      mil1818        14\n4   200 UKG      mil1816       255\n5   200 UKG      mil1817       190\n6   200 UKG      mil1818       173\n7   365 RUS      mil1816       800\n8   365 RUS      mil1817       700\n9   365 RUS      mil1818       600\n\n\nFYI: If you wanted to turn the year column there into a proper year, you could mutate() it to remove the “mil” text and then convert it to a number.\n\ndf_too_wide |&gt;\n  pivot_longer(\n    starts_with(\"mil\"),\n    names_to = \"year\",\n    values_to = \"personnel\"\n  ) |&gt;\n  mutate(\n    year = str_replace(year, \"mil\", \"\"),\n    year = as.numeric(year)\n  )\n\n# A tibble: 9 × 4\n  ccode stateabb  year personnel\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     2 USA       1816        17\n2     2 USA       1817        15\n3     2 USA       1818        14\n4   200 UKG       1816       255\n5   200 UKG       1817       190\n6   200 UKG       1818       173\n7   365 RUS       1816       800\n8   365 RUS       1817       700\n9   365 RUS       1818       600\n\n\nHow did I know to use the str_replace() function there? The stringr package contains tons of helpful functions for manipulating text in R. You can see everything it does by googling the documentation for it, or by entering help(package = \"stringr\") at the R console.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "data_wrangling.html#merging",
    "href": "data_wrangling.html#merging",
    "title": "2  Data Wrangling",
    "section": "2.5 Merging",
    "text": "2.5 Merging\nData work in political science often involves combining data from multiple sources. For example, here we have one dataset on country participation in international crises, and another on the size of their militaries. How can we put these together?\nThe first thing to do is decide which dataset should be the “base” that we merge other data sources into. What’s appropriate depends on the question you’re trying to answer. For our purposes here, let’s imagine we want to know whether crisis winners tend to spend more on their military than crisis losers. That means we only care about the countries that actually ended up in crises, so we should use the crisis data as our base and merge the military size data into that.\nThe next thing we need to make sure of is that we have some way to identify the same observation across datasets. Here we are going to match observations by the numerical “country code” and by year. First we’ll edit the military size data so that the country code indicator has the same name as in df_crises_cw. (We’ll also drop the country name variable, since we already have that in the original data.)\n\ndf_military_wide &lt;- df_military_wide |&gt;\n  rename(actor_id = ccode) |&gt;\n  select(-stateabb)\n\ndf_military_wide\n\n# A tibble: 15,951 × 4\n   actor_id  year spending personnel\n      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1        2  1816     3823        17\n 2        2  1817     2466        15\n 3        2  1818     1910        14\n 4        2  1819     2301        13\n 5        2  1820     1556        15\n 6        2  1821     1612        11\n 7        2  1822     1079        10\n 8        2  1823     1170        11\n 9        2  1824     1261        11\n10        2  1825     1336        11\n# ℹ 15,941 more rows\n\n\nNext we’ll use left_join() to merge the datasets together. We need to tell this function three things:\n\nThe data frame to use as our base.\nThe data frame that we want to merge values in from.\nThe column names to match observations by.\n\n\ndf_crisis_and_mil &lt;- left_join(\n  df_crises_cw,\n  df_military_wide,\n  by = c(\"actor_id\", \"year\")\n)\n\ndf_crisis_and_mil\n\n# A tibble: 87 × 10\n   crisis_name         actor_name actor_id  year duration outcome duration_weeks\n   &lt;chr&gt;               &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 COMMUNISM IN POLAND RUS             365  1946      204 victory          29.1 \n 2 TURKISH STRAITS     USA               2  1946       81 victory          11.6 \n 3 COMMUNISM IN HUNGA… RUS             365  1947      112 victory          16   \n 4 TRUMAN DOCTRINE     USA               2  1947       91 victory          13   \n 5 MARSHALL PLAN       RUS             365  1947        9 victory           1.29\n 6 COMMUNISM IN CZECH. RUS             365  1948       13 victory           1.86\n 7 BERLIN BLOCKADE     RUS             365  1948      340 defeat           48.6 \n 8 BERLIN BLOCKADE     UKG             200  1948      323 victory          46.1 \n 9 BERLIN BLOCKADE     USA               2  1948      323 victory          46.1 \n10 CHINA CIVIL WAR     USA               2  1948       34 defeat            4.86\n# ℹ 77 more rows\n# ℹ 3 more variables: duration_years &lt;dbl&gt;, spending &lt;dbl&gt;, personnel &lt;dbl&gt;\n\n\nNow we can use group_by() and summarize() to calculate the average size and spending of the military among crisis winners, crisis losers, and so on.\n\ndf_crisis_and_mil |&gt;\n  group_by(outcome) |&gt;\n  summarize(avg_spending = mean(spending),\n            avg_personnel = mean(personnel))\n\n# A tibble: 5 × 3\n  outcome    avg_spending avg_personnel\n  &lt;chr&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n1 compromise    57787476.         2436.\n2 defeat        57416910.         2638.\n3 other          4409778           839 \n4 stalemate     81993296          2714.\n5 victory       65411474.         2568.\n\n\n\n\n\n\n\n\nIn-class exercise\n\n\n\nWinners have smaller militaries than losers on average. Does this mean that increasing the size of your military actually hurts your ability to win a crisis? Why or why not? How could you dig deeper into the data to better figure out whether this is the case?\n\n[Write your answer here]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "univariate_analysis.html",
    "href": "univariate_analysis.html",
    "title": "3  Univariate Analysis",
    "section": "",
    "text": "3.1 Data: County-level presidential election results\nBy now you’re experts on wrangling data. It’s time to start using data to learn about politics.\nWe will start by analyzing a single variable at a time. Our goal is to come up with good quantitative summaries of each variable in our data — i.e., to collapse the complexity of what we observe into a few numerical values that are easier to wrap our minds around. The major concepts we will hit are:\nWe will work with the data file county_pres.csv, stored online at https://bkenkel.com/qps1/data/county_pres.csv, which records county-by-county results of US presidential elections from 2000 to 2020.\nlibrary(\"tidyverse\")\n\ndf_county_pres &lt;- read_csv(\"https://bkenkel.com/qps1/data/county_pres.csv\")\n\nglimpse(df_county_pres)\n\nRows: 22,093\nColumns: 12\n$ year            &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, …\n$ state           &lt;chr&gt; \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\",…\n$ region          &lt;chr&gt; \"South\", \"South\", \"South\", \"South\", \"South\", \"South\", …\n$ county          &lt;chr&gt; \"AUTAUGA\", \"BALDWIN\", \"BARBOUR\", \"BIBB\", \"BLOUNT\", \"BU…\n$ county_fips     &lt;chr&gt; \"01001\", \"01003\", \"01005\", \"01007\", \"01009\", \"01011\", …\n$ total_votes     &lt;dbl&gt; 17208, 56480, 10395, 7101, 17973, 4904, 7803, 38909, 1…\n$ dem_votes       &lt;dbl&gt; 4942, 13997, 5188, 2710, 4977, 3395, 3606, 15781, 5616…\n$ rep_votes       &lt;dbl&gt; 11993, 40872, 5096, 4273, 12667, 1433, 4127, 22306, 60…\n$ margin          &lt;dbl&gt; -7051, -26875, 92, -1563, -7690, 1962, -521, -6525, -4…\n$ pct_margin      &lt;dbl&gt; -0.409751278, -0.475832153, 0.008850409, -0.220109844,…\n$ competitiveness &lt;dbl&gt; -3, -3, 0, -3, -3, 3, -1, -2, 0, -1, -3, 0, -2, -3, -3…\n$ dem_win_state   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\nEach observation (row) in this dataset is a particular county in a particular year. Here are all of the variables (columns) in the data, though we’ll only be using some of them in class today:\nWe’re going to use this data to try to answer some simple questions about the competitiveness of presidential elections.\nData will help us answer these questions objectively, but any data analysis also involves judgment calls. For example, how should we define competitiveness? Is it closeness of the absolute number of votes, or of the vote percentages, or something else entirely? Can we sort races into “competitive” and “not competitive”, or does competitiveness lie on a continuum? Is competitiveness best assessed at the county, state, regional, or national level?\nThe important thing is to keep in mind the purpose of your data analysis. What are you trying to learn? What decisions are you hoping to make? If you’re working for a campaign and trying to help decide how to spend the advertising budget, the right notion of “competitiveness” might be different than if you’re a political science professor studying the effects of the economy on electoral competition. No matter what decision you make, it’s crucial that you:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Univariate Analysis</span>"
    ]
  },
  {
    "objectID": "univariate_analysis.html#data-county-level-presidential-election-results",
    "href": "univariate_analysis.html#data-county-level-presidential-election-results",
    "title": "3  Univariate Analysis",
    "section": "",
    "text": "Name\nDefinition\n\n\n\n\nyear\nElection year\n\n\nstate\nState name\n\n\nregion\nRegion of the country (US Census codings) the state is in\n\n\ncounty\nCounty name\n\n\ncounty_fips\nCounty FIPS (Federical Information Processing Standards) code\n\n\ntotal_votes\nTotal votes cast for any candidate in presidential race\n\n\ndem_votes\nVotes cast for the Democratic presidential candidate\n\n\nrep_votes\nVotes cast for the Republican presidential candidate\n\n\nmargin\nDifference in votes cast: dem_votes - rep_votes\n\n\npct_margin\nDifference relative to total votes: margin / total_votes\n\n\ncompetitiveness\nCategorization from -3 to +3 of how close the result was\n\n\ndem_win_state\nDid the Democrat win the state? 0 = No, 1 = Yes\n\n\n\n\n\nHow competitive are recent presidential elections?\nAre they getting more or less competitive?\nWhich places are competitive and non-competitive?\n\n\n\n\nPut some thought and reasoning into the judgment calls you make.\nBe clear and transparent with your audience about which decisions you made, and why.\nBe open to thinking about your problem in a different way! Try out alternative measures, conceptualizations, choices, etc., and see how sensitive your conclusions are.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Univariate Analysis</span>"
    ]
  },
  {
    "objectID": "univariate_analysis.html#central-tendency-whats-a-typical-value",
    "href": "univariate_analysis.html#central-tendency-whats-a-typical-value",
    "title": "3  Univariate Analysis",
    "section": "3.2 Central tendency: What’s a typical value?",
    "text": "3.2 Central tendency: What’s a typical value?\n\nData analysis is all about summarizing. Think about our county-level presidential vote data. We’ve got 22093 rows and 12 columns, for a whopping 265116 separate pieces of information — and this isn’t even a particularly “big” dataset! In order to wrap our puny human brains around this much information, we need to find ways to summarize it much more concisely.\nOne way to summarize a variable is to characterize its central tendency: to answer the question “what’s a typical value for this variable?” For any variable, there are multiple different measures of central tendency available to us, and it is a judgment call which of them are most useful for a given purpose. But to even know what our options are, we have to know what kind of variable we are dealing with.\n\n3.2.1 Continuous variables\nBy the strictest definition, a continuous variable is a variable whose values may lie in a continuum of numbers. The height of a person is a continuous variable: they might be 66 inches tall, or 66.5 inches tall, or 66.52 inches tall, or even 66.5240549 inches tall. We aren’t going to be that strict about continuous variables. A candidate can receive 10,000 votes or 10,001 votes, but she can’t receive 10,000.5 votes. Nonetheless, vote totals are close enough that we will treat them as continuous. What’s really important, for our purposes, is that it makes sense to add and subtract the values of continuous variables.\nIn other stats classes you may have heard of a further subdivision of continuous variables into “interval” and “ratio” variables. Typical measures of central tendency (and spread, covered below) don’t care about this distinction, so we won’t either.\n\nOne of the most common measures of central tendency for a continuous variable is the mean, also called the average. To calculate the mean, we add up all the values of the variable, then divide by the number of observations. If there are \\(N\\) observations of a continuous variable, say \\(x_1, x_2, \\ldots, x_N\\), then the mean is \\[\\bar{x} = \\frac{\\sum_{i=1}^N x_i}{N}.\\]\nAs you saw a bit in our section on data wrangling, we can use the mean() function in R to calculate the mean. For example, let’s calculate the average number of votes in a county across the 2000–2020 presidential elections.\n\nmean(df_county_pres$total_votes)\n\n[1] 44419.82\n\n\n\n\n\n\n\n\nIn-class exercise\n\n\n\nCalculate the average percentage by which Joe Biden won/lost across all counties in 2020. How does your result compare to his 4.5 percentage point margin of victory in the national popular vote? If there is a substantial difference, why do you think that is?\n\n#\n# [Write your answer here]\n#\n\n\n\nSometimes when you calculate an average, you want to place more weight on some observations than others. This is common with survey data. For example, imagine you have a survey sample where 50% of respondents have a college degree and 50% do not. In the overall population, we know that about 38% of American adults have a college degree and 62% do not. So if you are trying to create a representative estimate, you might want to downweight the degree-holding respondents who are overrepresented in your sample, and upweight the degree-less respondents who are underrepresented. We call this a weighted mean.\nFor the math lovers out there, we define a weighted mean by associating positive weights \\(w_1, w_2, \\ldots, w_N\\) with each observation \\(1, 2, \\ldots, N\\). The formula is then \\[\\text{weighted mean} = \\frac{\\sum_{i=1}^N w_i \\times x_i}{\\sum_{i = 1}^N w_i}.\\] The ordinary mean is the special case of the weighted mean where the weight on every observation is the same: \\(w_1 = w_2 = \\cdots = w_N\\).\nTo calculate a weighted mean in R, we use the weighted.mean() function. This takes two arguments: the vector of values that we are averaging, and then the weight to place on each observation.\n\n\n\n\n\n\nIn-class exercise\n\n\n\nReturn to the problem of calculating Biden’s average margin by county in 2020. Now calculate a weighted mean, where the weight on each county is the number of votes cast there. How does this compare to the raw mean, and to his 4.5 percentage point national margin of victory? What do you learn from the comparison?\n\n#\n# [Write your answer here]\n#\n\n\n\nThe other most common measure of central tendency for a continuous variable is the median. If the median of some variable is \\(m\\), that means half of the observations are less than or equal to \\(m\\), and half of the observations are greater than or equal to \\(m\\). We calculate the median in R with the median() function. For example, let’s look at the median number of votes cast across the counties in our data frame.\n\nmedian(df_county_pres$total_votes)\n\n[1] 11140\n\n\nYou can imagine it this way: Suppose we lined up each row of the data frame in order by total_votes, lowest to highest. The row at exactly the halfway point would have 11,140 total votes.\nThe mean and the median differ in terms of their sensitivity to outliers, data points that are extremely far from typical. For example, imagine you had a sample of 100 people: 99 normal people and Elon Musk. Because Elon Musk’s net worth is $420 billion, the average net worth of this sample would be at least $4.2 billion. And yet it would be misleading, in a sense, to say the average person in the sample is a billionaire — really you have one mega-billionaire and 99 regular people. The median net worth of the sample would be much, much lower.\n\n\nIn case you’re curious: I could only find statistics at the household level, but the median net worth of an American household is $192,000.\nBecause the mean is sensitive to outliers and the median is not, you might think we should prefer the median over the mean as a measure of central tendency. I think it’s more complicated than that. There are some situations where you care about outliers!\n\n\n\n\n\n\nIn-class exercise\n\n\n\nYou are running analytics for a political campaign. You recently tested two different templates for texts seeking political donations. You sent both texts to 1,000 different phone numbers from your list of supporters:\n\nTemplate A yielded a median donation of $5 and a mean donation of $7.\nTemplate B yielded a median donation of $0 (in other words, most receipients donated nothing) and a mean donation of $20.\n\nIf your goal is to just maximize the total amount of money donated, which template would you recommend sending out to the full list of supporters?\n\n[Write your answer here]\n\n\n\nHow competitive are recent presidential elections? Are they getting more or less competitive? Let’s take a few different calculations. Which one do you think best captures the competitiveness of each election?\n\ndf_county_pres |&gt;\n  group_by(year) |&gt;\n  summarize(\n    avg_pct_margin = mean(pct_margin),\n    avg_weighted_pct_margin = weighted.mean(pct_margin, total_votes),\n    med_pct_margin = median(pct_margin)\n  )\n\n# A tibble: 7 × 4\n   year avg_pct_margin avg_weighted_pct_margin med_pct_margin\n  &lt;dbl&gt;          &lt;dbl&gt;                   &lt;dbl&gt;          &lt;dbl&gt;\n1  2000         -0.170                 0.00517         -0.175\n2  2004         -0.212                -0.0248          -0.226\n3  2008         -0.148                 0.0726          -0.162\n4  2012         -0.205                 0.0395          -0.234\n5  2016         -0.308                 0.0210          -0.376\n6  2020         -0.313                 0.0445          -0.380\n7  2024         -0.323                -0.0279          -0.375\n\n\nIt wouldn’t be a introductory stats class if I didn’t mention the mode, which is the single value that appears most commonly in the data. The mode is not very useful for continuous variables. For example, in our total votes variable, there are 16,941 distinct values observed. The most common is 3,930, as there just happen to be eight counties where this was the exact total of votes. Just because we get this exact total in eight out of 22,000+ observations doesn’t make it “typical” in any meaningful sense.\n\ndf_county_pres |&gt;\n  group_by(total_votes) |&gt;\n  summarize(number = n()) |&gt;\n  arrange(desc(number))\n\n# A tibble: 16,941 × 2\n   total_votes number\n         &lt;dbl&gt;  &lt;int&gt;\n 1        3930      8\n 2        1017      7\n 3        1369      7\n 4        2031      6\n 5        3906      6\n 6         928      5\n 7        1110      5\n 8        1390      5\n 9        1434      5\n10        1440      5\n# ℹ 16,931 more rows\n\n\n\n\n3.2.2 Unordered categorical variables\nA categorical variable is one with a discrete set of possible values. Usually these are stored as character strings in a data frame, though sometimes (like we saw in the previous unit with the “outcome” variable in the crisis data) people use numerical codes to represent different categories.\nWe say a categorical variable is unordered when none of the categories is “greater” or “less” than the others. For example, the region variable in our county-level election data has four categories: Midwest, Northeast, South, and West. Because no region is “more” or “less” than the others, this variable is unordered.\n\ndf_county_pres |&gt;\n  group_by(region) |&gt;\n  summarize(number = n())\n\n# A tibble: 4 × 2\n  region    number\n  &lt;chr&gt;      &lt;int&gt;\n1 Midwest     7391\n2 Northeast   1560\n3 South       9934\n4 West        3208\n\n\nYou can’t calculate a mean for an unordered categorical variable, because you can’t add up the values. Nor can you calculate a median, because you can’t put them in order. The only calculation you can make is the mode—the category with the most observations. In the example here, the modal region is the South.\nI find it more informative to look at the distribution of values of an unordered categorical variable: what proportion of values fall into each category?\n\n\n\n\n\n\nIn-class exercise\n\n\n\nAdd a proportion column to the above table of frequency counts for census region, then order it from highest proportion to lowest.\n\n#\n# [Write your answer here]\n#\n\n\n\n\n\n3.2.3 Ordered categorical variables\nA categorical variable is ordered when the categories can be put in order from “least” to “most”. For example, every election cycle the Cook Political Report issues qualitative ratings of Congressional races:\n\nLikely Republican\nLean Republican\nToss-up\nLean Democrat\nLikely Democrat\n\nOur county-level presidential elections data here contains an analogue of the Cook ratings in the Competitiveness column. It’s a 7-point scale where the lowest values most strongly favor Republicans, while the highest values most strongly favor Democrats.\n\ndf_county_pres |&gt;\n  group_by(competitiveness) |&gt;\n  summarize(\n    number = n(),\n    avg_dem_margin = mean(pct_margin),\n    med_dem_margin = median(pct_margin)\n  )\n\n# A tibble: 7 × 4\n  competitiveness number avg_dem_margin med_dem_margin\n            &lt;dbl&gt;  &lt;int&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1              -3  12891       -0.444         -0.429  \n2              -2   2631       -0.152         -0.151  \n3              -1   1371       -0.0714        -0.0719 \n4               0   1502       -0.00178       -0.00250\n5               1    916        0.0691         0.0686 \n6               2   1073        0.147          0.146  \n7               3   1709        0.405          0.329  \n\n\nWith an ordered categorical variable, we can look at the mode and the distribution just like we did with an unordered categorical variable. We still can’t calculate a mean, because we can’t sensibly add and subtract values of a non-continuous variable. However, unlike with an unordered categorical variable, here it is also sensible to calculate the median.\n\nmedian(df_county_pres$competitiveness)\n\n[1] -3\n\n\nClosely tracking what we saw with vote shares by county, we see here that the median county leans strongly in favor of Republican candidates.\nWhat if your raw data is coded as a character string — e.g., \"Strong Republican\", \"Likely Republican\", etc.? You can still calculate the mode and the distribution the same way you did with an unordered categorical variable. However, to calculate the median, you’ll need to convert the character categories to numbers using case_when(), then run the median() function on the numerical version.\n\n\n3.2.4 Binary variables\nA binary variable is a categorical variable that has exactly two categories. Binary variables are special because we can essentially treat them as if they were continuous, specifically by treating one category as 0 and the other as 1.\nIn our county-level presidential data, the indicator for whether the Democrat won the state is a binary variable, already coded in 0/1 format.\n\ndf_county_pres |&gt;\n  group_by(dem_win_state) |&gt;\n  summarize(number = n())\n\n# A tibble: 2 × 2\n  dem_win_state number\n          &lt;dbl&gt;  &lt;int&gt;\n1             0  15262\n2             1   6831\n\n\nWith a binary variable coded in 0/1 format, the mean tells us the proportion of observations that are a 1.\n\nmean(df_county_pres$dem_win_state)\n\n[1] 0.309193\n\n\nThis value indicates that in about 33% of the counties in our data, the Democrat won the presidential election in the state that year.\n\n\n\n\n\n\nIn-class exercise\n\n\n\nCreate a new column with a binary variable that indicates whether the Democrat won the given county, rather than the state as a whole. What is its mean, and how do you interpret that?\n\n#\n# [Write your answer here]\n#\n\n\n\nYou can also calculate the median and the mode of a binary variable. These will be the same as each other: a median and mode of 1 if more than 50% of observations are 1, and 0 if less than 50% are.\n\n\nIn the rare case of a variable that’s exactly 50-50, the calculated median depends on precisely which algorithm you use. For our purposes we’re not going to worry about that sort of edge case.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Univariate Analysis</span>"
    ]
  },
  {
    "objectID": "univariate_analysis.html#spread-how-far-from-the-central-tendency-is-typical",
    "href": "univariate_analysis.html#spread-how-far-from-the-central-tendency-is-typical",
    "title": "3  Univariate Analysis",
    "section": "3.3 Spread: How far from the central tendency is typical?",
    "text": "3.3 Spread: How far from the central tendency is typical?\nImagine two hypothetical states, both with 9 counties. Say the average margin for the Democratic candidate across counties is 2% in both states. Furthermore say the median is 2% in both states too. Then they must be pretty politically similar — right?\nNot necessarily. One way to end up with a mean and median of 2% is for every county to be slightly left of center, as in our hypothetical State A.\n\nmargins_state_a &lt;- c(0.01, 0.01, 0.01, 0.01, 0.02, 0.03, 0.03, 0.03, 0.03)\nmean(margins_state_a)\n\n[1] 0.02\n\nmedian(margins_state_a)\n\n[1] 0.02\n\n\nBut you could also end up with these same numbers if the state is wildly polarized between heavily Democratic and heavily Republican counties, as in our hypothetical State B.\n\nmargins_state_b &lt;- c(-0.40, -0.30, -0.25, -0.20, 0.02, 0.20, 0.30, 0.36, 0.45)\nmean(margins_state_b)\n\n[1] 0.02\n\nmedian(margins_state_b)\n\n[1] 0.02\n\n\nIf we only look at central tendency, these two places look the same. When we also examine the spread, as in the dispersion of values around the central tendency, we see important differences. Our hypothetical State B has much more spread in vote margins than our hypothetical State A. The goal for us now is to work through some precise quantitative measures of spread.\nWe’ll only look at measures of spread for continuous variables. Measures of spread for categorical variables do exist, but I honestly don’t find them particularly useful as data summaries.\n\n\nThe measure I’m most familiar with for categorical variables is information entropy. It does come up sometimes in machine learning applications, but not for the analyses we’ll do in PSCI 2300.\n\n3.3.1 Standard deviation\nBy far the most common measure of spread for a continuous variable is the standard deviation. Very loosely, I think of the standard deviation as telling us “What’s a normal difference between a random observation and the average?”\n\n\n\nTable 3.1: Calculating the “surprise factor” for an observation by looking at how many standard deviations away from the mean it is.\n\n\n\n\n\nDistance from mean\nHow surprising?\n\n\n\n\nWithin 1 SD\nNot surprising at all\n\n\n1–2 SDs away\nMildly surprising\n\n\n2–3 SDs away\nRare, but not out of this world\n\n\n3+ SDs away\nSurprisedPikachu.jpg\n\n\n\n\n\n\nWe can calculate the standard deviation in R using the sd() function. Let’s look at variation in total votes by county.\n\nmean(df_county_pres$total_votes)\n\n[1] 44419.82\n\nsd(df_county_pres$total_votes)\n\n[1] 135957.8\n\n\nSo we have a mean of about 44K and a standard deviation of about 136K. The standard deviation is always measured in the same units as the underlying variable. Because our total_votes column is measured in number of votes, so is its standard deviation.\nLet’s see how much of the data falls into my quick categorization from Table 3.1.\n\nvote_mean &lt;- mean(df_county_pres$total_votes)\nvote_sd &lt;- sd(df_county_pres$total_votes)\n\ndf_county_pres |&gt;\n  # New column: How many SDs from the mean is total_votes in this row?\n  mutate(sd_from_mean = (total_votes - vote_mean) / vote_sd) |&gt;\n  # Categorization based on absolute distance from mean\n  mutate(category = case_when(\n    abs(sd_from_mean) &lt; 1 ~ \"Within 1 SD\",\n    abs(sd_from_mean) &lt; 2 ~ \"Within 1-2 SD\",\n    abs(sd_from_mean) &lt; 3 ~ \"Within 2-3 SD\",\n    abs(sd_from_mean) &gt;= 3 ~ \"Within 3+ SD\"\n  )) |&gt;\n  # Count number in each categorization\n  group_by(category) |&gt;\n  summarize(number = n()) |&gt;\n  # Calculate proportion in each categorization\n  mutate(proportion = number / sum(number))\n\n# A tibble: 4 × 3\n  category      number proportion\n  &lt;chr&gt;          &lt;int&gt;      &lt;dbl&gt;\n1 Within 1 SD    20883     0.945 \n2 Within 1-2 SD    624     0.0282\n3 Within 2-3 SD    272     0.0123\n4 Within 3+ SD     314     0.0142\n\n\nHere we see the vast majority of the data, 94% of counties, is within 1 standard deviation of the mean in terms of total votes. Only about 2.8% is within 1–2 SDs of the mean, and less than half of that is within 2–3 SDs.\n\n\n\n\n\n\nIn-class exercise\n\n\n\nCalculate the standard deviation of the Democratic candidate’s percentage margin across counties for each different election in the data. Do you notice any trends? What does this tell us about how elections are changing over time?\n\n#\n# [Write your answer here]\n#\n\n\n\nI will admit that the formula for the standard deviation is kind of intense: \\[\n\\text{sd} = \\sqrt{\\frac{\\sum_{i=1}^N (x_i - \\bar{x})^2}{N - 1}}.\n\\tag{3.1}\\] Here’s what’s going on:\n\nWe take each of the \\(N\\) observations of our variable, \\(x_1, x_2, \\ldots, x_N\\).\nFor each of these observations, \\(x_i\\), we find the difference between it and the mean: \\(x_i - \\bar{x}\\).\nWe square each of those differences, so now they’re each a positive number, namely the squared distance between the observation and the mean: \\((x_i - \\bar{x})^2\\).\nWe take the average of all the squared differences. Well, not quite the average, because for statistical reasons I won’t get into here, we divide the sum by \\(N - 1\\) instead of \\(N\\). But with any moderately large sample, dividing by \\(N - 1\\) has almost the same result as dividing by \\(N\\) anyway.\nWe take the square root of the whole thing so that it’s measured in the same units as the original variable.\n\n\n\n3.3.2 Median absolute deviation\nThe standard deviation shares some of the same problems as the mean, namely a sensitivity to outliers. One single observation can make a huge difference to the standard deviation.\n\n# Create a vector of evenly-spaced hypothetical data points\nx &lt;- seq(from = 1, to = 2, length.out = 20)\nx\n\n [1] 1.000000 1.052632 1.105263 1.157895 1.210526 1.263158 1.315789 1.368421\n [9] 1.421053 1.473684 1.526316 1.578947 1.631579 1.684211 1.736842 1.789474\n[17] 1.842105 1.894737 1.947368 2.000000\n\nsd(x)\n\n[1] 0.3113726\n\n# Take same vector of data, but make a single outlier\ny &lt;- x\ny[20] &lt;- 10\ny\n\n [1]  1.000000  1.052632  1.105263  1.157895  1.210526  1.263158  1.315789\n [8]  1.368421  1.421053  1.473684  1.526316  1.578947  1.631579  1.684211\n[15]  1.736842  1.789474  1.842105  1.894737  1.947368 10.000000\n\nsd(y)  # Will be way higher!\n\n[1] 1.928213\n\n\nThe median absolute deviation is to the standard deviation as the median is to the mean. It is another measure of spread that is less sensitive to outliers. We calculate it by going through the following steps:\n\nCalculate the median of the sample, call it \\(m\\).\nCalculate the distance between each observation and the sample median: \\(|x_i - m|\\).\nTake the median of these distances.\n\nWe can calculate the median absolute deviation in R using the mad() function. There’s a kind of annoying default scaling option to try to make the MAD more similar to the standard deviation; I don’t like that, so I add the argument constant = 1 to turn that off.\n\nmedian(df_county_pres$total_votes)\n\n[1] 11140\n\nmad(df_county_pres$total_votes, constant = 1)\n\n[1] 7762\n\n\nThis means half of the counties in the data have between 3,378 and 18,902 votes (median of 11,140, plus or minus the MAD of 7,762 votes).\n\nmedian_votes &lt;- median(df_county_pres$total_votes)\nmad_votes &lt;- mad(df_county_pres$total_votes, constant = 1)\n\ndf_county_pres |&gt;\n  # Categorize each row:\n  # 1. below median - MAD\n  # 2. above median - MAD, but below median + MAD\n  # 3. above median + MAD\n  mutate(category = case_when(\n    total_votes &lt; median_votes - mad_votes ~ \"lower\",\n    total_votes &lt; median_votes + mad_votes ~ \"middle\",\n    total_votes &gt;= median_votes + mad_votes ~ \"upper\"\n  )) |&gt;\n  # Count by category and calculate proportions\n  group_by(category) |&gt;\n  summarize(number = n()) |&gt;\n  mutate(proportion = number / sum(number))\n\n# A tibble: 3 × 3\n  category number proportion\n  &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;\n1 lower      3355      0.152\n2 middle    11047      0.500\n3 upper      7691      0.348\n\n\n\n\n\n\n\n\nIn-class exercise\n\n\n\nRepeat the previous exercise, on the spread of the Democratic candidate’s percentage margin across counties by year, but now using the MAD instead of the standard deviation. Does the data tell the same big-picture story, or a different one?\n\n#\n# [Write your answer here]\n#\n\n\n\n\n\n3.3.3 Quartiles and other percentages\nAnother common way to gauge the spread of a variable is to look at its quartiles:\n\nThe first/lower quartile is the value that 25% of the data is below and 75% is above.\nThe second quartile is the value that 50% of the data is below and 50% is above. In other words, the second quartile is the median.\nThe third quartile is the value that 75% of the data is below and 25% is above.\n\nWe can calculate quartiles using the quantile() function in R. Yes, they’re called quartiles with an “r”, yet we use quantile() with an “n”. (Typing quartile() for quantile() is one of my most frequent R mistakes.)\n\nquantile(df_county_pres$total_votes)\n\n     0%     25%     50%     75%    100% \n     64    5073   11140   29674 5488998 \n\n\nThis function also helpfully tells us the minimum and maximum. What we see here is that:\n\nThe smallest quarter of counties had 64 to 5,073 votes cast.\nThe next quarter had 5,073 to 11,140 votes cast.\nThe next had 11,140 to 29,674 votes cast.\nThe largest quarter had 28,138 to 5,488,998 votes cast.\n\n\n\nI think the very largest value in the data is an error. It’s the 2024 value for Harris County, TX, home of Houston. The total population of the county is estimated to be about 5,000,000 — which includes children, noncitizens, and others ineligible to vote — and the sum of the Trump and Harris vote totals there is only about 2,750,000. Let this be a lesson on working with real-world data: take a careful look at data quality before trying to draw firm conclusions!\nThe difference between the third and first quartiles is called the interquartile range. You can calculate it in R with the IQR() function. Some people find this value useful, though frankly I don’t.\n\nIQR(df_county_pres$total_votes)\n\n[1] 24601\n\n\nThe quartiles are special cases of percentiles of the data. For any number \\(p\\) between 0 and 100, the \\(p\\)’th percentile of the data is the value that \\(p\\)% of the data is below and \\((100-p)\\)% of the data is above. To calculate a percentile, we can again use the quantile() function. For example, let’s find the 95th percentile, separating the smallest 95% of counties from the largest 5%. Note that we have to specify the argument to quantile() in terms of a proportion (between 0 and 1) rather than a percentage (between 0 and 100).\n\nquantile(df_county_pres$total_votes, 0.95)\n\n     95% \n196344.8 \n\n\nSo we see that 95% counties had less than 196,344 votes cast, while 5% had more.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Univariate Analysis</span>"
    ]
  },
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "4  Data Visualization",
    "section": "",
    "text": "4.1 Data: World Development Indicators\nVisualization is one of the most powerful ways to convey the results of a statistical analysis. With great power comes great responsibility. Here are the things to think about as you construct a data visualization.\nStatistical analysis should be question driven. The goal of a data visualization is not to make something pretty and colorful. It’s to help you and your audience answer some question about the world. Before even starting to write code for a data visualization—or any other form of statistical analysis, for that matter—you need to pin down the question you’re trying to answer.\nYour visualization should suggest an answer. When you make a graph, ask yourself “What story is this graph telling?” Ideally, the story it’s telling—the visual impression it conveys to the audience—should be an answer to the research question you’re asking. If that’s not the story your graph is telling, then either you made the wrong graph, or you need to tweak it in some way to convey the point more clearly.\nAccuracy over attractiveness. Don’t lie with statistics. The real world is messy, and your data visualizations should reflect that. I’m not saying that you shouldn’t think about aesthetics, or that you should just plot millions of raw data points and leave it to the audience to figure it out. But ultimately what you present must be an honest, accurate, good-faith representation of the data you’re working with, even if it might look “better” if you smoothed some things over. As a simple example, if you remove outliers from your plot to make it easier to see the central portion of the data, you should make it clear in an axis label or caption that you did so.\nOne more note on data visualization in R. Sometimes the way a graph looks in the RStudio window is slightly different than how it shows up in your rendered Quarto output. I strongly recommend rendering and checking the PDF output every time you create or alter plotting code. This will help you make sure that your problem set looks the way you expect it to, and to catch problems as early as possible.\nTo run all of the code in this chapter, you’ll need to make sure the following packages are installed (in addition to the tidyverse package we’ve been using all along):\nTo check whether a package is installed, try to load it:\nIf you get an error like the one above, install the package by running the following code in the R console.\nOnly run install.packages() at the R console, not within a Quarto file — you only have to run it once ever, and you don’t want R to reinstall the package every time you render the file.\nOur data for this unit comes from the World Development Indicators dataset assembled by the World Bank. The data file is called wdi.csv, available from my website at https://bkenkel.com/qps1/data/wdi.csv.\nThe WDI is a collection of tons of useful statistics about economic development for every country in the world. We’ll be working with a slice of this data, with indicators for each country’s development as of 2019. To keep things manageable, we’ll only be working with a few of the very many variables that are available in the WDI.\nlibrary(\"tidyverse\")\n\ndf_wdi &lt;- read_csv(\"https://bkenkel.com/qps1/data/wdi.csv\")\n\nglimpse(df_wdi)\n\nRows: 215\nColumns: 12\n$ country         &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"American Samoa\",…\n$ iso3c           &lt;chr&gt; \"AFG\", \"ALB\", \"DZA\", \"ASM\", \"AND\", \"AGO\", \"ATG\", \"ARG\"…\n$ year            &lt;dbl&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, …\n$ gdp_per_capita  &lt;dbl&gt; 496.6025, 5460.4305, 4468.4534, 12886.1360, 41257.8046…\n$ gdp_growth      &lt;dbl&gt; 3.9116034, 2.0625779, 0.9000000, -0.4878049, 2.0155476…\n$ population      &lt;dbl&gt; 37856121, 2854191, 43294546, 50209, 76474, 32375632, 9…\n$ inflation       &lt;dbl&gt; 2.3023725, 1.4110908, 1.9517682, NA, NA, 17.0809541, 1…\n$ unemployment    &lt;dbl&gt; 11.185, 11.466, 12.259, NA, NA, 16.497, NA, 9.843, 18.…\n$ life_expectancy &lt;dbl&gt; 62.94100, 79.46700, 75.68200, 72.75100, 84.09800, 63.0…\n$ region          &lt;chr&gt; \"South Asia\", \"Europe & Central Asia\", \"Middle East & …\n$ income          &lt;chr&gt; \"1. Low\", \"3. Upper-middle\", \"3. Upper-middle\", \"4. Hi…\n$ lending         &lt;chr&gt; \"IDA\", \"IBRD\", \"IBRD\", \"Not classified\", \"Not classifi…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data_visualization.html#data-world-development-indicators",
    "href": "data_visualization.html#data-world-development-indicators",
    "title": "4  Data Visualization",
    "section": "",
    "text": "Name\nDefinition\n\n\n\n\ncountry\nName of the country\n\n\niso3c\nISO 3-character country code\n\n\nyear\nYear of observation (2019 for all data here)\n\n\ngdp_per_capita\nGross domestic product per person, in US dollars\n\n\ngdp_growth\nAnnual growth rate of GDP, in percentage points\n\n\npopulation\nPopulation of the country\n\n\ninflation\nInflation rate, in percentage points\n\n\nunemployment\nUnemployment rate, in percentage points\n\n\nlife_expectancy\nAverage life expectancy, in years\n\n\nregion\nThe region of the world the country is in\n\n\nincome\nWorld Bank’s categorization of the country’s income\n\n\nlending\nWorld Bank’s categorization of lending group",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data_visualization.html#ggplot-essentials",
    "href": "data_visualization.html#ggplot-essentials",
    "title": "4  Data Visualization",
    "section": "4.2 ggplot essentials",
    "text": "4.2 ggplot essentials\nThe tidyverse package contains a function ggplot() that we’ll use for all data visualization in this course. The gg in ggplot stands for “grammar of graphics,” ostensibly part of some deep philosophy about how data visualization is supposed to be done. I just use ggplot because it provides relatively easy functions to make the kinds of visualizations I typically want.\nThe code to make a data visualization in ggplot will look something like this:\ndata_frame_name |&gt;\n  ggplot(aes(x = varname, y = other_varname)) +\n  geom_point(size = 4, color = \"red\") +\n  labs(\n    title = \"My amazing plot\",\n    x = \"The independent variable\",\n    y = \"The dependent variable\"\n  )\nIn the first two lines, we specify:\n\nThe data frame that the data being visualized will come from.\nWhich column of the data frame will be plotted on the x-axis (the horizontal axis).\nWhich column of the data frame will be plotted on the y-axis (the vertical axis).\n\n\n\nYou could combine the first two lines into one: ggplot(data_frame_name, aes(x = varname, y = other_varname)). I prefer to use a pipe, in case later I decide I want to filter rows or do other transformations before plotting.\nThe aes() that you’ll see in the second line is short for “aesthetic.” The important thing to know is that whenever you’re telling ggplot() to use information from the data frame that you supplied, you have to use aes() to do so.\nIf all you did was run data_frame_name |&gt; ggplot(aes(x = varname, y = other_varname)), you’d end up with an empty plot. For example:\n\ndf_wdi |&gt;\n  ggplot(aes(x = inflation, y = unemployment))\n\n\n\n\n\n\n\n\nThe subsequent lines tell ggplot() how to actually plot your data. For example, geom_point() tells it to plot the data as individual points. The additional arguments of size and color, as in geom_point(size = 4, color = \"red\"), adjust the look and feel of the plotting process. Once we add the geom_point() call to our plot, we get something that looks like this:\n\ndf_wdi |&gt;\n  ggplot(aes(x = inflation, y = unemployment)) +\n  geom_point(size = 4, color = \"red\")\n\nWarning: Removed 46 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nAs you’ll see in the R output for the example here, ggplot() spits out a warning whenever the variables you’re plotting have missing data. Don’t worry when you see this warning — unless you expected that nothing would be missing, in which case you should go back and investigate further.\nIf you’re planning to present your data visualizations to other people — including in your problem sets for this course — you need to label the axes and provide a title so the audience understands what they’re looking at. The labs() function accomplishes this.\n\ndf_wdi |&gt;\n  ggplot(aes(x = inflation, y = unemployment)) +\n  geom_point(size = 4, color = \"red\") +\n  labs(\n    title = \"The global macroeconomy in 2019\",\n    subtitle = \"Back before the post-pandemic economy reminded everyone that they hate inflation\",\n    x = \"Inflation rate (percentage points)\",\n    y = \"Unemployment rate (percentage points)\"\n  )\n\nWarning: Removed 46 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nOne last optional note on plotting and aesthetics. I really don’t like the big ugly gray box that ggplot puts behind every graph by default. I prefer the theme provided by the (unfortunately stupidly named) cowplot package. So before I start making ggplots, I load this package and set its theme as the default.\n\n# Load cowplot and default to it\nlibrary(\"cowplot\")\ntheme_set(\n  theme_cowplot()\n)\n\n# Same code now produces a less hideous-looking plot\ndf_wdi |&gt;\n  ggplot(aes(x = inflation, y = unemployment)) +\n  geom_point(size = 4, color = \"red\") +\n  labs(\n    title = \"The global macroeconomy in 2019\",\n    subtitle = \"Back before the post-pandemic economy reminded everyone that they hate inflation\",\n    x = \"Inflation rate (percentage points)\",\n    y = \"Unemployment rate (percentage points)\"\n  )\n\nWarning: Removed 46 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIf I gridlines for readability, I’ll add a background_grid() specifying which axes I want them on.\n\ndf_wdi |&gt;\n  ggplot(aes(x = inflation, y = unemployment)) +\n  geom_point(size = 4, color = \"red\") +\n  labs(\n    title = \"The global macroeconomy in 2019\",\n    subtitle = \"Back before the post-pandemic economy reminded everyone that they hate inflation\",\n    x = \"Inflation rate (percentage points)\",\n    y = \"Unemployment rate (percentage points)\"\n  ) +\n  background_grid(\"xy\")\n\nWarning: Removed 46 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nEven by the end of this chapter, we will have only scratched the surface of the plotting options available in ggplot(). You can look at the extensive ggplot documentation if you want a comprehensive guide to all of the plotting functions and their arguments. Personally, I find the documentation so overwhelming that I typically just ask ChatGPT which ggplot functions I need to accomplish what I’m looking for.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data_visualization.html#visualizing-a-single-variable",
    "href": "data_visualization.html#visualizing-a-single-variable",
    "title": "4  Data Visualization",
    "section": "4.3 Visualizing a single variable",
    "text": "4.3 Visualizing a single variable\n\n4.3.1 Histograms\nA histogram lets us look at the distribution of a continuous variable. For example, let’s take a look at the distribution of GDP growth across countries in 2019.\n\ndf_wdi |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 7 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nA histogram shows how much of the data falls into different slices, or “bins”. Looking at the histogram here, we see that a majority of countries had positive economic growth in 2019, but a substantial minority had negative growth. The vast majority of the data falls between about -3% growth and +8% growth, with only a few outliers outside that range.\nA more full-featured, aesthetically pleasing (at least to me), audience-friendly version of the same histogram might look like this:\n\ndf_wdi |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(color = \"black\", fill = \"lightblue\", binwidth = 1) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(\n    x = \"GDP growth (percentage points)\",\n    y = \"Number of countries\",\n    title = \"Worldwide economic growth in 2019\"\n  ) +\n  background_grid(\"x\")\n\nWarning: Removed 7 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nHere’s what I added to the original plot code:\n\nArguments to geom_histogram() changing the look of the histogram itself\n\ncolor: outline color for each bar\nfill: color of each bar\nbinwidth: width of each bar, which I set to 1 percentage point to make it a bit easier to interpret\n\ngeom_vline() to add a vertical line separating negative (below 0) from positive (above 0) growth, additionally specifying that the line should be dashed\nlabs() to change the x- and y-axis labels, as well as add a title\nbackground_grid() adding guides along the x-axis\n\n\n\n\n\n\n\nFurther plots are not presentation-quality\n\n\n\nIn order to reduce distraction and focus on the critical part of each chunk of ggplot code, I am not doing things like adding titles, editing axis labels, adding colors, or changing the theme in the remainder of the plots here. This is a “Do as I say, not as I do” situation. The data visualizations you present, both in your problem sets and just generally in your professional life, should have quality axis labels, titles, and aesthetics.\n\n\nSometimes you will see a density plot used in place of a histogram. To make a density plot, just replace geom_histogram() with geom_density().\n\ndf_wdi |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_density()\n\nWarning: Removed 7 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nThere are two main differences between a density plot and a histogram:\n\nInstead of putting the data in discrete bins, the density plot uses statistical calculations to “smooth” the distribution.\nInstead of the y-axis telling how many observations fall into each bin, it is scaled so that the total area under the curve equals 1.0. (In case you’re curious, this is to mimic a probability density function, hence the name “density plot.”)\n\nWhen I’m truly just plotting the distribution of a single variable, I prefer a histogram over a density plot because it’s easier to interpret. However, the smoothness of density plots can make them better for overlaid comparisons of the distribution of a variable across groups, as we’ll see below.\n\n\n4.3.2 Bar charts\nThe best way to visualize the distribution of a categorical variable is with a simple bar chart. In non-academic sources you’ll often see circular pie charts used to visualize the distribution of categories. Scientists don’t like pie charts, in part because it’s much harder for the human brain to compare slice areas than it is to compare bar heights, and in part because pie charts are poorly suited for variables with many categories.\nTo create a bar chart for a categorical variable in ggplot(), we set the x-axis to be the variable we want to visualize, and then use the geom_bar() geometry. As an example, let’s look at the distribution of regions for the countries in our WDI data.\n\ndf_wdi |&gt;\n  ggplot(aes(x = region)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nThe default output here is unreadable because the axis labels overlap. That means we need to work more — if ggplot() (or whatever data visualization software you’re using) spits out incomprehensible output, you don’t get to say “Welp, that’s just what the computer told me” and move on. If you’re planning to show this data visualization to other people, then you need to put in the work to make the computer give you something both readable and accurate.\nOne option would be to modify the data frame to shorten the axis labels, perhaps using case_when() within mutate(). But it would be even easier to reorient the bar chart so the labels are on the y-axis instead of the x-axis.\n\ndf_wdi |&gt;\n  ggplot(aes(y = region)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nThe bars here represent the raw number of observations in each category. What if we wanted the percentage or proportion instead? Then we need to edit our code a bit:\n\nGroup and summarize to calculate the proportion of observations in each category.\nUse geom_col() in place of geom_bar(), being sure to instruct ggplot to put our proportion values on the x-axis.\n\n\ndf_wdi |&gt;\n  group_by(region) |&gt;\n  summarize(number = n()) |&gt;\n  mutate(proportion = number / sum(number)) |&gt;\n  ggplot(aes(x = proportion, y = region)) +\n  geom_col()\n\n\n\n\n\n\n\n\nIf you don’t look carefully at the x-axis, this looks just like the previous bar chart. The difference is that now the x-axis represents proportions. For example, you can tell immediately that Europe and sub-Saharan Africa each account for more than 20% of countries.\nTo see the proportions and the raw number in each category, you can use geom_text() to overlay the raw numbers. The hjust = -0.3 argument puts the numbers just after each bar ends (I had to fiddle around with trial and error to get the right value).\n\ndf_wdi |&gt;\n  group_by(region) |&gt;\n  summarize(number = n()) |&gt;\n  mutate(proportion = number / sum(number)) |&gt;\n  ggplot(aes(x = proportion, y = region)) +\n  geom_col(color = \"black\", fill = \"gray70\") +\n  geom_text(aes(label = number), hjust = -0.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChanging the category order and omitting categories\n\n\n\nWhen we put a categorical variable on the x-axis, ggplot() will default to going in alphabetical order from left to right. When we put one on the y-axis, it defaults to alphabetical order from bottom to top. I find the y-axis behavior annoying, as I think we are more naturally inclined to read from top to bottom.\nIf you want to change the order of labels on the x- or y-axis, use xlim() or ylim() respectively. You can also use these functions if you only want to plot a subset of categories — just omit the label for any category you don’t want to appear.\n\n# Alphabetical order from top to bottom\nregion_labels &lt;- unique(df_wdi$region)  # list of region names\nregion_labels &lt;- sort(region_labels)    # put in alphabetical order\nregion_labels &lt;- rev(region_labels)     # reverse alphabetical order\n\ndf_wdi |&gt;\n  ggplot(aes(y = region)) +\n  geom_bar() +\n  ylim(region_labels)\n\n\n\n\n\n\n\n# Bespoke ordering (remember it'll go bottom to top)\n# Note: any category not named in ylim() will be dropped from plot\ndf_wdi |&gt;\n  ggplot(aes(y = region)) +\n  geom_bar() +\n  ylim(\n    \"North America\", \"Latin America & Caribbean\",\n    \"Europe & Central Asia\", \"South Asia\"\n  )\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_count()`).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data_visualization.html#visualizing-relationships",
    "href": "data_visualization.html#visualizing-relationships",
    "title": "4  Data Visualization",
    "section": "4.4 Visualizing relationships",
    "text": "4.4 Visualizing relationships\nSo far we have just seen how to visualize the distribution of a single variable. But data visualization is an especially powerful tool to learn and communicate about relationships between two or more variables.\n\n4.4.1 Between categorical variables\nThe World Bank categorizes countries into four levels: low income, lower middle income, upper middle income, and high income. Let’s look for regional differences in the distribution of these income categories.\nOne way to visualize the distribution of income categories across regions is to make a separate bar chart of income levels for each region. The facet_wrap() command makes this easy. To make a separate plot for each level of a variable, just add facet_wrap(~ variable_name) to your series of ggplot commands. In our case, the variable we want to use is region.\n\ndf_wdi |&gt;\n  ggplot(aes(y = income)) +\n  geom_bar() +\n  facet_wrap(~ region) +\n  panel_border()  # optional: thin border around each facet\n\n\n\n\n\n\n\n\nWe see here that almost all of the countries categorized as “low income” are in sub-Saharan Africa. Europe, Latin America, and North America consist primarily of upper income countries, while South Asia is mostly lower middle income countries. We see the most diversity in income level by country in the Middle East and in East Asia.\nAnother way to visualize the income composition by region is to make a single bar plot of countries across regions, with different colors for different income levels. We saw earlier that you can use the fill argument of geom_bar() to change the color of the bars generally, as in geom_bar(fill = \"lightblue\"). To make different colors depending on a variable, we need to put fill inside aes(), as in geom_bar(aes(fill = variable_name)). In this case, the variable we want to use is income.\n\ndf_wdi |&gt;\n  ggplot(aes(y = region)) +\n  geom_bar(aes(fill = income))\n\n\n\n\n\n\n\n\nThis plot easily lets us compare the number of countries across regions, while showing the income composition of each individual region. It’s a bit harder to make precise comparisons within each region, or to count the number of countries in a particular income category in a particular region. If we’re more interested in those comparisons, but for some reason don’t want to use facet_wrap(), we can make a “dodged” bar plot like in the example below.\n\ndf_wdi |&gt;\n  ggplot(aes(y = region)) +\n  geom_bar(aes(fill = income), position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking dodged bar plots less goofy\n\n\n\nThere are a couple of goofy things about the default dodged bar plot. First, the width of the bar changes when some income categories aren’t represented within a region (e.g., North America, where all three countries are high income). Second, the bars have high income at the bottom and go down from there, while the legend goes in the opposite direction. The following code, using position = position_dodge2(), fixes these issues.\n\ndf_wdi |&gt;\n  ggplot(aes(y = region)) +\n  geom_bar(\n    aes(fill = income),\n    position = position_dodge2(\n      preserve = \"single\",\n      reverse = TRUE\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.2 Between a categorical variable and a continuous variable\nHow does the rate of economic growth vary across regions of the world? Above we saw how to visualize the overall distribution of growth using histograms. One way to analyze region-by-region variation would be to make a separate histogram for each region, using facet_wrap() like we did when visualizing the relationship between two categorical variables.\n\ndf_wdi |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 1.0) +\n  facet_wrap(~ region, scales = \"free_y\")\n\nWarning: Removed 7 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThe scales = \"free_y\" argument in facet_wrap() allows each “facet” of the plot to have a different y-axis. Without this, the bars would be much shorter for regions like North America and South Asia that don’t have as many countries, making it harder to visually gauge the central tendency and spread for these regions with fewer countries.\nYou can also use overlapping density plots to compare the distribution of a continuous variable across categories of a discrete variable. This would probably be too messy with seven overlapping plots, but it can be useful for two or three categories. For example, let’s use overlapping density plots to compare the distribution of growth rates in East Asia to those in South Asia.\n\ndf_wdi |&gt;\n  filter(region == \"East Asia & Pacific\" | region == \"South Asia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_density(\n    aes(fill = region),  # different colors for different regions\n    alpha = 0.25,        # 75% transparency\n    color = \"black\"      # black outline\n  )\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nThere are a couple of evident takeaways from this overlapping density plot. First, average growth is a bit higher in South Asia than in East Asia. Second, the spread of the distribution of growth rates is wider in East Asia than in South Asia.\nThere are more compact ways to view how the distribution of a continuous variable varies across levels of a categorical variable. One of these is a box and whiskers plot, also called a box plot.\n\ndf_wdi |&gt;\n  ggplot(aes(x = gdp_growth, y = region)) +\n  geom_boxplot() +\n  background_grid(\"x\", minor = \"x\")  # \"minor\": adds thinner gridlines at halfway points\n\nWarning: Removed 7 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nEach entry in the box plot gives us at least five pieces of information:\n\nThe thick line represents the median. For example, this plot shows that median growth in South Asia was just below 5%.\nThe rectangular “box” shows the 25th and 75th percentiles. For example, the 25th percentile in Latin America was about 0% growth, and the 75th percentile was about 2.5%. (If you forgot what “percentile” means, go back and reread Section 3.3.3.)\nThe “whisker” lines extend between:\n\nThe lowest value that is no more than 1.5 IQR below the 25th percentile. (IQR = distance between 75th and 25th percentile.)\nThe highest value that is no more than 1.5 IQR above the 75th percentile.\n\nFor example, other than a couple of low outliers (see below), observed growth rates in sub-Saharan Africa ranged from -2.5% to just below 10%.\nAny observed values that are outside the “whisker” range are plotted individually. These can be thought of as outliers. For example, while the vast majority of countries in East Asia and the Pacific had growth rates between -3% and 10%, there was one outlier with growth below -10%, another with growth around 13.5%, and one last one with growth above 20%.\n\nThe violin plot is a compromise between the box plot and the faceted histogram. Like the box plot, it puts all of the categories together in one plot, rather than splitting up into many tiny plots. But like the histogram (or a density plot), it provides a visualization of the full distribution of the continuous variable across each category level, rather than using a few statistics to summarize the distribution.\n\ndf_wdi |&gt;\n  ggplot(aes(x = gdp_growth, y = region)) +\n  geom_violin() +\n  background_grid(\"x\", minor = \"x\")\n\nWarning: Removed 7 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n\n\n\nI personally think the choice between a box plot and a violin plot is a matter of judgment and aesthetics. It depends on what point about the data you’re trying to convey. Non-scientific audiences have trouble with both types of visualization in my experience, but probably less with box plots than with a violin plot.\nAll of the methods we have looked at so far involve visualizing the full distribution of the continuous variable across each different level of the categorical variable. Sometimes you may want to visualize a simpler summary, like just the mean and standard deviation of the continuous variable across categories.\nTo visualize this kind of summary across groups, first we need to calculate the statistics we want for each category. For example, let’s calculate the average growth rate and the standard deviation for each region.\n\ndf_wdi |&gt;\n  group_by(region) |&gt;\n  summarize(\n    avg_growth = mean(gdp_growth, na.rm = TRUE),\n    sd_growth = sd(gdp_growth, na.rm = TRUE)\n  )\n\n# A tibble: 7 × 3\n  region                     avg_growth sd_growth\n  &lt;chr&gt;                           &lt;dbl&gt;     &lt;dbl&gt;\n1 East Asia & Pacific             3.69       5.47\n2 Europe & Central Asia           3.21       1.82\n3 Latin America & Caribbean       1.77       2.80\n4 Middle East & North Africa      0.979      4.10\n5 North America                   1.60       1.17\n6 South Asia                      4.66       2.68\n7 Sub-Saharan Africa              3.49       3.37\n\n\n\n\n\n\n\n\nRemoving missing data\n\n\n\nMissing data in an R data frame is marked with a special value called NA. Any kind of mathematical operation with an NA value will produce an NA.\n\n1 + NA\n\n[1] NA\n\n0 * NA\n\n[1] NA\n\nmean(c(1, 2, 3, NA, 5, 6))\n\n[1] NA\n\n\nSo if you take a mean() or median() or sd() and it returns NA, that means your input data has at least one missing value. If you want to ignore the missing values and calculate the average (or median or standard deviation…) of the non-missing values, just add the argument na.rm = TRUE to the function call, as in the code block above.\n\n\nAfter calculating the summary statistics we want for each group, we will pipe the output into ggplot() to make a bar plot. Since we’re plotting a statistic we computed rather than counting up group membership, we will use geom_col() for the bar plot. We’ll also add the geom_errorbar() function to plot \\(\\pm 1\\) standard deviation around each mean.\n\ndf_wdi |&gt;\n  group_by(region) |&gt;\n  summarize(\n    avg_growth = mean(gdp_growth, na.rm = TRUE),\n    sd_growth = sd(gdp_growth, na.rm = TRUE)\n  ) |&gt;\n  ggplot(aes(x = avg_growth, y = region)) +\n  geom_col() +\n  geom_errorbar(\n    aes(\n      xmin = avg_growth - sd_growth,\n      xmax = avg_growth + sd_growth\n    ),\n    width = 0.2\n  )\n\n\n\n\n\n\n\n\nThis is … ok. Bar plots can get weird when negative values are possible, as is the case with growth. The average growth is positive in each region, but the standard deviations are large enough that the error bars cross over into the negatives, making the plot overall somewhat hard to comprehend.\nA better choice for summary statistics is often a dot plot, where we plot the same information as in a bar chart, but using dots instead of solid bars. We can use geom_point() to make a simple dot plot without error bars, or geom_pointrange() if we want to be a bit fancier.\n\n## No error bars\ndf_wdi |&gt;\n  group_by(region) |&gt;\n  summarize(\n    avg_growth = mean(gdp_growth, na.rm = TRUE),\n    sd_growth = sd(gdp_growth, na.rm = TRUE)\n  ) |&gt;\n  ggplot(aes(x = avg_growth, y = region)) +\n  geom_point() +\n  background_grid(\"x\")\n\n\n\n\n\n\n\n## With error bars\ndf_wdi |&gt;\n  group_by(region) |&gt;\n  summarize(\n    avg_growth = mean(gdp_growth, na.rm = TRUE),\n    sd_growth = sd(gdp_growth, na.rm = TRUE)\n  ) |&gt;\n  ggplot(aes(\n    x = avg_growth,\n    y = region,\n    xmin = avg_growth - sd_growth,\n    xmax = avg_growth + sd_growth\n  )) +\n  geom_pointrange() +\n  background_grid(\"x\")\n\n\n\n\n\n\n\n\nThe second dot plot lets us clearly see the average growth rates and the range of “unsurprising” values (mean \\(\\pm\\) 1 standard deviation) across regions. We see here that average growth is highest in South Asia. South Asia is also one of three regions — the others being Sub-Saharan Africa, North America, and Europe & Central Asia — where a negative growth rate would be more than one standard deviation below average. The lowest growth rates on average are in the Middle East & North Africa.\n\n\n4.4.3 Between continuous variables\nIn general, the best way to visualize a relationship between two continuous variables is with a scatterplot. We use geom_point() to create scatterplots, as in the following example looking at the relationship between GDP per capita and life expectancy.\n\ndf_wdi |&gt;\n  ggplot(aes(x = gdp_per_capita, y = life_expectancy)) +\n  geom_point()\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe point way up in the top right shows us that there was a country with a roughly $170K per capita GDP, where the life expectancy was almost 85 years old. At the other extreme, down in the bottom left, we see a cluster of countries with per capita GDP close to $0, with a life expectancy below 60 years old.\nA scatterplot can give us a good idea of whether the correlation between two continuous variables is positive or negative.\n\nPositively correlated: Above-average values of \\(x\\) tend to go with above-average values of \\(y\\), and below-average values of \\(x\\) tend to go with below-average values of \\(y\\).\nIn a scatterplot of positively correlated variables, the majority of the data will be in the top-right and bottom-left quadrants.\nNegatively correlated: Above-average values of \\(x\\) tend to go with below-average values of \\(y\\), and below-average values of \\(x\\) tend to go with above-average values of \\(y\\).\nIn a scatterplot of negatively correlated variables, the majority of the data will be in the top-left and bottom-right quadrants.\nNo correlation is when knowing whether \\(x\\) is above or below average tells you little about whether \\(y\\) is above or below average, and vice versa.\nIn a scatterplot of variables with no correlation, the amount of data in the different quadrants will be roughly equal.\n\nGDP per capita is positively correlated with life expectancy. In other words, countries with above-average GDP per capita tend to have above-average life expectancy. To see this, let’s overlay lines indicating the average GDP per capita and life expectancy in the data.\n\navg_gdp &lt;- mean(df_wdi$gdp_per_capita, na.rm = TRUE)\navg_life &lt;- mean(df_wdi$life_expectancy, na.rm = TRUE)\n\ndf_wdi |&gt;\n  ggplot(aes(x = gdp_per_capita, y = life_expectancy)) +\n  geom_point() +\n  geom_vline(xintercept = avg_gdp, linetype = \"dashed\") +\n  geom_hline(yintercept = avg_life, linetype = \"dashed\")\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe see here that there is plenty of data in the top-right (above average on both GDP per capita and life expectancy) and bottom-left (below average on both). There is also a decent amount of data in the upper-left (below average GDP per capita, but above-average life expectancy). However, there is very little data in the bottom-right—every country with an above-average GDP per capita has an above-average life expectancy, except for three that are just slightly below average.\nAnother way to visualize the correlation is to overlay a trend line. We can do this by adding the geom_smooth() function.\n\ndf_wdi |&gt;\n  ggplot(aes(x = gdp_per_capita, y = life_expectancy)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe blue line gives us a predicted value of \\(y\\) as a function of the value of \\(x\\). For example, for a country with a GDP per capita just above $0, the predicted life expectancy is about 62.5 years old. For a country with a GDP per capita of $25,000, the predicted life expectancy is just below 80. Finally, for a country with a GDP per capita of $50,000, the predicted life expectancy is just above 80. The gray band indicates the amount of uncertainty in the predicted average. In general, the prediction is less certain in ranges with less data, such as in the $100,000+ range for per capita GDP here.\nYou might want to know which countries correspond to which points on the scatterplot. To do that, you can switch out geom_point() for geom_text(), plotting text labels instead of dots. You’ll need to supply the label aesthetic to geom_text() to tell it where to get the labels from. Here, we’ll use the \"iso3c\" column with three-character country name abbreviations.\n\ndf_wdi |&gt;\n  ggplot(aes(x = gdp_per_capita, y = life_expectancy)) +\n  geom_text(aes(label = iso3c))\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\nBut with so many overlapping data points, this plot is tough to make sense of. When the geom_text() output is hard to read like this, I use the geom_text_repel() function from the ggrepel package to label the most distinctive points.\n\nlibrary(\"ggrepel\")\n\ndf_wdi |&gt;\n  ggplot(aes(x = gdp_per_capita, y = life_expectancy)) +\n  geom_point() +\n  geom_text_repel(aes(label = country))\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_text_repel()`).\n\n\nWarning: ggrepel: 200 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\nThe default scatterplot is less useful when one or both variables has a highly skewed distribution, with outliers far away from the central tendency. Think about population, where China and India are each about triple the size of the next-largest country.\n\ndf_wdi |&gt;\n  ggplot(aes(x = population)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIf we try to make a scatterplot of life expectancy against country population, we end up with something that’s very hard to read.\n\ndf_wdi |&gt;\n  ggplot(aes(x = population, y = life_expectancy)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe can deal with this issue by putting population on a logarithmic scale. By default, each “tick” along the \\(x\\)-axis moves us up by the same number of people, which is why China and India are so far out compared to all the other countries. When we use a logarithmic scale, each “tick” instead represents an increase by a factor of ten. Specifically, the distance on the \\(x\\)-axis between a country with 100,000 residents and one with 1,000,000 residents will be the same as between a country with 10,000,000 residents and one with 100,000,000 residents.\n\ndf_wdi |&gt;\n  ggplot(aes(x = population, y = life_expectancy)) +\n  geom_point() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nLooking at it this way, we can see much more clearly that there is close to no correlation between population size and life expectancy.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data_visualization.html#maps",
    "href": "data_visualization.html#maps",
    "title": "4  Data Visualization",
    "section": "4.5 Maps",
    "text": "4.5 Maps\nA lot of time in political science, our data has some kind of geographical component. A natural way to visualize relationships with a geographical variable is by making maps! Making a map requires a bit more work up front than other data visualizations, so let’s get into it.\nLet’s start by loading the two additional packages we’ll need to make maps and work with mapping data.\n\nlibrary(\"maps\")\nlibrary(\"countrycode\")\n\nWe can access the (ugly and difficult to understand) raw mapping data by calling map_data(\"world\"). This returns a data frame with the information that R needs to plot the countries of the world. In order to map variables from the World Development Indicators, we need to merge variables from our df_wdi into this data frame. The annoying part is that the default map data only has full country names, not the standardized 3-character country codes. The following block of code adds those to the map data, then merges in our WDI data.\n\ndf_world_map &lt;- map_data(\"world\") |&gt;\n  as_tibble() |&gt;\n  rename(name = region) |&gt;\n  mutate(iso3c = countryname(name, destination = \"iso3c\")) |&gt;\n  left_join(df_wdi, by = \"iso3c\")\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `iso3c = countryname(name, destination = \"iso3c\")`.\nCaused by warning:\n! Some values were not matched unambiguously: Ascension Island, Azores, Barbuda, Canary Islands, Chagos Archipelago, Grenadines, Heard Island, Madeira Islands, Micronesia, Saba, Saint Martin, Siachen Glacier, Sint Eustatius, Virgin Islands\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\ndf_world_map\n\n# A tibble: 99,338 × 18\n    long   lat group order name  subregion iso3c country  year gdp_per_capita\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n 1 -69.9  12.5     1     1 Aruba &lt;NA&gt;      ABW   Aruba    2019         31096.\n 2 -69.9  12.4     1     2 Aruba &lt;NA&gt;      ABW   Aruba    2019         31096.\n 3 -69.9  12.4     1     3 Aruba &lt;NA&gt;      ABW   Aruba    2019         31096.\n 4 -70.0  12.5     1     4 Aruba &lt;NA&gt;      ABW   Aruba    2019         31096.\n 5 -70.1  12.5     1     5 Aruba &lt;NA&gt;      ABW   Aruba    2019         31096.\n 6 -70.1  12.6     1     6 Aruba &lt;NA&gt;      ABW   Aruba    2019         31096.\n 7 -70.0  12.6     1     7 Aruba &lt;NA&gt;      ABW   Aruba    2019         31096.\n 8 -70.0  12.6     1     8 Aruba &lt;NA&gt;      ABW   Aruba    2019         31096.\n 9 -69.9  12.5     1     9 Aruba &lt;NA&gt;      ABW   Aruba    2019         31096.\n10 -69.9  12.5     1    10 Aruba &lt;NA&gt;      ABW   Aruba    2019         31096.\n# ℹ 99,328 more rows\n# ℹ 8 more variables: gdp_growth &lt;dbl&gt;, population &lt;dbl&gt;, inflation &lt;dbl&gt;,\n#   unemployment &lt;dbl&gt;, life_expectancy &lt;dbl&gt;, region &lt;chr&gt;, income &lt;chr&gt;,\n#   lending &lt;chr&gt;\n\n\nJust to be clear on what each line of the above code block accomplishes:\n\nmap_data(\"world\") retrieves the data frame of raw mapping data.\nas_tibble() turns it from a standard R data frame into a tidyverse-style “tibble.” I only did this so I could print it in R Markdown without 1000 rows showing up.\nrename(name = region) renames the region column in the mapping data, which actually stores country names rather than regions, to be more sensibly called name instead.\ncountryname(name, destination = \"iso3c\") takes the name column of the mapping data and converts it to the ISO 3-character abbreviation format. Hence the line mutate(iso3c = countryname(...)) takes that output and stores it as a new column called iso3c.\nleft_join(df_wdi, by = \"iso3c\") merges in the variables from the df_wdi data frame, matching observations according to the iso3c column.\n\nWe’re ready to make a map! For example, let’s color code each country according to the WDI’s classification of income groups.\n\ndf_world_map |&gt;\n  ggplot(aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = income), color = \"black\", linewidth = 0.1) +\n  coord_fixed(1.3) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nggplot(aes(x = long, y = lat, group = group)) tells ggplot() to put longitude (long) on the \\(x\\)-axis, latitude (lat) on the \\(y\\) axis, and to use the group column to separate out different countries (or non-contiguous pieces of countries).\ngeom_polygon() instructs ggplot() to draw the map.\n\naes(fill = income) tells it to color-code each country by income category.\ncolor = \"black\" tells it to draw country borders as black lines.\nlinewidth = 0.1 tells it to draw relatively thin country borders (the default is kind of thick and ugly).\n\ncoord_fixed(1.3) instructs ggplot() to make the \\(x\\)-axis 1.3 times the size of the \\(y\\)-axis, so that the output is proportional to a standard map projection.\ntheme_void() instructs ggplot() not to draw grid lines, axis labels, or any of the other standard plotting elements that we saw on the earlier figures.\n\nIf you don’t like the default choice of colors (and indeed I don’t!), you can use scale_fill_manual() to explicitly set colors for each category, or scale_fill_grey() to do a grayscale spectrum.\n\ndf_world_map |&gt;\n  ggplot(aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = income), color = \"black\", linewidth = 0.1) +\n  coord_fixed(1.3) +\n  theme_void() +\n  scale_fill_grey(start = 0.8, end = 0.2, na.value = \"white\")\n\n\n\n\n\n\n\n\nFinally, if you want to color code by a continuous variable, you can use scale_fill_gradient() to create a spectrum of colors going from low to high.\n\ndf_world_map |&gt;\n  ggplot(aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = unemployment), color = \"black\", linewidth = 0.1) +\n  coord_fixed(1.3) +\n  theme_void() +\n  scale_fill_gradient(low = \"green\", high = \"red\", na.value = \"white\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "correlation_regression.html",
    "href": "correlation_regression.html",
    "title": "5  Correlation and Regression",
    "section": "",
    "text": "5.1 Data: 2020 American National Election Study\nIn the previous section on data visualization (Chapter 4), we started to think about how to analyze relationships between variables. Now we’re going to extend some of those intuitions into more formal statistical calculations.\nTo study correlation and regression, we’ll look at a selection of data from the 2020 American National Election Study. The data file is called anes_2020.csv and is hosted on my website at https://bkenkel.com/qps1/data/anes_2020.csv.\nEach row in the dataset is a different American survey respondent who was randomly sampled for inclusion in the survey. The columns record a variety of demographic and political information about each respondent. There are many more variables available in the raw data — we are only scratching the surface here.\nlibrary(\"tidyverse\")\n\ndf_anes &lt;- read_csv(\"https://bkenkel.com/qps1/data/anes_2020.csv\")\n\nglimpse(df_anes)\n\nRows: 8,280\nColumns: 31\n$ id                   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ state                &lt;chr&gt; \"Oklahoma\", \"Idaho\", \"Virginia\", \"California\", \"C…\n$ female               &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0…\n$ lgbt                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ race                 &lt;chr&gt; \"Hispanic\", \"Asian\", \"White\", \"Asian\", \"Native Am…\n$ age                  &lt;dbl&gt; 46, 37, 40, 41, 72, 71, 37, 45, 70, 43, 37, 55, 3…\n$ education            &lt;chr&gt; \"Bachelor's degree\", \"Some college\", \"High school…\n$ employed             &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1…\n$ hours_worked         &lt;dbl&gt; 40, 40, 0, 40, 0, 0, 30, 40, 0, 30, 25, 50, 0, 50…\n$ watch_tucker         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ watch_maddow         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ therm_biden          &lt;dbl&gt; 0, 0, 65, 70, 15, 85, 50, 50, 85, 85, 100, 60, 0,…\n$ therm_trump          &lt;dbl&gt; 100, 0, 0, 15, 85, 0, 75, 100, 0, 0, 0, 0, 100, 0…\n$ therm_harris         &lt;dbl&gt; 0, 0, 65, 85, 15, 85, 15, 50, 85, 50, 100, 85, 0,…\n$ therm_pence          &lt;dbl&gt; 85, 0, 0, 15, 90, 0, 75, 50, 0, 50, 0, 50, 0, 0, …\n$ therm_obama          &lt;dbl&gt; 0, 50, 90, 85, 10, 60, 15, 50, 60, 100, 100, 85, …\n$ therm_dem_party      &lt;dbl&gt; 0, 0, 60, 50, 20, 85, 15, 50, NA, 60, 100, 50, 0,…\n$ therm_rep_party      &lt;dbl&gt; 85, 50, 0, 70, 70, 15, 75, 100, NA, 50, 0, 30, 85…\n$ therm_feminists      &lt;dbl&gt; 65, 100, 75, 70, 30, 60, 60, 100, 50, 50, 0, 70, …\n$ therm_liberals       &lt;dbl&gt; 30, 0, 75, 70, 10, 70, 0, NA, 30, 50, 50, 70, 0, …\n$ therm_labor_unions   &lt;dbl&gt; 30, 70, 75, 70, 50, 50, 50, 0, 30, 50, 50, 50, 50…\n$ therm_big_business   &lt;dbl&gt; 70, 50, 0, 85, 0, 40, 50, 0, 50, 15, 50, 60, 0, 0…\n$ therm_conservatives  &lt;dbl&gt; 85, 15, 0, 70, 60, 40, 60, NA, 50, 50, 50, 40, 85…\n$ therm_supreme_court  &lt;dbl&gt; 100, 50, 25, 85, 60, 60, 70, 50, 50, 50, 40, 50, …\n$ therm_congress       &lt;dbl&gt; 40, 15, 0, 100, 10, 85, 50, 50, 50, 40, 50, 50, 0…\n$ therm_police         &lt;dbl&gt; 85, 90, 40, 100, 70, 70, 60, 100, 60, 70, 50, 70,…\n$ therm_scientists     &lt;dbl&gt; 100, 70, 100, 85, 60, 85, 85, NA, 60, 50, 85, 100…\n$ contributed_to_party &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ voted                &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1…\n$ voted_for_biden      &lt;dbl&gt; NA, 0, 1, 1, 0, 1, 0, NA, NA, 1, 1, 1, 0, NA, NA,…\n$ voted_for_trump      &lt;dbl&gt; NA, 0, 0, 0, 1, 0, 1, NA, NA, 0, 0, 0, 1, NA, NA,…",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "correlation_regression.html#data-2020-american-national-election-study",
    "href": "correlation_regression.html#data-2020-american-national-election-study",
    "title": "5  Correlation and Regression",
    "section": "",
    "text": "Column name\nVariable type\nDescription\n\n\n\n\nid\nCategorical, unordered\nUnique identifier for each respondent\n\n\nstate\nCategorical, unordered\nThe state where the respondent is registered to vote\n\n\nfemale\nBinary\nWhether the respondent identifies as female (1 = yes)\n\n\nlgbt\nBinary\nWhether the respondent identifies as LGBT (1 = yes)\n\n\nrace\nCategorical, unordered\nRespondent’s racial identification\n\n\nage\nContinuous\nRespondent’s age in years, capped at 80\n\n\neducation\nCategorical, ordered\nHighest level of education that respondent has attained\n\n\nemployed\nBinary\nWhether the respondent worked for pay in the week before the survey (1 = yes)\n\n\nhours_worked\nContinuous\nRespondent’s average hours per week worked over the past year\n\n\nwatch_tucker\nBinary\nWhether the respondent watches Tucker Carlson’s Fox News show (1 = yes)\n\n\nwatch_maddow\nBinary\nWhether the respondent watches Rachel Maddow’s MSNBC show (1 = yes)\n\n\ntherm_*\nContinuous\nRespondent’s “feeling thermometer” (0 = coldest, 100 = warmest) toward various politicians and groups\n\n\ncontributed_to_party\nBinary\nWhether the respondent made a donation to a political party in the 2020 election cycle (1 = yes)\n\n\nvoted\nBinary\nWhether the respondent voted in 2020 (1 = yes)\n\n\nvoted_for_biden\nBinary\nWhether the respondent voted for Joe Biden in 2020 (1 = yes, NA = did not vote)\n\n\nvoted_for_trump\nBinary\nWhether the respondent voted for Donald Trump in 2020 (1 = yes, NA = did not vote)\n\n\n\n\n\n\n\n\n\nWorking with survey data\n\n\n\nWhen working with the ANES data in this course, we are going to treat the dataset as if it were a simple random sample from the population of potential American voters. In particular, we will not use any weighting in our analysis — every data point will be treated the same.\nFor real-world academic or political research with survey data, you need to be much more careful. If certain groups are overrepresented in your sample, you may want to downweight their data so that your final results are representative of the population as a whole (and vice versa for groups that are underrepresented in the sample). To calculate these weights and apply them in statistical research, you’d need to learn techniques that are outside the scope of PSCI 2300.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "correlation_regression.html#correlation",
    "href": "correlation_regression.html#correlation",
    "title": "5  Correlation and Regression",
    "section": "5.2 Correlation",
    "text": "5.2 Correlation\n\n5.2.1 Research question: Feelings toward police and Trump\nPolicing was one of the major topics in the 2020 presidential election. The murder of George Floyd by a police officer in Minneapolis in May 2020 brought attention to the issue of police brutality. The Black Lives Matter movement gained political support, as did proposals ranging from minor police reforms all the way to defunding police departments. These criticisms and policy proposals sparked a backlash, with many politicians — including Donald Trump — making support for the police a key element of their appeal to voters.\nThe question I want to ask today is how strongly was support for the police in 2020 related to support for Donald Trump? For now, we will quantify support using the feeling thermometer scores from the ANES. Scores from 0–49 represent cold feelings, 50 is neutral, and 51–100 is warm. Just to orient ourselves here, let’s calculate the average feeling toward the police and toward Trump in this data.\n\nmean(df_anes$therm_trump, na.rm = TRUE)\n\n[1] 40.44061\n\nmean(df_anes$therm_police, na.rm = TRUE)\n\n[1] 70.57485\n\n\nIn our previous unit on Data Visualization, we saw how to use scatterplots to get an idea of the correlation between two variables. So let’s make a scatterplot of feelings towards the police versus feelings toward Trump.\n\n# Change ggplot theme to cowplot\nlibrary(\"cowplot\")\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\ntheme_set(\n  theme_cowplot()\n)\n\ndf_anes |&gt;\n  ggplot(aes(x = therm_police, y = therm_trump)) +\n  geom_point(position = \"jitter\", alpha = 0.1) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 1090 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1090 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nBefore substantively interpreting this plot, I want to draw your attention to a couple of arguments I put into geom_point() that you haven’t seen before:\n\nposition = \"jitter\" jitters the points on the plot so they don’t all sit on top of each other. This is useful when there is a lot of exact overlap among points on a scatterplot. For example, there are a lot of respondents who say their feeling toward the police is 100 and their feeling toward Trump is 100 too. The jittering lets us see the individual points.\nalpha = 0.1 sets the transparency of the points. The alpha parameter ranges from 0 (completely transparent) to 1 (completely opaque), with a default of 1. I use it here so that the parts of the graph with more data will show up darker, giving us a further idea of how much data lies where. Like jittering, transparency is particularly useful when you have many closely overlapping data points.\n\nNow let’s think about what the scatterplot is telling us. Among respondents with a below-average feeling toward the police, feelings toward Trump also tend to be below-average. Among those with an above-average feeling toward the police, the typical feeling toward Trump tends to be above-average. This is the hallmark of a positive correlation. At the same time, the relationship is by no means one-to-one. There are plenty of respondents who love the police and hate Trump, and a smaller number who hate the police and love Trump.\n\n\n5.2.2 The correlation coefficient\nHow can we more precisely quantify the idea that police support is positively correlated, but only imperfectly, with support for Trump? We will calculate the correlation coefficient, a measure of the strength of the relationship between two continuous variables.\n\nThe correlation coefficient takes a value between -1 and 1.\nPositive values represent a positive correlation. The closer the value is to 1, the tighter the relationship is.\nNegative values represent a negative correlation. The closer the value is to -1, the tighter the relationship is.\nA value of zero represents no correlation: the value of one variable has no relationship with whether the other is above- or below-average.\n\nWhat do I mean when I say a correlation closer to 1 (or -1) represents a “tighter” relationship? A picture of some hypothetical datasets might illustrate it best:\n\n\n\n\n\n\n\n\n\nNow let’s calculate the correlation between feelings toward police and feelings toward Trump among the 2020 ANES respondents. We do that with the cor() function. Just like mean() and sd(), by default it will return NA in the presence of missing data, so we have to tell it to ignore the missing values.\n\n\nAnnoyingly, unlike the vast majority of statistical functions in R, cor() does not allow us to remove missing data via na.rm = TRUE. Instead, we have to write use = \"complete\" (as in: only use the rows with complete data).\n\ncor(df_anes$therm_police, df_anes$therm_trump, use = \"complete\")\n\n[1] 0.467033\n\n\nWe see that there is a moderate positive correlation between feelings toward police and feelings toward Trump. However, just like the scatterplot showed us, the correlation is telling us that police support is not perfectly predictive of Trump support. If all we know about someone is how they feel about the police, we will only have a so-so guess about how they feel about Trump — better than nothing, but certainly not wildly accurate.\nOne way to interpret the precise value of the correlation coefficient is in terms of “variance explained.” The square of the correlation between \\(x\\) and \\(y\\) represents the proportion of the variance in \\(y\\) that can be accounted for by the value of \\(x\\) (or vice versa). For a correlation of 0.47 like we’re seeing here, this means that about 22% (22% = 0.22 = 0.47^2) of the variation in feelings toward Trump can be “explained” by feelings toward police.\nI’ll give you a more precise definition of “variance explained” once we get into regression. The important thing to understand for now is not to interpret correlation in terms of cause and effect. There could be some third factor (or more likely, a whole lot of outside factors) that are responsible both for one’s feelings toward the police and for one’s feelings toward Trump. A nonzero correlation, or even a strong correlation, doesn’t at all imply that one variable has a causal effect on the other.\nI prefer to think about correlation in terms of prediction. If I know someone’s feeling thermometer toward the police, I can predict their feelings toward Trump 22% better (in a sense that I’ll be more precise about soon) than if I didn’t have any additional information about them. It’s easier to remind ourselves about this in contexts where the causal relationships are simpler. There’s a very strong correlation between me wearing a rain jacket and whether it’s raining in Nashville that day. If you told me that I wore a rain jacket on October 1, I’d say there’s a very solid chance it was raining that day. But obviously me wearing a rain jacket doesn’t cause it to rain in Nashville.\n\n\n5.2.3 Calculating the correlation coefficient\n\n\n\n\n\n\nMath ahead\n\n\n\nProfessor’s note because I know students get anxious about this kind of thing.\n\nYou don’t need to know the precise mathematical formulas for the correlation coefficient for the exams.\nYou do need to know what a Z-score is (number of standard deviations above or below the mean), and to understand at a conceptual level how Z-scores are related to correlation.\n\n\n\nThe formula behind the correlation coefficient is designed to capture whether an above-average value of one variable is predictive of an above- or below-average value of the other variable.\nSay we have \\(N\\) observations of variables \\(x\\) and \\(y\\), so our data consists of a sequence of pairs: \\((x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N)\\). We’ll use \\(\\bar{x}\\) to refer to the mean of \\(x\\), and similarly \\(\\bar{y}\\) to refer to the mean of \\(y\\). We’ll also use \\(s_x\\) to refer to the standard deviation of \\(x\\), and \\(s_y\\) for the standard deviation of \\(y\\).\nWe start by calculating the Z-score for both variables for each observation in our data. The Z-score is simply how many standard deviations above or below the mean the observation is: \\[Z_{x, i} = \\frac{x_i - \\bar{x}}{s_x}, \\qquad Z_{y, i} = \\frac{y_i - \\bar{y}}{s_y}.\\] For example, if \\(\\bar{x} = 10\\) and \\(s_x = 2\\), and the first observation’s value of \\(x\\) is \\(9\\), then its Z-score for \\(x\\) would be -0.5 (half a standard deviation below the mean): \\[Z_{x, 1} = \\frac{9 - 10}{2} = -\\frac{1}{2}.\\]\nNow let’s think about multiplying together the Z-scores of \\(x\\) and \\(y\\) for a particular observation. This might seem like a weird thing to do, but it’s helpful for our purposes. Remember that a positive correlation means above-average values of \\(x\\) tend to go with above-average values of \\(y\\), and below-average values of \\(x\\) tend to go with below-average values of \\(y\\). Now think about what happens when we multiply Z-scores together:\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(y_i\\)\n\\(Z_{x, i}\\)\n\\(Z_{y, i}\\)\n\\(Z_{x, i} \\cdot Z_{y, i}\\)\n\n\n\n\nabove \\(\\bar{x}\\)\nabove \\(\\bar{y}\\)\n+\n+\n+\n\n\nabove \\(\\bar{x}\\)\nbelow \\(\\bar{y}\\)\n+\n–\n–\n\n\nbelow \\(\\bar{x}\\)\nabove \\(\\bar{y}\\)\n-\n+\n–\n\n\nbelow \\(\\bar{x}\\)\nbelow \\(\\bar{y}\\)\n-\n-\n+\n\n\n\nAltogether this implies:\n\nIf \\(x\\) tends to be above-average when \\(y\\) is above-average, and below-average when \\(y\\) is below-average, the product of Z-scores will usually be positive.\nIf \\(x\\) tends to be below-average when \\(y\\) is above-average, and above-average when \\(y\\) is below-average, the product of Z-scores will usually be negative.\n\nThis observation leads us to the formula for the correlation between \\(x\\) and \\(y\\). It is the mean of the product of the Z-scores: \\[r_{x, y} = \\frac{1}{N - 1} \\sum_{i=1}^N Z_{x, i} \\cdot Z_{y, i}.\\] As with the formula for the standard deviation (Equation 3.1), we divide by \\(N - 1\\) rather than \\(N\\) for statistical reasons that I won’t get into here.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "correlation_regression.html#sec-regression",
    "href": "correlation_regression.html#sec-regression",
    "title": "5  Correlation and Regression",
    "section": "5.3 Regression with one feature",
    "text": "5.3 Regression with one feature\nIf all you knew about someone was their feelings toward police, what would you guess are their feelings toward Trump? The positive correlation between these two variables tells us that we’d guess the Trump score is above its average of 40 if the police score is above its average of 70, and the opposite if the police score is below average. But can we get a more precise answer than that?\n\n5.3.1 The statistical model\nLinear regression is probably the most popular statistical model of a relationship between variables. It is called linear because we will model the relationship between \\(x\\) and \\(y\\) as a linear function. It is called regression for weird historical reasons that actually don’t have to do with the underlying statistics.\n\n\nThere’s a biological phenomenon known as “regression to the mean”. If your parents are much taller than average, then you’ll likely also be taller than average—but not as tall as they are. Early statisticians used the statistical techniques we now call regression to analyze data on how fathers’ heights compared to sons’ heights. Somehow the name stuck, even though “regression” was more of a property of the findings for their particular application than a description of the technique itself.\nIn a regression analysis, the first thing we need to do is to identify the variable we want to explain or predict. In this case, that’s feelings toward Trump. This is most often called the dependent variable, though to avoid accidentally slipping into cause-and-effect language I prefer to call it the response variable. I’ll usually use \\(y_i\\) to denote the value of the response variable for the \\(i\\)’th observation in the data.\nNext up is to identify the variable(s) we wish to use in order to explain or predict the response. For our research question here, that’s feelings toward the police. These are often called independent variables in social science, though again to avoid causal language I’ll call them features. When there is only one feature, like we have for now, I’ll use \\(x_i\\) to denote its value for the \\(i\\)’th observation. Once we start working with multiple features, say \\(K\\) of them, I’ll write \\(x_{i, 1}\\) for the value of the first feature for the \\(i\\)’th observation, \\(x_{i, 2}\\) for the second feature, and so on up to \\(x_{i, K}\\).\nLinear regression “assumes” a linear relationship between the feature and the response. I put “assumes” in scare quotes because we usually know the relationship isn’t truly linear — it’s more accurate to say we’re using a linear approximation. In any case, the formula is \\[y_i \\approx \\alpha + \\beta x_i.\\] In this formula, \\(y_i\\) and \\(x_i\\) are the “knowns” — the values of the variable we observe in our data. The “unknowns” \\(\\alpha\\) and \\(\\beta\\) will be estimated using a statistical formula.\n\n\\(\\alpha\\) is the intercept. It represents the value we would guess for the response \\(y_i\\) if we knew that the value of the feature \\(x_i\\) were zero.\n\\(\\beta\\) is the slope. It represents how much our predicted value for the response \\(y_i\\) would increase (if \\(\\beta\\) is positive) or decrease (if \\(\\beta\\) is negative) due to a one-unit increase in \\(x_i\\).\n\nWe refer to the intercept and slope together as the regression coefficients.\nI think it’ll be easier to wrap our heads around regression once we see it in practice. We calculate linear regressions in R using the lm() command, where lm stands for “linear model”. The basic syntax of lm() is lm(response ~ feature, data = df), where response and feature are names of columns in df.\n\n#   response   ~  feature(s)  \nlm(therm_trump ~ therm_police, data = df_anes)\n\n\nCall:\nlm(formula = therm_trump ~ therm_police, data = df_anes)\n\nCoefficients:\n (Intercept)  therm_police  \n    -12.7889        0.7476  \n\n\nOur estimate of the intercept is roughly -13, and our estimate of the slope is roughly 0.75:\n\nThe intercept means we would predict a Trump feeling thermometer of -13 for someone whose police feeling thermometer is 0.\nThis might seem like an error, since feeling thermometers only go from 0 to 100! But it’s not an error; it’s a natural limitation of a linear approximation. Regression predictions tend to be most accurate close to the mean of the feature, which in this case is about 70. At the extremes of the feature space, the predictions might be implausibly extreme or even impossible.\nThe slope means that for each one-point increase in police feeling thermometer, our predicted Trump feeling thermometer increases by 0.75 points.\n\nYou can visualize the regression line in ggplot using geom_smooth(method = \"lm\"):\n\ndf_anes |&gt;\n  ggplot(aes(x = therm_police, y = therm_trump)) +\n  geom_point(alpha = 0.1, position = \"jitter\") +\n  geom_smooth(method = \"lm\") +\n  background_grid(\"xy\", minor = \"xy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1090 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1090 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nUsing the formula \\[ThermTrump \\approx -13 + 0.75 ThermPolice,\\] here is what we would predict about a respondent’s Trump opinion given various potential values of their police opinion.\n\n\n\nTable 5.1: Predictions from the linear regression of Trump opinions on police opinions.\n\n\n\n\n\nPolice thermometer\nPredicted Trump thermometer\n\n\n\n\n20\n2\n\n\n40\n17\n\n\n60\n32\n\n\n80\n47\n\n\n100\n62\n\n\n\n\n\n\nSo in order to predict that a respondent feels warmly toward Trump, we’d need their feeling thermometer toward the police to be at least a few points above 80.\nThe table above is an example of extracting predicted values, sometimes also called fitted values, from a regression model. The regression estimates give us a predicted value of the response for each observation in our data, \\[\\hat{y}_i = \\alpha + \\beta x_i.\\] (In case you’re curious, \\(\\hat{y}\\) is pronounced “y hat”.)\n\n\nYou’ll often see “fitted” used when the formula is used to calculate the expected response for data points used to calculate the regression, and “predicted” when the formula is used for new observations not used to calculate the regression coefficients. The distinction between in- and out-of-sample fit is particularly important in machine learning applications, less so for our purposes today.\nThe regression coefficients are calculated to make the predicted values as close as possible to the observed values. The residual is the difference between the observed value and the predicted value, \\[y_i - \\hat{y}_i.\\] A positive residual indicates that the observed value is greater than the prediction. For example, in the model we’ve been working with, if a respondent had a police thermometer of 60 and a Trump thermometer of 40 (compared to the predicted value of 32), the residual for that observation would be \\[y_i - \\hat{y}_i = 40 - 32 = +8.\\] A negative residual indicates the opposite — the observed value is less than the predicted value.\nIf our goal is to make the regression line fall as close as possible to the observed values, we want each residual to be close to 0. The intercept and slope are calculated using a formula that tries to accomplish this goal. The formula is designed to minimize the sum of squared residuals, \\[(y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\cdots + (y_N - \\hat{y}_N)^2 = \\sum_{i=1}^N (y_i - \\hat{y}_i)^2.\\] This way, the formula tries to get each residual as close to 0 as possible. In case you know a bit of calculus and are curious about the details, the optional Section 5.5 shows the formula and how it’s derived.\n\n\n5.3.2 Working with regression results\nWhen you run a regression in R, it calculates a lot more useful information than just the slope and intercept. To access this, you will want to save the regression as a variable. This lets you run additional functions with the regression and extract additional information from it.\n\nfit_trump_police &lt;- lm(therm_trump ~ therm_police, data = df_anes)\n\nfit_trump_police\n\n\nCall:\nlm(formula = therm_trump ~ therm_police, data = df_anes)\n\nCoefficients:\n (Intercept)  therm_police  \n    -12.7889        0.7476  \n\n\nRunning summary() on a regression model object gives you a bunch of additional info about the model:\n\nsummary(fit_trump_police)\n\n\nCall:\nlm(formula = therm_trump ~ therm_police, data = df_anes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-61.972 -32.068  -1.972  34.242 112.789 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -12.7889     1.2503  -10.23   &lt;2e-16 ***\ntherm_police   0.7476     0.0167   44.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35.69 on 7188 degrees of freedom\n  (1090 observations deleted due to missingness)\nMultiple R-squared:  0.2181,    Adjusted R-squared:  0.218 \nF-statistic:  2005 on 1 and 7188 DF,  p-value: &lt; 2.2e-16\n\n\nHere’s what each component of the output means:\n\nCall: The formula you used to estimate the regression. (Useful if you have run a bunch of different regressions and forgot which was which.)\nResiduals: Quantiles of the regression residuals. Here we see that half of the regression predictions are between 32 points below the actual value and 34 points above it. The other half are even further away.\nCoefficients: The most important part of the summary() output, giving us detailed information about the estimates.\n\nEstimate: The estimated value of the coefficient.\nStandard error: A measure of variability in the coefficient estimate, closely related to the concept of a standard deviation. Specifically, this is an estimate of how much we would expect the coefficients to vary if we took a different random sample and ran the same regression model on it. We’ll go into more detail on this in Chapter 6.\nt value: Estimate divided by standard error, used to calculate certain statistical tests.\nPr(&gt;|t|), better known as the p-value: A statistical test that answers a particular (kind of weird) question, namely “How likely is it that a random sample of this size would give us a coefficient at least this far from zero, if in fact the coefficient in the full population were zero?” As with the standard error, we’ll go into further detail later.\n\nResidual standard deviation: The standard deviation of the residuals. Another way to think about “If we use this regression model to predict \\(y\\), how far away is our guess likely to be?” Smaller values indicate more accurate predictions.\n\n\n\n\n\n\nResidual standard deviation terminology\n\n\n\nR calls this the “residual standard error,” which is stupid because it isn’t a standard error. Thanks to R’s poor wording choice, there is no non-confusing way to do this, so I am going to proceed with calling it the residual standard deviation.\n\n\nR-squared: A measure of model fit, specifically the square of the correlation coefficient between the predicted values and the observed values. In a regression with just one feature, this is just the square of the ordinary correlation coefficient.\nF-statistic: Another statistical test that we won’t worry about.\n\nIf you want to extract information about the regression for further calculations or visualization, the easiest way is using the broom package. This is yet another overly cutely named R package, intended to “sweep up” the results of statistical models in R.\n\nlibrary(\"broom\")\n\nThe tidy() function extracts a data frame containing the coefficients, standard errors, and related information.\n\ntidy(fit_trump_police)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -12.8      1.25       -10.2 2.17e-24\n2 therm_police    0.748    0.0167      44.8 0       \n\n\nThe glance() function extracts a data frame containing the R-squared and other model-level information.\n\nglance(fit_trump_police)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.218         0.218  35.7     2005.       0     1 -35904. 71813. 71834.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe values we most care about are \"r.squared\", \"sigma\" (the residual standard deviation), and \"nobs\" (the number of observations used to estimate the regression).\nThe augment() function “augments” the data used to fit the model with regression predictions.\n\naugment(fit_trump_police)\n\n# A tibble: 7,190 × 9\n   .rownames therm_trump therm_police .fitted .resid     .hat .sigma   .cooksd\n   &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 1                 100           85    50.8   49.2 0.000185   35.7 0.000176 \n 2 2                   0           90    54.5  -54.5 0.000222   35.7 0.000259 \n 3 3                   0           40    17.1  -17.1 0.000343   35.7 0.0000395\n 4 4                  15          100    62.0  -47.0 0.000329   35.7 0.000285 \n 5 5                  85           70    39.5   45.5 0.000139   35.7 0.000113 \n 6 6                   0           70    39.5  -39.5 0.000139   35.7 0.0000855\n 7 7                  75           60    32.1   42.9 0.000163   35.7 0.000118 \n 8 8                 100          100    62.0   38.0 0.000329   35.7 0.000187 \n 9 9                   0           60    32.1  -32.1 0.000163   35.7 0.0000660\n10 10                  0           70    39.5  -39.5 0.000139   35.7 0.0000855\n# ℹ 7,180 more rows\n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\nThe \".fitted\" column contains the fitted values, and \".resid\" contains the residuals.\nWe can also use the augment() function to calculate predictions for new data that wasn’t used to estimate the regression. For example, let’s look at the subset of ANES respondents who answered the question about their feelings towards the police but didn’t say how they felt about Trump.\n\ndf_anes_notrump &lt;- df_anes |&gt;\n  filter(is.na(therm_trump)) |&gt;\n  filter(!is.na(therm_police)) |&gt;\n  select(id, therm_police)\n\ndf_anes_notrump\n\n# A tibble: 198 × 2\n      id therm_police\n   &lt;dbl&gt;        &lt;dbl&gt;\n 1    29           70\n 2    74           85\n 3   122           40\n 4   153           70\n 5   179          100\n 6   180          100\n 7   216          100\n 8   241           85\n 9   255           40\n10   323          100\n# ℹ 188 more rows\n\n\nBy using the newdata argument of augment(), we can predict how these respondents feel about Trump based on their feelings toward the police.\n\naugment(fit_trump_police, newdata = df_anes_notrump)\n\n# A tibble: 198 × 3\n      id therm_police .fitted\n   &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n 1    29           70    39.5\n 2    74           85    50.8\n 3   122           40    17.1\n 4   153           70    39.5\n 5   179          100    62.0\n 6   180          100    62.0\n 7   216          100    62.0\n 8   241           85    50.8\n 9   255           40    17.1\n10   323          100    62.0\n# ℹ 188 more rows\n\n\nUsing the interval argument, we can get a rough estimate of how precise these predictions are.\n\naugment(\n  fit_trump_police,\n  newdata = df_anes_notrump,\n  interval = \"prediction\",\n  conf.level = 0.5\n)\n\n# A tibble: 198 × 5\n      id therm_police .fitted .lower .upper\n   &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1    29           70    39.5  15.5    63.6\n 2    74           85    50.8  26.7    74.8\n 3   122           40    17.1  -6.96   41.2\n 4   153           70    39.5  15.5    63.6\n 5   179          100    62.0  37.9    86.0\n 6   180          100    62.0  37.9    86.0\n 7   216          100    62.0  37.9    86.0\n 8   241           85    50.8  26.7    74.8\n 9   255           40    17.1  -6.96   41.2\n10   323          100    62.0  37.9    86.0\n# ℹ 188 more rows\n\n\nconf.level is a number between 0 and 1 specifying the confidence level for the prediction. Here I went with 50%, aka conf.level = 0.5. What this is telling us, for example, is that for a respondent with the same features as our first one (namely, a police feeling thermometer of 70), we’d expect their Trump feeling thermometer to be between 15.5 and 63.6 about half of the time. In other words, there is a lot of uncertainty about these predictions!\nThe augment() function can also be useful for visualizing our regression results. For example, suppose we wanted to plot our prediction + the uncertainty around it for each possible police thermometer that’s a multiple of 5. First we’d create a data frame with the values of the police thermometer we care about. I’ll use the seq() function to generate an evenly spaced sequence of numbers.\n\ndf_police_spaced &lt;- tibble(\n  therm_police = seq(from = 0, to = 100, by = 5)\n)\n\ndf_police_spaced\n\n# A tibble: 21 × 1\n   therm_police\n          &lt;dbl&gt;\n 1            0\n 2            5\n 3           10\n 4           15\n 5           20\n 6           25\n 7           30\n 8           35\n 9           40\n10           45\n# ℹ 11 more rows\n\n\nNow we can calculate predictions with augment() and plot them.\n\nfit_trump_police |&gt;\n  augment(\n    newdata = df_police_spaced,\n    interval = \"prediction\",\n    conf.level = 0.5\n  ) |&gt;\n  ggplot(aes(x = therm_police, y = .fitted)) +\n  geom_pointrange(aes(ymin = .lower, ymax = .upper)) +\n  background_grid(\"xy\", minor = \"xy\")\n\n\n\n\n\n\n\n\n\n\n5.3.3 Correlation and R-squared\nWhen I introduced the correlation coefficient, I mentioned that its square could be thought of as the percentage of variation in one variable that is “explained” by the other. Having worked through regression, we can be much more precise about what that means.\nRemember that the correlation between police feeling thermometer and Trump feeling thermometer is roughly 0.47, so its square is roughly 0.22.\n\ncor_trump_police &lt;- cor(\n  df_anes$therm_trump,\n  df_anes$therm_police,\n  use = \"complete\"\n)\n\ncor_trump_police\n\n[1] 0.467033\n\ncor_trump_police^2\n\n[1] 0.2181198\n\n\nThis lines up exactly with the R-squared value from our regression model.\n\nglance(fit_trump_police)$r.squared\n\n[1] 0.2181198\n\n\nNow I’m going to compare two values:\n\nAverage squared difference between the observed Trump thermometers and the mean Trump thermometer (what we’d use for prediction in the absence of any other information).\nAverage squared regression residual, aka average squared difference between the observed Trump thermometers and the regression predictions.\n\nWhat I want to show you is that the second value is about 78% as large as the first. Why 78%? Because \\(1 - R^2 = 1 - 0.22 = 0.78\\).\n\n# Calculate the two squared differences\naugment(fit_trump_police) |&gt;\n  mutate(dist_from_mean = therm_trump - mean(therm_trump)) |&gt;\n  summarize(\n    sq_dist = mean(dist_from_mean^2),\n    sq_residual = mean(.resid^2)\n  ) |&gt;\n  mutate(ratio = sq_residual / sq_dist)\n\n# A tibble: 1 × 3\n  sq_dist sq_residual ratio\n    &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1   1628.       1273. 0.782\n\n# Compare to 1 - R^2\n1 - cor_trump_police^2\n\n[1] 0.7818802\n\n\nIn this particular sense, we do 22% better at predicting respondents’ opinions toward Trump when we base our prediction on their opinion of the police, compared to just predicting that every single respondent has an average opinion toward Trump.\n\n\n5.3.4 Binary and categorical features\nWhat if we want to predict the response as a function of a binary or categorical feature? The R commands stay pretty much the same — all that changes is the way we interpret the results.\nAs an example of a binary feature, let’s think about modeling each survey respondent’s opinion toward Trump as a function of their gender identity. Remember that gender identity is represented by the female column of our data frame, with 0 representing men and 1 representing women.\n\ndf_anes |&gt;\n  group_by(female) |&gt;\n  summarize(number = n())\n\n# A tibble: 3 × 2\n  female number\n   &lt;dbl&gt;  &lt;int&gt;\n1      0   3763\n2      1   4450\n3     NA     67\n\n\nWe use the same lm(response ~ feature, data = df) syntax to run a regression when the feature is a binary variable.\n\nfit_trump_gender &lt;- lm(therm_trump ~ female, data = df_anes)\n\nsummary(fit_trump_gender)\n\n\nCall:\nlm(formula = therm_trump ~ female, data = df_anes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.897 -37.548  -7.548  41.103  62.452 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  43.8969     0.6622  66.292  &lt; 2e-16 ***\nfemale       -6.3494     0.9022  -7.038 2.12e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.21 on 7990 degrees of freedom\n  (288 observations deleted due to missingness)\nMultiple R-squared:  0.00616,   Adjusted R-squared:  0.006036 \nF-statistic: 49.53 on 1 and 7990 DF,  p-value: 2.118e-12\n\n\nWe end up with an intercept of about 44 and a slope of about -6. To interpret these numbers, let’s think again about the regression formula: \\[\\text{Trump Feeling} \\approx 44 - 6 \\times \\text{female}.\\] We have female = 0 for male respondents, so the formula predicts a Trump thermometer score of 44 for men. We have female = 1 for female respondents, so the formula predicts a Trump thermometer of 38 for women. These values line up exactly with the averages we see for each group in the raw data:\n\ndf_anes |&gt;\n  group_by(female) |&gt;\n  summarize(avg_therm_trump = mean(therm_trump, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  female avg_therm_trump\n   &lt;dbl&gt;           &lt;dbl&gt;\n1      0            43.9\n2      1            37.5\n3     NA            35.3\n\n\nIn general, when we run a regression on a single binary feature:\n\nThe intercept represents the raw prediction when feature = 0.\nThe slope represents the difference in predictions between when feature = 1 and when feature = 0.\n\nNow let’s move on to more general categorical features, with potentially multiple categories. For example, we might want to predict someone’s opinion toward Trump as a function of their educational attainment, represented by the education column in our dataset.\n\ndf_anes |&gt;\n  group_by(education) |&gt;\n  summarize(number = n())\n\n# A tibble: 6 × 2\n  education             number\n  &lt;chr&gt;                  &lt;int&gt;\n1 Bachelor's degree       2055\n2 Graduate degree         1592\n3 High school             1336\n4 Less than high school    376\n5 Some college            2790\n6 &lt;NA&gt;                     131\n\n\nOnce again, the syntax for running a regression is exactly the same. All that changes is how we interpret the results.\n\nfit_trump_educ &lt;- lm(therm_trump ~ education, data = df_anes)\n\nsummary(fit_trump_educ)\n\n\nCall:\nlm(formula = therm_trump ~ education, data = df_anes)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-49.91 -35.55 -13.25  39.14  71.75 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      35.555      0.878  40.496  &lt; 2e-16 ***\neducationGraduate degree         -7.308      1.332  -5.486 4.25e-08 ***\neducationHigh school             14.351      1.415  10.139  &lt; 2e-16 ***\neducationLess than high school   10.498      2.272   4.620 3.90e-06 ***\neducationSome college            10.306      1.160   8.883  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.55 on 7926 degrees of freedom\n  (349 observations deleted due to missingness)\nMultiple R-squared:  0.03764,   Adjusted R-squared:  0.03715 \nF-statistic:  77.5 on 4 and 7926 DF,  p-value: &lt; 2.2e-16\n\n\nYou might notice that we now have an intercept and four coefficients — one for each education category other than “bachelor’s degree.” To understand why we end up with this many coefficients, you need to know what’s happening under the hood in R when you run a regression with a categorical variable. In order to do a statistical analysis, R needs some way to turn the category text into numbers. R does this by turning a categorical variable with \\(K\\) categories into \\(K\\) separate binary variables, one for each category. So if the raw data on the education feature looked like this …\nBachelor's degree\nGraduate degree\nGraduate degree\nHigh school\nHigh school\nHigh school\nLess than high school\nSome college\nSome college\nSome college\n… R would translate it into this set of binary variables before running the regression:\n\n\n# A tibble: 10 × 5\n   `Bachelor's degree` `Graduate degree` `High school` `Less than high school`\n                 &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;                   &lt;dbl&gt;\n 1                   1                 0             0                       0\n 2                   0                 1             0                       0\n 3                   0                 1             0                       0\n 4                   0                 0             1                       0\n 5                   0                 0             1                       0\n 6                   0                 0             1                       0\n 7                   0                 0             0                       1\n 8                   0                 0             0                       0\n 9                   0                 0             0                       0\n10                   0                 0             0                       0\n   `Some college`\n            &lt;dbl&gt;\n 1              0\n 2              0\n 3              0\n 4              0\n 5              0\n 6              0\n 7              0\n 8              1\n 9              1\n10              1\n\n\nGoing back to the regression results, you might notice that there are only four categories listed. What happened to the bachelor’s degree category? Whenever we put a categorical variable into a regression, there will be a single omitted category. This happens for mathematical reasons beyond the scope of this course.\n\n\nFor those who have taken linear algebra: The formula that we use to calculate the regression coefficients involves a matrix inversion, and the relevant matrix is not invertible unless we exclude one of the binary category indicators. See my graduate lecture notes if for some reason you really want to go deep on this.\nThe important thing for you to know is to be able to identify the omitted category — in this case, those whose highest level of educational attainment is a bachelor’s degree. For these respondents, all of the other four binary indicators will equal 0. Our prediction for them will just be the intercept, which in this case is roughly 36. For the other categories, the reported coefficient reflects the difference in the prediction compared to the omitted category. In this case:\n\nThe coefficient of -7 for those with a graduate degree indicates that our predicted Trump thermometer for this category is 7 less than for the omitted category of bachelor’s degree holders. Roughly, this gives us a prediction of 36 - 7 = 29.\nThe coefficient of 14 for those with a high school education indicates that our prediction for high school graduates is 14 higher than for those with a bachelor’s degree: 36 + 14 = 50.\nThe coefficients of 10 for less than high school and for some college indicates that our predictions for these two groups are 10 higher than for those with a bachelor’s degree: 36 + 10 = 46.\n\nOnce again, these predictions line up exactly with the raw averages in the data.\n\ndf_anes |&gt;\n  group_by(education) |&gt;\n  summarize(avg_therm_trump = mean(therm_trump, na.rm = TRUE))\n\n# A tibble: 6 × 2\n  education             avg_therm_trump\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 Bachelor's degree                35.6\n2 Graduate degree                  28.2\n3 High school                      49.9\n4 Less than high school            46.1\n5 Some college                     45.9\n6 &lt;NA&gt;                             41.8\n\n\nBy default, R makes the omitted category whichever one comes first in alphabetical order. Sometimes you might find it easier to interpret the results by omitting a different category. Say we wanted the coefficients to reflect comparisons to those whose highest educational attainment is a high school diploma. You can use the fct_relevel() function to tell R which category to put first, thereby making it the omitted category.\n\ndf_anes &lt;- df_anes |&gt;\n  mutate(education = fct_relevel(education, \"High school\"))\n\nfit_trump_educ_2 &lt;- lm(therm_trump ~ education, data = df_anes)\n\nsummary(fit_trump_educ_2)\n\n\nCall:\nlm(formula = therm_trump ~ education, data = df_anes)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-49.91 -35.55 -13.25  39.14  71.75 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      49.906      1.110  44.953  &lt; 2e-16 ***\neducationBachelor's degree      -14.351      1.415 -10.139  &lt; 2e-16 ***\neducationGraduate degree        -21.659      1.495 -14.483  &lt; 2e-16 ***\neducationLess than high school   -3.853      2.372  -1.624  0.10434    \neducationSome college            -4.046      1.345  -3.009  0.00263 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.55 on 7926 degrees of freedom\n  (349 observations deleted due to missingness)\nMultiple R-squared:  0.03764,   Adjusted R-squared:  0.03715 \nF-statistic:  77.5 on 4 and 7926 DF,  p-value: &lt; 2.2e-16\n\n\nReordering the categories doesn’t change the regression predictions at all, just the way R structures and reports them.\nThe inferential statistics (standard errors and \\(p\\)-values) on the category coefficients should be interpreted in terms of differences with the baseline category. For example, we see a very small \\(p\\)-value on the “Bachelor’s degree” category in the regression above — meaning that if bachelor’s degree holders and high school diploma holders had the same opinion of Trump in the population at large, it would be highly unlikely to draw a sample of this size with as big of a difference as the one we observed.\nOn the other hand, the \\(p\\)-value on the “Less than high school” category is just above 0.10, not statistically significant by the typical political science standard of 0.05. This \\(p\\)-value means that if there were no difference in Trump opinion between high school diploma holders and those who didn’t finish high school in the population at large, there would be about a 10% chance of drawing a sample with as big of a difference as we see here. That’s not incredibly likely, but it’s a great enough chance that we wouldn’t feel confident ruling out the idea of no difference in the population at large.\n\n\n5.3.5 Binary responses\nWe can also use linear models to predict a binary response. For example, instead of modeling mere opinions toward Trump as a function of opinions toward the police, let’s model voting for Trump as a function of these opinions.\nOnce again, the R syntax is the same; the only difference is how we interpret the numbers in the results. When we put a binary response in our regression model, we are essentially modeling the data as \\[\\Pr(\\text{response} = 1) \\approx \\text{intercept} + \\text{slope} \\times \\text{feature}.\\]\n\nIntercept: predicted probability of response = 1 given feature = 0.\nSlope: predicted increase (if positive) or decrease (if negative) in the probability of response = 1 due to a 1-unit increase in feature.\n\n\nfit_vote_police &lt;- lm(voted_for_trump ~ therm_police, data = df_anes)\n\nsummary(fit_vote_police)\n\n\nCall:\nlm(formula = voted_for_trump ~ therm_police, data = df_anes)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68341 -0.40106 -0.09519  0.41071  1.25774 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.2577404  0.0177155  -14.55   &lt;2e-16 ***\ntherm_police  0.0094115  0.0002335   40.31   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4364 on 5842 degrees of freedom\n  (2436 observations deleted due to missingness)\nMultiple R-squared:  0.2176,    Adjusted R-squared:  0.2174 \nF-statistic:  1625 on 1 and 5842 DF,  p-value: &lt; 2.2e-16\n\n\nThe regression equation here is \\[\\Pr(\\text{Voted for Trump}) \\approx -0.25 + 0.01 \\times \\text{Police Feeling}.\\] The slope of 0.01, aka 1%, means that our prediction about the probability that a respondent would vote for Trump goes up 1 percentage point for each point on their feeling thermometer toward the police. In other words, if we compared Person A whose feeling toward the police is 70 to Person B whose feeling is 60, we’d guess that A is about 10% more likely to vote for Trump than B is.\nThe intercept is a bit trickier to interpret. It tells us that for someone whose feeling thermometer toward the police is 0, their predicted probability of voting for Trump is -25% … which is nonsensical. This kind of nonsense prediction is one of the downsides of using a linear model — if you go out far enough to one extreme or the other, you’ll potentially start getting predictions below 0% or above 100%. If you find yourself in a real-world setting where you absolutely need your predictions to be between 0% and 100%, you can use an alternative model like logistic regression. For now, just know that these types of predictions are a natural consequence of the geometry of linear regression.\n\ndf_anes |&gt;\n  ggplot(aes(x = therm_police, y = voted_for_trump)) +\n  geom_point(\n    position = position_jitter(width = 2.5, height = 0.1),\n    alpha = 0.1\n  ) +\n  geom_smooth(method = \"lm\") +\n  scale_y_continuous(\n    breaks = c(0, 1),\n    minor_breaks = seq(0, 1, by = 0.2)\n  ) +\n  background_grid(\"xy\", minor = \"xy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2436 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2436 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYou may wonder about a categorical response with more than two categories. Unlike a binary response, linear regression can’t be naturally adapted to that setting. We’ll need to use more sophisticated classification models to model a (non-binary) categorical response, so we won’t tackle that just yet.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "correlation_regression.html#regression-with-multiple-features",
    "href": "correlation_regression.html#regression-with-multiple-features",
    "title": "5  Correlation and Regression",
    "section": "5.4 Regression with multiple features",
    "text": "5.4 Regression with multiple features\nThere are two reasons to include multiple features in a regression.\n\nIncluding multiple features lets us make all-else-equal statements. In particular, if we want to analyze the relationship between the feature and the response while holding other features fixed, regression gives us a way to do that.\nMore features may increase the predictive accuracy of the model. Essentially, the more information we have about each observation, the more precisely we should be able to predict the value of the response.\n\nAs an example of when we might want to make an all-else-equal statement, let’s think about the role racial identity plays in opinions toward the police and toward Donald Trump. Police brutality disproportionately affects Black and Latino communities, and movements like Black Lives Matter have sought to heighten the salience of the racially disparate impacts of policing, so we might expect overall opinions toward the police to be lower among Black and Latino respondents. Donald Trump also has a history of denigrating both individuals and groups with minority racial identities, so we might expect overall opinions of Trump to be lower among Black and Latino respondents. Putting this all together, is it possible that the relationship we see between police opinions and Trump opinions would go away — or at least be less strong — if we only made comparisons between respondents with the same racial identity?\nRegression allows us to make this kind of all-else-equal comparison, by controlling for additional features. With \\(K\\) features, the linear regression model becomes \\[\\text{response} \\approx \\text{intercept} + \\text{slope}_1 \\times \\text{feature}_1 + \\text{slope}_2 \\times \\text{feature}_2 + \\cdots + \\text{slope}_K \\times \\text{feature}_K.\\] The slope on each individual feature represents: “How much does the predicted value of the response change due to a 1-unit increase in this feature, holding all other feature values fixed?”\nIt’s easy to include multiple features in a regression model in R. Just use lm(response ~ feature1 + feature2 + ...). For example, to control for race when analyzing the relationship between police opinion and Trump opinion:\n\nfit_trump_police_race &lt;- lm(\n  therm_trump ~ therm_police + race,\n  data = df_anes\n)\n\nsummary(fit_trump_police_race)\n\n\nCall:\nlm(formula = therm_trump ~ therm_police + race, data = df_anes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-65.828 -32.854  -1.938  33.062 114.752 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -10.76435    2.52044  -4.271 1.97e-05 ***\ntherm_police          0.70982    0.01737  40.867  &lt; 2e-16 ***\nraceBlack           -11.06104    2.68564  -4.119 3.86e-05 ***\nraceHispanic         -3.98716    2.66588  -1.496    0.135    \nraceMultiracial       1.38913    3.27692   0.424    0.672    \nraceNative American   5.60968    3.69893   1.517    0.129    \nraceWhite             2.36714    2.32122   1.020    0.308    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35.48 on 7113 degrees of freedom\n  (1160 observations deleted due to missingness)\nMultiple R-squared:  0.2283,    Adjusted R-squared:  0.2277 \nF-statistic: 350.8 on 6 and 7113 DF,  p-value: &lt; 2.2e-16\n\n\nWhen we control for race, we still see a substantial relationship between opinions about the police and opinions about Donald Trump. Among individuals of the same racial identity, for each 1-point increase in the police feeling thermometer we would predict about a 0.71-point greater feeling toward Donald Trump. Importantly, unlike our original model, this accounts for the fact that Black and Hispanic respondents tend to have lower opinions toward Trump on average. Even so, the slope is not hugely different from the 0.75 we saw in the uncontrolled regression above.\nSo can we conclude that more favorable attitudes toward the police cause more favorable attitudes toward Donald Trump? Not so fast. Loosely speaking, it’s true that controlled comparisons like this get us closer to cause-and-effect statements than uncontrolled bivariate regressions. But “closer” does not mean “all the way there”. In order to make a precise causal inference from this sort of regression model, we would need to control for every possible confounding variable that might affect both attitudes toward the police and one’s opinion of Donald Trump. Even with a pretty deep survey like the one the ANES fields, it’s pretty unlikely we would ever really be able to control for all of the confounding variables.\nComparing the regressions with and without the race control, we also see that the predictive accuracy is higher with the control. The residual standard deviation is lower, and the \\(R^2\\) is higher.\n\n# w/o race control\nglance(fit_trump_police) |&gt;\n  select(sigma, r.squared)\n\n# A tibble: 1 × 2\n  sigma r.squared\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  35.7     0.218\n\n# w/ race control\nglance(fit_trump_police_race) |&gt;\n  select(sigma, r.squared)\n\n# A tibble: 1 × 2\n  sigma r.squared\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  35.5     0.228\n\n\nThe more information we use to generate our prediction, the more accurate it can be. This isn’t always true in the realm of out-of-sample prediction, where we use a regression model to try to predict outcomes on new data not used to estimate the regression coefficients. However, we won’t get into that issue just yet here.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "correlation_regression.html#sec-regression-formula",
    "href": "correlation_regression.html#sec-regression-formula",
    "title": "5  Correlation and Regression",
    "section": "5.5 Optional: Deriving the regression formula",
    "text": "5.5 Optional: Deriving the regression formula\n\n\n\n\n\n\nFor intellectual edification only\n\n\n\n“Optional” means “No, this won’t be on the test.”\n\n\nThe formula for the regression slope is \\[\\beta = \\frac{\\sum_{i=1}^N x_i (y_i - \\bar{y})}{\\sum_{i=1}^N x_i (x_i - \\bar{x})},\\] where \\(\\bar{x}\\) and \\(\\bar{y}\\) are the means of \\(x_1, x_2, \\ldots, x_N\\) and \\(y_1, y_2, \\ldots, y_N\\) respectively. The formula for the regression intercept is \\[\\alpha = \\bar{y} - \\beta \\bar{x}.\\] In other words, the intercept is calculated to ensure that the predicted value given \\(x_i = \\bar{x}\\) is \\(y_i = \\bar{y}\\); i.e., the regression line must pass through the point \\((\\bar{x}, \\bar{y})\\).\nHow did statisticians arrive at these formulas? Remember that the regression coefficients are chosen to minimize the sum of squared residuals, \\[\\sum_{i=1}^N (y_i - \\hat{y}_i)^2.\\] Let’s rewrite this as an explicit function of the intercept and slope, \\[SSR(\\alpha, \\beta) = \\sum_{i=1}^N (y_i - \\alpha - \\beta x_i)^2.\\]\nIf you’ve taken multivariate calculus, you know that a necessary condition for the minimization of a differentiable function is that its partial derivatives equal zero. (I won’t prove this here, but the SSR function is convex, so the necessary conditions also turn out to be sufficient.) So our estimates of the intercept and slope must satisfy \\[\\begin{alignat*}{2}\n\\frac{\\partial SSR(\\alpha, \\beta)}{\\partial \\alpha} &= -2 \\sum_{i=1}^N (y_i - \\alpha - \\beta x_i) &&= 0, \\\\\n\\frac{\\partial SSR(\\alpha, \\beta)}{\\partial \\beta} &= -2 \\sum_{i=1}^N x_i (y_i - \\alpha - \\beta x_i) &&= 0.\n\\end{alignat*}\\] If we cancel out the -2s and break up the sums, we end up with \\[\\begin{align*}\n\\sum_{i=1}^N y_i - \\sum_{i=1}^N \\alpha - \\beta \\sum_{i=1}^N x_i &= 0, \\\\\n\\sum_{i=1}^N x_i y_i - \\alpha \\sum_{i=1}^N x_i - \\beta \\sum_{i=1}^N x_i^2 &= 0.\n\\end{align*}\\] Let’s start with the first of these equations. \\(\\alpha\\) is a constant, so \\(\\sum_{i=1}^N \\alpha = N \\alpha\\). And remember that the sample mean is defined as \\(\\bar{x} = \\frac{1}{N} \\sum_{i=1}^N x_i\\), which means that \\(\\sum_{i=1}^N x_i = N \\bar{x}\\). For the same reason, \\(\\sum_{i=1}^N y_i = N \\bar{y}\\). Substituting these facts into the first equation turns it into \\[N \\bar{y} - N \\alpha - N \\beta \\bar{x} = 0.\\] Cancelling out \\(N\\) and moving \\(\\alpha\\) to the other side confirms our formula for the intercept, \\[\\alpha = \\bar{y} - \\beta \\bar{x}.\\]\nThis formula for the intercept is only of limited use so far—it relies on the slope \\(\\beta\\), which we haven’t calculated yet. So now let’s work with the second equation we derived up above, \\[\\sum_{i=1}^N x_i y_i - \\alpha \\sum_{i=1}^N x_i - \\beta \\sum_{i=1}^N x_i^2 = 0.\\] By moving the middle term to the other side, we can rearrange this equation to \\[\\sum_{i=1}^N x_i y_i - \\beta \\sum_{i=1}^N x_i^2 = \\alpha \\sum_{i=1}^N x_i.\\] By replacing \\(\\alpha\\) with the formula we derived above, we can expand this equation to \\[\\begin{align*}\n\\sum_{i=1}^N x_i y_i - \\beta \\sum_{i=1}^N x_i^2 &= (\\bar{y} - \\beta \\bar{x}) \\sum_{i=1}^N x_i \\\\\n&= \\bar{y} \\sum_{i=1}^N x_i - \\beta \\bar{x} \\sum_{i=1}^N x_i.\n\\end{align*}\\] By rearranging terms, we end up with \\[\\sum_{i=1}^N x_i y_i - \\bar{y} \\sum_{i=1}^N x_i = \\beta \\left[\\sum_{i=1}^N x_i^2 - \\bar{x} \\sum_{i=1}^N x_i\\right].\\] Finally, we can divide and do a bit more rearranging to end up with our formula for the slope: \\[\\begin{align*}\n\\beta &= \\frac{\\sum_{i=1}^N x_i y_i - \\bar{y} \\sum_{i=1}^N x_i}{\\sum_{i=1}^N x_i^2 - \\bar{x} \\sum_{i=1}^N x_i} \\\\\n&= \\frac{\\sum_{i=1}^N x_i (y_i - \\bar{y})}{\\sum_{i=1}^N x_i (x_i - \\bar{x})}.\n\\end{align*}\\]\nTo confirm that this formula works, let’s try it out with the feeling thermometer data, checking that it gives us the same answers as lm() does.\n\n## Make sure to use same observations as used in the regression\ndf_anes_no_na &lt;- df_anes |&gt;\n  filter(!is.na(therm_trump)) |&gt;\n  filter(!is.na(therm_police))\n\nx &lt;- df_anes_no_na$therm_police\ny &lt;- df_anes_no_na$therm_trump\n\n## Calculate slope\nslope_num &lt;- sum(x * (y - mean(y)))\nslope_denom &lt;- sum(x * (x - mean(x)))\nslope &lt;- slope_num / slope_denom\n\n## Calculate intercept\nintercept &lt;- mean(y) - slope * mean(x)\n\n## Compare to regression values\nc(intercept, slope)\n\n[1] -12.7889017   0.7476096\n\ncoef(lm(therm_trump ~ therm_police, data = df_anes))\n\n (Intercept) therm_police \n -12.7889017    0.7476096 \n\n\nWhat about regression with more than one feature? To derive that formula, you need not only calculus but also a bit of linear algebra. You can take a look at my graduate statistics notes if you want the details for that case.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "simulation_resampling.html",
    "href": "simulation_resampling.html",
    "title": "6  Simulation and Resampling",
    "section": "",
    "text": "6.1 Simulation\nSome of the calculations in our discussion of regression (Section 5.3), like the standard error and the \\(p\\)-value, depended on some quirky statistical logic: If we could repeat our statistical procedure over and over on new samples, what would the distribution of results look like?\nOur main goal in this unit is to accomplish two things.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation and Resampling</span>"
    ]
  },
  {
    "objectID": "simulation_resampling.html#simulation",
    "href": "simulation_resampling.html#simulation",
    "title": "6  Simulation and Resampling",
    "section": "",
    "text": "6.1.1 Random number generators\nR has built-in random number generators, which let you draw random numbers from a specified distribution. There are tons available, but we will use just a few in this course:\n\nsample(), to randomly sample values from a particular vector.\nrunif(), to randomly draw numbers from a uniform distribution — every value between two points is equally likely.\nrnorm(), to randomly draw numbers from a normal distribution — the familiar bell curve.\n\nIf we want to tell R “randomly pick a number from 1 to 10”, we can use the sample() function.\n\nsample(x = 1:10, size = 1)\n\n[1] 6\n\n\nThe x argument gives the function the set of values to choose from, and the size argument tells it how many to choose. When your size is greater than 1, you may also want to set replace = TRUE to allow for sampling with replacement, where the same value can be chosen more than once. (The default is to sample without replacement.)\n\nsample(x = 1:10, size = 5)  # no repeats possible\n\n[1] 8 2 3 5 4\n\nsample(x = 1:10, size = 5, replace = TRUE)  # repeats possible\n\n[1] 9 4 4 5 2\n\nsample(x = c(\"bear\", \"moose\", \"pizza\"), size = 2)  # sampling non-numeric values\n\n[1] \"pizza\" \"moose\"\n\n\nEvery time you use a random number function like sample(), you will get different results. That can be annoying when you’re writing a Quarto document — every time you render it, the numbers change! To prevent this, you can set a random seed with the set.seed() function.\n\nset.seed(14)\nsample(x = 1:10, size = 5, replace = TRUE)\n\n[1]  9  9  4  4 10\n\n# Will get same results after setting same seed\nset.seed(14)\nsample(x = 1:10, size = 5, replace = TRUE)\n\n[1]  9  9  4  4 10\n\n\nYou can put any whole number into the set.seed() function. I often use the jersey numbers of athletes I like and the years of memorable historical events.\nAnother useful function is runif(), which samples from the uniform distribution between two points. Use the min and max arguments to specify these points.\n\nrunif(n = 5)  # between 0 and 1 by default\n\n[1] 0.1637900 0.4741345 0.8512112 0.8574638 0.7396847\n\nrunif(n = 5, min = 10, max = 100)\n\n[1] 41.77897 70.60358 86.64114 63.58222 41.58333\n\n\nUse runif() when you want a flat histogram.\n\nlibrary(\"tidyverse\")\nlibrary(\"cowplot\")\ntheme_set(\n  theme_cowplot()\n)\n\ntibble(fake_x = runif(10000, min = 0, max = 100)) |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(binwidth = 5)\n\n\n\n\n\n\n\n\nThe last random number generator we’ll look at is rnorm(), which samples from the normal distribution. The normal distribution is a bell-curve-shaped probability distribution. Use the mean and sd arguments to specify the mean and standard deviation.\n\nrnorm(n = 5)  # mean 0, sd 1 by default\n\n[1] -0.5305785  0.1476253  0.4697936  0.4822537 -1.4676395\n\nrnorm(n = 5, mean = 100, sd = 10)\n\n[1]  96.95443 106.14779 107.88788 100.81268  94.27664\n\n\nThe histogram of normally distributed values has the familiar bell curve shape.\n\ndf_fake_normal &lt;- tibble(fake_x = rnorm(10000, mean = 50, sd = 20))\n\ndf_fake_normal |&gt;\n  ggplot(aes(x = fake_x)) +\n  geom_histogram(binwidth = 5)\n\n\n\n\n\n\n\n\nThe heuristic I mentioned in Section 3.3.1, that 95% of the data falls within 2 standard deviations of the mean, is exact in the case of a normal distribution.\n\ndf_fake_normal |&gt;\n  mutate(within2 = if_else(fake_x &gt;= 10 & fake_x &lt;= 90, \"within\", \"outside\")) |&gt;\n  count(within2) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  within2     n   prop\n  &lt;chr&gt;   &lt;int&gt;  &lt;dbl&gt;\n1 outside   487 0.0487\n2 within   9513 0.951 \n\n\n\n\n6.1.2 Simulating regression data\nIt can often be useful to simulate data using random number generators. That might seem weird, since our ultimate goal is to learn about the real world, and numbers that we make up on our computer aren’t from the real world. But there are a couple of ways a simulation can help us:\n\n\nThe weirdness of simulations hasn’t stopped certain political scientists from trying to use ChatGPT responses in place of real survey data! See my Political Analysis article with fellow Vanderbilt professors Jim Bisbee, Josh Clinton, Cassy Dorff, and Jenn Larson.\n\nGiving us a benchmark to test against. If we know exactly what patterns our analysis should find — which we do if we control exactly how the data was generated — then we can use a simulation to gauge how far off our statistical models might be in practice.\nWhat we’ll be doing today — helping us wrap our heads around difficult statistical concepts. Concepts like the standard error and \\(p\\)-value are about what would happen if you took lots of samples from a population with certain properties. It can be weird to think about those with real data, since we typically only have one sample to work with. Simulations make it much easier to think through.\n\nLet’s simulate data that follows a linear regression model: \\[y \\approx 10 + 0.5 x.\\] To make it a bit more fun than just calling the variables “x” and “y” — with no effect on the simulation results, as this is fake data — let’s assume the feature represents someone’s feeling thermometer score toward Chancellor Diermeier, and the response is their feeling toward Elon Musk. We’ll set it up so that we have a residual standard deviation of 5.\n\nset.seed(1873)  # crescere aude!\ndf_simulation &lt;- tibble(\n  x_diermeier = runif(\n    n = 50,\n    min = 20,\n    max = 80\n  ),\n  y_musk = 10 + 0.5 * x_diermeier + rnorm(n = 50, mean = 0, sd = 5)\n)\n\n# Scatterplot\ndf_simulation |&gt;\n  ggplot(aes(x = x_diermeier, y = y_musk)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Regression results\nfit_musk_diermeier &lt;- lm(y_musk ~ x_diermeier, data = df_simulation)\n\nsummary(fit_musk_diermeier)\n\n\nCall:\nlm(formula = y_musk ~ x_diermeier, data = df_simulation)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7309  -3.5182  -0.4258   4.1335  10.8545 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.01924    2.30978   3.905 0.000294 ***\nx_diermeier  0.52482    0.04285  12.249  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.437 on 48 degrees of freedom\nMultiple R-squared:  0.7576,    Adjusted R-squared:  0.7526 \nF-statistic:   150 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\nThe “true” intercept and slope are 10 and 0.5, respectively. We estimate an intercept and slope of roughly 9.0 and 0.52, each within a standard error of the true value. The \\(p\\)-value also tells us that it would be very unlikely to get such a steep slope from a sample of this size if the slope in the population were 0.\nYou might remember that the standard error has a kind of weird definition. The sampling distribution of a statistic (e.g., a regression coefficient) is the distribution of values it would take across every possible sample from the population. The standard error is the standard deviation of that distribution. Usually we can’t directly observe sampling distributions because we only have one sample. However, when we are running a simulation where we generate the data, we can generate as many samples as we want to!\nNow we can’t take every possible sample from the distribution we used to generate our data, as there are infinitely many. But we can take a lot of samples, in order to closely approximate the sampling distribution. We then run into another problem — isn’t this going to be really tedious? If we want to run a hundred simulations, do we have to repeat our code hundreds of times?\ndf_simulation_1 &lt;- tibble(\n  x_diermeier = runif(n = 50,\n                      min = 20,\n                      max = 80),\n  y_musk = 10 + 0.5 * x_diermeier + rnorm(n = 50, mean = 0, sd = 5)\n)\nfit_1 &lt;- lm(y_musk ~ x_diermeier, data = df_simulation_1)\n\ndf_simulation_2 &lt;- tibble(\n  x_diermeier = runif(n = 50,\n                      min = 20,\n                      max = 80),\n  y_musk = 10 + 0.5 * x_diermeier + rnorm(n = 50, mean = 0, sd = 5)\n)\nfit_2 &lt;- lm(y_musk ~ x_diermeier, data = df_simulation_2)\n\n# ... and so on?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation and Resampling</span>"
    ]
  },
  {
    "objectID": "simulation_resampling.html#loops",
    "href": "simulation_resampling.html#loops",
    "title": "6  Simulation and Resampling",
    "section": "6.2 Loops",
    "text": "6.2 Loops\nIt turns out we don’t have to write the same code hundreds of times. R, like virtually every programming language, has a feature called a for loop that lets us repeatedly run code with the same structure.\nA for loop in R has the following syntax:\nfor (i in values) {\n  ## code goes here\n}\n\nvalues: The values to iterate over. Whatever the length of values is, that’s the number of times the loop will run. So if you put 1:10 here, it will run ten times. If you put c(\"bear\", \"moose\", \"pizza\"), it will run three times.\ni: A variable name that is assigned to the corresponding element of values each time the loop runs. You can use any valid variable name in place of i here, though i is what you’ll see most commonly.\nInside the brackets: Code that is run every time the loop is executed.\n\nBefore getting into a more complicated situation, let’s look at a couple of simple examples of for loops. Here’s one where we repeat the following process ten times: Sample five random numbers from a normal distribution with mean 100 and standard deviation 10, and calculate their mean.\n\nset.seed(97)\nfor (i in 1:10) {\n  x &lt;- rnorm(5, mean = 100, sd = 10)\n  print(mean(x))\n}\n\n[1] 105.1824\n[1] 97.267\n[1] 98.56986\n[1] 101.0031\n[1] 95.62228\n[1] 108.3287\n[1] 98.22445\n[1] 96.06675\n[1] 103.3609\n[1] 95.77259\n\n\nWhen we do it that way, the results just disappear into the ether. If you want to save the results to work with later (e.g., to plot them), you need to write the loop to save them.\n\n\nNerd note: In the code here, I just set up a null variable to hold the results, and then in each iteration of the loop I append the current iteration’s results to that variable. If you’re ever doing high-end computational work in R, this way will run more slowly than if you set up the full vector to store the results first. For example, in this case I could have done sim_results &lt;- rep(NA, 10) before the loop, and then did sim_results[i] &lt;- mean(x) inside the loop. For our purposes in this class I’m going to stick with the easier way, but you should be aware in case you ever find yourself doing more advanced computational work.\n\nset.seed(97)\nsim_results &lt;- NULL  # variable that will hold the results\nfor (i in 1:10) {\n  x &lt;- rnorm(5, mean = 100, sd = 10)\n  sim_results &lt;- c(sim_results, mean(x))\n}\n\nsim_results\n\n [1] 105.18240  97.26700  98.56986 101.00311  95.62228 108.32866\n [7]  98.22445  96.06675 103.36093  95.77259\n\n\nNow let’s use a loop to look at the sampling distribution of the regression coefficients when we simulate data according to our regression formula. We will repeat the following process 100 times:\n\nSimulate a dataset according to our formula.\nRun a regression on the simulated data.\nExtract the coefficient and standard error estimates, storing them as a data frame.\n\nOur goal is to see how much our estimates of the slope and intercept vary from sample to sample.\n\nlibrary(\"broom\")\n\n# Set random seed as described above\nset.seed(1618)\n\n# Empty data frame to store results\ndf_regression_sim &lt;- NULL\n\nfor (i in 1:100) {\n  # Simulate data\n  df_loop &lt;- tibble(\n    x_diermeier = runif(n = 50, min = 20, max = 80),\n    y_musk = 10 + 0.5 * x_diermeier + rnorm(n = 50, mean = 0, sd = 5)\n  )\n\n  # Run regression\n  fit_loop &lt;- lm(y_musk ~ x_diermeier, data = df_loop)\n\n  # Store results of this iteration\n  df_regression_sim &lt;- bind_rows(df_regression_sim, tidy(fit_loop))\n}\n\ndf_regression_sim\n\n# A tibble: 200 × 5\n   term        estimate std.error statistic  p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)    7.87     2.85        2.76 8.19e- 3\n 2 x_diermeier    0.513    0.0590      8.70 2.00e-11\n 3 (Intercept)    9.81     2.00        4.91 1.08e- 5\n 4 x_diermeier    0.503    0.0394     12.8  4.98e-17\n 5 (Intercept)    8.92     2.48        3.59 7.69e- 4\n 6 x_diermeier    0.496    0.0469     10.6  3.82e-14\n 7 (Intercept)    6.32     2.22        2.85 6.45e- 3\n 8 x_diermeier    0.573    0.0424     13.5  5.68e-18\n 9 (Intercept)    8.80     2.37        3.71 5.36e- 4\n10 x_diermeier    0.507    0.0439     11.6  1.83e-15\n# ℹ 190 more rows\n\n\ndf_regression_sim contains the output from our simulation. It has 200 rows — two per simulation iteration, one for the intercept and one for the slope. You can see just from glancing at the results that there’s some variation from simulated sample to simulated sample, but the intercepts are fairly close to the true value of 10 and the slopes are very close to the true value of 0.5.\nRemember from above that the standard error is the standard deviation of the sampling distribution. So to get an idea of how much the intercept and slope vary from sample to sample, we will calculate the standard deviation of the 100 draws we took from the sampling distribution. The number that R spits out as the “standard error” with regression results (the \"std.error\" column of tidy() output) is actually an estimate of the true standard error. We’ll also take the average of these standard error estimates to see how they compare to the true variability of the coefficients from sample to sample.\n\ndf_regression_sim |&gt;\n  group_by(term) |&gt;\n  summarize(\n    avg_est = mean(estimate),\n    true_se = sd(estimate),\n    avg_se_est = mean(std.error)\n  )\n\n# A tibble: 2 × 4\n  term        avg_est true_se avg_se_est\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)  10.1    2.21       2.24  \n2 x_diermeier   0.498  0.0417     0.0422\n\n\nHere’s what this table is telling us:\n\nAcross our 100 simulations, the average intercept is very close to its true value of 10, and the average slope is very close to its true value of 0.5. This is an illustration of the fact that linear regression is unbiased in statistical terms.\nThe true standard error of the intercept is about 2.2, so across samples we would expect to estimate an intercept within \\(10 \\pm 4.4\\) about 95% of the time. The true standard error of the slope is about 0.04, so across samples we would expect to estimate a slope within \\(0.5 \\pm 0.08\\) about 95% of the time.\nOn average, R’s estimate of the standard error of both coefficients comes pretty close to the true standard error. The upshot is that these values are pretty reliable for statistical inference — any given sample will give us a good idea as to the range of plausible values for the intercept and slope.\n\nTo illustrate that final point, there’s another question we can use our simulation results to answer. Let’s use the formula \\[\\text{estimate} \\pm 2 \\times \\text{std error}\\] to determine a range of plausible values for the coefficient. If we do this with each of the samples in our simulation, how often does the range of plausible values include the true value of 0.5?\n\ndf_regression_sim |&gt;\n  filter(term == \"x_diermeier\") |&gt;\n  arrange(estimate) |&gt;\n  mutate(\n    id = row_number(),\n    lower = estimate - 2 * std.error,\n    upper = estimate + 2 * std.error,\n    type = if_else(\n      lower &lt;= 0.5 & upper &gt;= 0.5,\n      \"includes true slope\",\n      \"doesn't include true slope\"\n    )\n  ) |&gt;\n  ggplot(aes(xmin = lower, xmax = upper, y = id)) +\n  geom_linerange(aes(color = type)) +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\")\n\n\n\n\n\n\n\n\nThe range includes the true slope in 94 out of our 100 samples. That’s very close to the 95% we would expect from a confidence interval calculation like this one.\n\n\n\n\n\n\nIn-class exercise\n\n\n\nRun a new simulation with a similar regression equation, except where the true slope is 0. This time do 1000 simulation iterations instead of 100. In how many of your iterations is the slope “statistically significant”, in terms of having a \\(p\\)-value of 0.05 or less?\n\n#\n# [Write your answer here]\n#",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation and Resampling</span>"
    ]
  },
  {
    "objectID": "simulation_resampling.html#resampling",
    "href": "simulation_resampling.html#resampling",
    "title": "6  Simulation and Resampling",
    "section": "6.3 Resampling",
    "text": "6.3 Resampling\nThe basic idea behind these simulations can also be applied to estimate uncertainty in real data problems. To see this principle in action, we will work with the same survey data as in Chapter 5.\n\ndf_anes &lt;- read_csv(\"https://bkenkel.com/qps1/data/anes_2020.csv\")\n\ndf_anes\n\n# A tibble: 8,280 × 31\n      id state  female  lgbt race    age education employed hours_worked\n   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     1 Oklah…      0     0 Hisp…    46 Bachelor…        1           40\n 2     2 Idaho       1     0 Asian    37 Some col…        1           40\n 3     3 Virgi…      1     0 White    40 High sch…        0            0\n 4     4 Calif…      0     0 Asian    41 Some col…        1           40\n 5     5 Color…      0     0 Nati…    72 Graduate…        0            0\n 6     6 Texas       1     0 White    71 Some col…        0            0\n 7     7 Wisco…      1     0 White    37 Some col…        1           30\n 8     8 &lt;NA&gt;        1     0 White    45 High sch…        0           40\n 9     9 Arizo…      1     0 White    70 High sch…        0            0\n10    10 Calif…      0     0 Hisp…    43 Some col…        1           30\n# ℹ 8,270 more rows\n# ℹ 22 more variables: watch_tucker &lt;dbl&gt;, watch_maddow &lt;dbl&gt;,\n#   therm_biden &lt;dbl&gt;, therm_trump &lt;dbl&gt;, therm_harris &lt;dbl&gt;,\n#   therm_pence &lt;dbl&gt;, therm_obama &lt;dbl&gt;, therm_dem_party &lt;dbl&gt;,\n#   therm_rep_party &lt;dbl&gt;, therm_feminists &lt;dbl&gt;, therm_liberals &lt;dbl&gt;,\n#   therm_labor_unions &lt;dbl&gt;, therm_big_business &lt;dbl&gt;,\n#   therm_conservatives &lt;dbl&gt;, therm_supreme_court &lt;dbl&gt;, …\n\n\n\n6.3.1 Resampling coefficients\nWe want to separate the signal from the noise — to figure out what inferences we can reliably draw from the data we have, as opposed to patterns that might have shown up through sheer random chance. Our goal is to gauge “If we could draw a bunch of new random samples of the same size, how much would we expect our regression estimates to vary from sample to sample?” This may seem like an impossible question to answer with a single sample. It turns out, however, that we can get an approximate answer using a technique called the bootstrap.\nThe basic idea behind the bootstrap is that we can generate new random samples by sampling with replacement from our original data. Because we are sampling with replacement, some of our original observations will show up 2, 3, or even more times in any given bootstrap sample, while others won’t be included at all. In the same fashion as the simulations we ran above, we won’t just draw one bootstrap sample—we will draw hundreds or even thousands of new samples, so as to get an idea of how our statistical calculations would vary from sample to sample.\nTo draw a single bootstrap sample from a data frame, we can use the slice_sample() function.\n\nset.seed(1789)\n\ndf_boot &lt;- df_anes |&gt;\n  slice_sample(prop = 1, replace = TRUE)\n\ndf_boot\n\n# A tibble: 8,280 × 31\n      id state  female  lgbt race    age education employed hours_worked\n   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1  3243 Calif…      1     0 Nati…    64 Some col…        0            0\n 2  7413 Michi…      1     0 White    70 Bachelor…        0            0\n 3  3511 Washi…      0     0 White    76 Bachelor…        0            0\n 4  4841 Massa…      1     0 White    80 &lt;NA&gt;             0            0\n 5  6165 Washi…      1     0 White    64 Bachelor…        1           40\n 6  2698 Maryl…      1     0 White    47 Graduate…        1           50\n 7  5578 Calif…      0     0 Asian    55 Bachelor…        1           60\n 8  3543 Nevada      1     0 Black    52 Some col…        1           40\n 9  7316 Calif…      1     0 White    80 Less tha…        0            0\n10  3827 Flori…      0    NA Nati…    NA Some col…        0            0\n# ℹ 8,270 more rows\n# ℹ 22 more variables: watch_tucker &lt;dbl&gt;, watch_maddow &lt;dbl&gt;,\n#   therm_biden &lt;dbl&gt;, therm_trump &lt;dbl&gt;, therm_harris &lt;dbl&gt;,\n#   therm_pence &lt;dbl&gt;, therm_obama &lt;dbl&gt;, therm_dem_party &lt;dbl&gt;,\n#   therm_rep_party &lt;dbl&gt;, therm_feminists &lt;dbl&gt;, therm_liberals &lt;dbl&gt;,\n#   therm_labor_unions &lt;dbl&gt;, therm_big_business &lt;dbl&gt;,\n#   therm_conservatives &lt;dbl&gt;, therm_supreme_court &lt;dbl&gt;, …\n\n\nThe prop = 1 argument tells slice_sample() to draw the same number of observations as in the original sample. (If we wanted half as many, for example, we would use prop = 0.5.) The replace = TRUE argument tells it to sample with replacement. Without this argument, we would just end up with our original data but with the rows in a different order, giving us exactly the same statistical calculations as we got originally.\nIf we look at the relationship between feelings toward the police and feelings toward Donald Trump, we see similar — but not exactly the same — estimates in the original sample and in our singular (so far) bootstrap resample.\n\nfit_main &lt;- lm(therm_trump ~ therm_police, data = df_anes)\nfit_boot &lt;- lm(therm_trump ~ therm_police, data = df_boot)\n\nsummary(fit_main)\n\n\nCall:\nlm(formula = therm_trump ~ therm_police, data = df_anes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-61.972 -32.068  -1.972  34.242 112.789 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -12.7889     1.2503  -10.23   &lt;2e-16 ***\ntherm_police   0.7476     0.0167   44.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35.69 on 7188 degrees of freedom\n  (1090 observations deleted due to missingness)\nMultiple R-squared:  0.2181,    Adjusted R-squared:  0.218 \nF-statistic:  2005 on 1 and 7188 DF,  p-value: &lt; 2.2e-16\n\nsummary(fit_boot)\n\n\nCall:\nlm(formula = therm_trump ~ therm_police, data = df_boot)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.277 -31.733  -1.461  34.177 114.083 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -14.0829     1.2558  -11.21   &lt;2e-16 ***\ntherm_police   0.7636     0.0168   45.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35.59 on 7168 degrees of freedom\n  (1110 observations deleted due to missingness)\nMultiple R-squared:  0.2238,    Adjusted R-squared:  0.2237 \nF-statistic:  2067 on 1 and 7168 DF,  p-value: &lt; 2.2e-16\n\n\nA single bootstrap sample doesn’t give us very much that’s interesting. The method requires that we resample from the data many times, then look at the distribution of the results. We can accomplish that with (hopefully you guessed it…) a for loop.\n\n\n# Set random seed as discussed above\nset.seed(1066)\n\n# Empty data frame to store resampling results\ndf_coef_boot &lt;- NULL\n\n# Resample the data 1000 times (aka: 1000 bootstrap iterations)\nfor (i in 1:1000) {\n  # Draw bootstrap sample\n  df_boot &lt;- slice_sample(df_anes, prop = 1, replace = TRUE)\n\n  # Rerun regression model on bootstrap sample\n  fit_boot &lt;- lm(therm_trump ~ therm_police, data = df_boot)\n\n  # Save results\n  df_coef_boot &lt;- bind_rows(df_coef_boot, tidy(fit_boot))\n}\n\ndf_coef_boot\n\n# A tibble: 2,000 × 5\n   term         estimate std.error statistic  p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)   -10.7      1.25       -8.57 1.27e-17\n 2 therm_police    0.722    0.0167     43.2  0       \n 3 (Intercept)   -11.3      1.21       -9.30 1.79e-20\n 4 therm_police    0.732    0.0163     45.0  0       \n 5 (Intercept)   -13.2      1.27      -10.4  3.79e-25\n 6 therm_police    0.752    0.0170     44.3  0       \n 7 (Intercept)   -14.0      1.30      -10.8  7.64e-27\n 8 therm_police    0.758    0.0173     43.9  0       \n 9 (Intercept)   -10.2      1.22       -8.36 7.58e-17\n10 therm_police    0.711    0.0164     43.4  0       \n# ℹ 1,990 more rows\n\n\nUsing the results of the bootstrap resampling, we can directly look at the distribution of regression coefficients across samples.\n\ndf_coef_boot |&gt;\n  ggplot(aes(x = estimate)) +\n  geom_histogram() +\n  facet_wrap(~ term, ncol = 2, scales = \"free_x\") +\n  panel_border()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nYou might notice that both distributions have a bell curve shape. This isn’t a coincidence. It’s an example of the central limit theorem, an important result showing that many common statistics have approximately normal sampling distributions. You’ll also notice that both distributions are centered roughly around the coefficients from the regression with the original data. The estimated intercept tends to be between -15 and -10, while the estimated slope tends to be between 0.72 and 0.78.\nWe can gauge the standard error by taking the standard deviation of the bootstrap distribution, and we can also calculate a range of plausible values by using the percentiles.\n\ndf_coef_boot |&gt;\n  group_by(term) |&gt;\n  summarize(\n    std_err = sd(estimate),\n    lower = quantile(estimate, 0.025),\n    upper = quantile(estimate, 0.975)\n  )\n\n# A tibble: 2 × 4\n  term         std_err   lower   upper\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   1.22   -15.2   -10.5  \n2 therm_police  0.0170   0.717   0.782\n\n\nThese are pretty close to the standard error from the original analysis, as well as the range of coefficients we would estimate with the “\\(\\pm\\) 2 standard errors” rule.\n\ntidy(fit_main) |&gt;\n  select(term, estimate, std.error) |&gt;\n  mutate(\n    lower = estimate - 2 * std.error,\n    upper = estimate + 2 * std.error\n  )\n\n# A tibble: 2 × 5\n  term         estimate std.error   lower   upper\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   -12.8      1.25   -15.3   -10.3  \n2 therm_police    0.748    0.0167   0.714   0.781\n\n\nWhat’s the point of bootstrapping if it ultimately yields close to the same answers as what we would get from the summary() output?\n\nThe bootstrap is more robust. In data situations with small samples, big outliers, or other issues, the bootstrap may be more reliable at estimating the amount of sample-to-sample variation in the regression coefficients than the standard formulas are.\nThe bootstrap is very widely applicable to statistical measures. For some calculations, there’s not a standard error calculation reported by default like there is for regression coefficients. But you can still bootstrap them: just resample from your original data with replacement, calculate the statistic on each bootstrap iteration, and then look at the distribution of bootstrap estimates to gauge the amount of variability and the plausible range of estimates.\nBy using the bootstrap and recovering close to the original results, you (I hope!) get a better idea of how it is that we can estimate sample-to-sample variability in a statistic even though we only actually observe a single sample of data.\n\n\n\n\n\n\n\nIn-class exercise\n\n\n\nSuppose the statistic you are interested in is the median of respondents’ feeling thermometer score toward Donald Trump. Use the bootstrap to estimate the standard error of the median, as well as a range of plausible medians.\n\n#\n# [Write your answer here]\n#\n\n\n\n\n\n6.3.2 Resampling predictions\nSuppose we want to predict someone’s feeling toward Donald Trump if all we know about them is that their feeling toward the police is a 50 on a 100-point scale. We know that we can use the regression formula, \\[\\text{response} \\approx \\text{intercept} + \\text{slope} \\times 50,\\] to come up with a prediction. What’s harder is to gauge the amount of uncertainty in that prediction—i.e., to nail down the range of estimates that are plausible based on the knowledge we have. In part that’s because there are two sources of uncertainty.\n\nThe regression line doesn’t perfectly fit the data. There’s some spread between the points and the line, summarized by what R calls the residual standard error.\nThe regression line we estimate from our sample might be different than the line we would fit if we had the full population of data. In other words, there’s statistical uncertainty about the values of the slope and the intercept.\n\nThe bootstrap method lets us account for both of these sources of uncertainty. To accomplish this, we will use a simulation-within-a-simulation. For each bootstrap sample, we will:\n\nRun the regression of Trump feeling on police feeling.\nUse the intercept and slope to calculate predicted Trump feeling for someone whose police feeling is 50.\nUse the predicted value and the residual standard error to simulate a distribution of potential values of Trump feeling.\n\n\n# Set random seed as discussed above\nset.seed(1985)\n\n# Null variable to store simulation results\npred_at_50 &lt;- NULL\n\n# Simulating 1000 times\nfor (i in 1:1000) {\n  # Draw bootstrap resample\n  df_boot &lt;- slice_sample(df_anes, prop = 1, replace = TRUE)\n\n  # Fit regression model and extract coefficients + residual sd\n  fit_boot &lt;- lm(therm_trump ~ therm_police, data = df_boot)\n  tidy_boot &lt;- tidy(fit_boot)\n  intercept &lt;- tidy_boot$estimate[1]\n  slope &lt;- tidy_boot$estimate[2]\n  sigma &lt;- glance(fit_boot)$sigma\n\n  # Calculate main prediction at therm_police = 50\n  pred_boot &lt;- intercept + slope * 50\n\n  # Simulate distribution of predictions\n  dist_pred_boot &lt;- rnorm(100, mean = pred_boot, sd = sigma)\n\n  # Append results to storage vector\n  pred_at_50 &lt;- c(pred_at_50, dist_pred_boot)\n}\n\nWe can take a look at the distribution of predictions:\n\ntibble(pred_at_50) |&gt;\n  ggplot(aes(x = pred_at_50)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe main takeaway here is that there’s a ton of uncertainty in this prediction. A sizable percentage of the simulated predictions are below 0, and a few are even above 100. Let’s look at the average and standard deviation.\n\nlength(pred_at_50)\n\n[1] 100000\n\nmean(pred_at_50)\n\n[1] 24.33769\n\nsd(pred_at_50)\n\n[1] 35.74144\n\n\nThe standard deviation here is not much larger than the residual standard deviation in our regression model on the original data. That tells me that the statistical uncertainty about the coefficient values is contributing relatively little to our uncertainty about the prediction. Instead, most of the variation in predictions is coming from the fact that the points are spread out pretty far from the regression line in the first place.\n\n\n\n\n\n\nIn-class exercise\n\n\n\nRepeat the simulation from this section, but for someone whose feeling toward the police is 75 rather than 50. What differences do you see in the central tendency and the spread of the distribution of resampled predictions?\n\n#\n# [Write your answer here]\n#",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation and Resampling</span>"
    ]
  },
  {
    "objectID": "using_chatgpt.html",
    "href": "using_chatgpt.html",
    "title": "Appendix A — Using ChatGPT",
    "section": "",
    "text": "A.1 The very basics of generative AI\nAs programming workflows become increasingly dependent on AI tools, it’s important to understand how to best use generative AI models like ChatGPT to help you code in R. There are a lot of tasks at which ChatGPT is only mediocre or worse (e.g., writing scripts for new episodes of my favorite old sitcoms), but it is a legitimately transformative tool for programmers. I have used R since 2008, and nonetheless I probably ask it at least one R question every day for research purposes. It’s also been invaluable to me as I’ve been learning Python over the past couple of years.\nChatbots like ChatGPT are built on large language models. The fundamental goal of a large language model is to predict the next word in a sequence of text. For example, think about the sequence:\n“Vanderbilt” is a plausible next word. So are “Belmont,” “Lipscomb,” and the names of various Nashville high schools. On the other hand, it’s pretty unlikely that the next word is “Duke,” “Emory,” or “Princeton.” A good large language model will know that. For example, when I gave this text sequence to Meta’s Llama-3.2 model, it completed the sentence with\nIt takes a lot of clever code and computing power to create one of these models. But the fundamentals of the process are fairly straightforward.\nThis is the most important thing to understand about large language models if you want to sort out what they’ll be good at doing versus what they’ll be bad at. They are trained to predict what words come next, based on the patterns it’s seen in billions of Internet documents. What this means is that they are not designed at a core level to tell you the truth. If the most common pattern in Internet text is a lie, the large language model will dutifully repeat the lie.\nFor example, again working with Meta’s Llama model, I gave it this prompt:\nIt responded by repeating a common but false assertion about vaccines and autism.\nThe LLM is a pattern matching device, not a truth seeking device. Now there is a certain wisdom of crowds, such that common patterns are often indicative of the truth. “Vanderbilt” and “Nashville” appear together a lot because Vanderbilt really is in Nashville. But once you get into the realm of the controversial or the obscure, your skepticism of large language model output should be all the higher.\nIn this class, we won’t be asking large language models to adjudicate the truth about medical procedures. We will be asking them for advice on writing R code. The advice we get will be based on the patterns the model has seen in the many millions of lines of R code it has seen from questions on StackExchange, repositories on GitHub, etc. So we should expect the best performance with simple questions where there are a lot of examples it can draw from. If we ask about more obscure techniques or functions, the performance is likely to be worse.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Using ChatGPT</span>"
    ]
  },
  {
    "objectID": "using_chatgpt.html#the-very-basics-of-generative-ai",
    "href": "using_chatgpt.html#the-very-basics-of-generative-ai",
    "title": "Appendix A — Using ChatGPT",
    "section": "",
    "text": "Learning more\n\n\n\nThis is a very brief and loose summary of the inner workings of large language models that power tools like ChatGPT. For a more technical but still accessible summary, I like this Substack post by Timothy B. Lee and Sean Trott.\n\n\n\nI live in Nashville, TN, where I'm a student at\n\nI live in Nashville, TN, where I'm a student at Belmont University.\nI'm a member of the Belmont University Choir, and I'm also a member of\n\n\nConstruct a “training set” containing as much of the text on the Internet as you can download. (See this Washington Post article on the main sources used.)\nTrain a machine learning model to identify patterns in the text. This involves first finding a way to turn words into numbers in the computer’s memory, then creating a statistical model of the relationships between those numbers.\nNews stories, Wikipedia entries, and other online sources that mention “Vanderbilt” are disproportionately likely to mention “Nashville” too, so the LLM will learn to associate these words with each other. Same with “Duke” and “Durham.”\nCreate a pipeline that takes input text from a user, then uses the underlying statistical model to predict the next sequence of output.\nWith our “I live in Nashville, TN, where I’m a student at” example, the underlying statistical model might look something like:\n\n20% chance of “Belmont”\n15% chance of “Vanderbilt”\n10% chance of “Lipscomb”\n8% chance of “Tennessee”\n4% chance of “Meharry”\n43% total chance of all the other of the many, many words it knows\n\nAt this point the model might pick the most likely option (“Belmont”), or it might sample from all of the options in proportion to its estimate of their probabilities, or it might do some compromise of these. For our purposes, it’s just important to know that it’s trying to predict the word that’s most likely to come next, based on all the patterns it’s seen across all of the crazy amounts of Internet text it’s processed.\n\n\n\nAs a parent, I know in my heart that vaccines cause\n\nAs a parent, I know in my heart that vaccines cause autism. I know this\nbecause I have seen it with my own eyes. I have seen it with\n\n\n\n\n\n\n\n\nReinforcement learning\n\n\n\nThe example above comes from a “raw” large language model, not a public-facing chatbot. Major chatbots like ChatGPT or Claude won’t tell you that vaccines cause autism. Even the Grok chatbot from Elon Musk’s X, which has been tuned to be less liberal and more MAGA than the other major chatbot products, clearly states that vaccines do not cause autism.\nWhy don’t the big chatbots repeat the mistake when it’s (probably) the dominant pattern in their training data? Once the underlying AI model has been trained to do pattern recognition from text, there’s often another step called reinforcement learning with human feedback (RLHF) that’s undertaken before making a public-facing product. In very simple terms, this involves using responses from human testers to guide the chatbot away from saying things that are false, offensive, or otherwise problematic.\nPrecisely because the prevalence of false beliefs about vaccines and autism is so well-known, I would bet that this is one of the topics that the AI labs have specifically focused on in their RLHF fine-tuning processes. But on more obscure topics, like the specifics of how some R package works, it’s much less likely that there’s been any kind of dedicated effort to prevent the bot from making things up or repeating false information.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Using ChatGPT</span>"
    ]
  },
  {
    "objectID": "using_chatgpt.html#productive-prompting-context-is-key",
    "href": "using_chatgpt.html#productive-prompting-context-is-key",
    "title": "Appendix A — Using ChatGPT",
    "section": "A.2 Productive prompting: Context is key",
    "text": "A.2 Productive prompting: Context is key\nChatbots and language models have existed for decades. One of the big advances in the past couple of years to make AI chatbots widely useful has been the expansion of the context window, the amount of information the model can account for as it tries to solve the “What word comes next?” problem. As a simple example of how context matters, compare how ChatGPT responds to the following prompts:\nComplete the sentence: \"I live in Nashville, TN, where I am a student at ___.\"\nComplete the sentence: \"I am seven years old. I live in Nashville, TN, where I\nam a student at ___.\"\nThe same principle applies to code questions. The more information you give ChatGPT, the better targeted help it can give you.\nOne important piece of contextual information is who you are and what you’re looking for. For a personal example, I often ask ChatGPT questions about college-level math that I need to know for my formal modeling research. Because ChatGPT is pattern-matching from what it finds in its corpus of knowledge, by default it will often respond as if I am an undergraduate looking for homework help or perhaps some kind of engineer. So if the default responses aren’t helpful, I’ll start a new chat where I start my prompt with something like:\nI am a political science professor writing a research paper using game theory.\nI want to publish this paper in a top-tier political science or economics journal.\nIn the context of monotone comparative statics for game theory, what is a lattice?\nI never advocate lying to humans, but sometimes it can be useful to lie to robots. For example, you might get higher-quality code if you start your prompt with “I am a machine learning engineer at Google” rather than “I am an undergraduate political science major”. On the other hand, you may get more comprehensive explanations of the output if you say you’re a student.\nRegardless of what (if anything) you tell the model about who you are and why you’re asking, you tend to get better responses the more contextual information you give. One of the tasks ChatGPT does best is explaining code.\n\n\n\n\n\n\nSample chatbot prompt: Code explanation\n\n\n\nWork through this R code line by line and explain what it’s doing:\nplot_pred_usa &lt;-\n  ggplot(\n    df_first_stage,\n    aes(x = log1p(d_usa_pred), y = log1p(d_usa_actual))\n  ) +\n  geom_text(aes(label = label), size = 3) +\n  geom_smooth() +\n  labs(\n    title = \"First stage predictions for USA aid\",\n    x = \"log(1 + predicted USA aid)\",\n    y = \"log(1 + actual USA aid)\"\n  ) +\n  theme_bw()\nggsave(\"figures/pred_usa_pretty.pdf\", plot_pred_usa, width = 7, height = 4.5)\n\n\nChatGPT can also be useful for debugging errors. You can often just plug in your code and ask it to spot the problem. However, you’ll usually do even better by giving ChatGPT some contextual information, including the error message you’re receiving when you try to run your code.\n\n\n\n\n\n\nSample chatbot prompt: Debugging\n\n\n\nI am working with a data frame in R called df_survey that looks like this:\n# A tibble: 6 × 3\n  party_id      age income\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n1 Republican     30  10000\n2 Democrat       40  20000\n3 Independent    50  30000\n4 Republican     60  40000\n5 Democrat       70  50000\n6 Independent    80  60000\nI am trying to run this code:\ndf_survey |&gt;\n  select(age &gt;= 45) |&gt;\n  group_by(party_id) |&gt;\n  summarize(avg_income = mean(income))\nBut it spits out this error:\nError in `select()`:\nIn argument: `age &gt;= 45`.\nCaused by error:\n! object 'age' not found\nRun `rlang::last_trace()` to see where the error occurred.\nHow can I fix it?\n\n\nAsking ChatGPT to write code can be dicier. It’s great at simple things. The more complicated your ask, the less likely it is to write code that actually works. (For example, I had a heck of a time trying to get it to help me with the material on making maps for the final section of the Data Visualization lecture notes.) With a complicated code writing task, your best bet is to give it as much detail as possible about your inputs and your desired outputs.\n\n\n\n\n\n\nSample chatbot prompt: Code writing\n\n\n\nI am trying to merge two datasets in R. The first is called df_survey and it looks like this:\n# A tibble: 100 × 5\n      id st      age income party_id   \n   &lt;int&gt; &lt;chr&gt; &lt;int&gt;  &lt;int&gt; &lt;chr&gt;      \n 1     1 OH       32  57076 Democrat   \n 2     2 TN       54  93213 Republican \n 3     3 TN       65  60679 Republican \n 4     4 CA       53  97740 Democrat   \n 5     5 CA       23  93538 Republican \n 6     6 TN       75  74921 Democrat   \n 7     7 NY       53  71817 Republican \n 8     8 NY       24  25826 Democrat   \n 9     9 OH       38  57562 Independent\n10    10 CA       63  49957 Independent\nThe second is called df_states and it looks like this:\n# A tibble: 3 × 3\n  state region    population\n  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 NY    Northeast   10000000\n2 OH    Midwest      5000000\n3 KY    South        3000000\nI want to merge them together, matching the “st” column from df_survey with the “state” column from df_states. However, I also want to drop any of the respondents in df_survey whose state does not appear in df_states. What’s the best way to accomplish this? Please explain each line of your response.\n\n\nFor code writing tasks like this one, it can be helpful to use ChatGPT’s “Canvas” feature. This mode puts the code in a separate window, allowing you to highlight portions to ask ChatGPT to explain or change. You have to dig a bit into the ChatGPT options to enable Canvas, as illustrated in Figure A.1.\n\n\n\nFigure A.1: How to use Canvas or Study mode in ChatGPT.\n\n\n\n\n\n\nYou can even use ChatGPT like a personal tutor. For example, if you wanted some additional midterm practice, you could have it write new questions based on the ones from the practice midterm.\n\n\n\n\n\n\nSample chatbot prompt: Study tool\n\n\n\nFile attachment: midterm1_2024.pdf\nI am studying for the midterm in my stats class. I’ve attached last year’s midterm for reference. We’re covering chapters 1-3 from the notes at https://bkenkel.com/qps1.\nCan you write brand new short answer questions based on the ones in the practice midterm? Ask them to me one at a time, and give me feedback on my answers.\n\n\nChatGPT recently introduced a study mode that might be useful to turn on when asking this type of question. From my brief testing, it doesn’t look like study mode fundamentally alters ChatGPT’s look or behavior. Instead, it just makes the chatbot a bit more teacher-y, in terms of giving detailed explanations instead of just spitting out an answer and moving on. You enable study mode similarly to how you’d enable Canvas (see Figure A.1).\nThese notes only scratch the surface of the kinds of prompts you can use. Vanderbilt’s Data Science Institute has a Prompt Patterns page with a ton more ideas, in addition to a full-fledged Prompt Engineering course for free on Coursera if you’re truly dedicated.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Using ChatGPT</span>"
    ]
  },
  {
    "objectID": "using_chatgpt.html#using-chatgpt-for-problem-sets",
    "href": "using_chatgpt.html#using-chatgpt-for-problem-sets",
    "title": "Appendix A — Using ChatGPT",
    "section": "A.3 Using ChatGPT for problem sets",
    "text": "A.3 Using ChatGPT for problem sets\nOnce we’ve covered this material in class, you are free to use ChatGPT or other chatbots to help with R coding on problem sets. Here’s what you need to keep in mind as you do so.\n\nMost importantly, use ChatGPT as a complement to your learning instead of as a substitute for learning things yourself. You will learn and retain more if you try things yourself, and only look to AI tools for help once you’ve hit a wall. You can also use AI in a way that reinforces your learning, by asking follow-up questions about why and how particular bits of code work (or don’t work!).\nRelated to that, keep in mind that you won’t have access to ChatGPT on exams. I design the problem sets so that you’ll learn from working through them. If you outsource all of the work to an AI model, you may (or may not!) score well on the problem set, but you’ll have missed out on critical knowledge-building and practice for the exam.\nMy policies are only for PSCI 2300. Don’t assume that other Vanderbilt professors, including in political science, are operating by the same policies as me. If you’re ever confused about what’s appropriate and what isn’t, check with the professor.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Using ChatGPT</span>"
    ]
  },
  {
    "objectID": "data_sources.html",
    "href": "data_sources.html",
    "title": "Appendix B — Data Sources",
    "section": "",
    "text": "B.1 US voter turnout 1980–2022: turnout.csv\nThis file uses data from the University of Florida Election Lab, specifically version 1.2 of the General Election Turnout Rates dataset.\nObtain the raw data:\n# Download raw data\nturnout_url &lt;- \"https://election.lab.ufl.edu/data-downloads/turnoutdata/Turnout_1980_2022_v1.2.csv\"\nturnout_file &lt;- \"_raw/Turnout_1980_2022_v1.2.csv\"\nif (!file.exists(turnout_file))\n  download.file(url = turnout_url, destfile = turnout_file)\n\n# Read in raw data\ndf_turnout_raw &lt;- read_csv(turnout_file)\n\nglimpse(df_turnout_raw)\n\nRows: 1,144\nColumns: 15\n$ YEAR                    &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022…\n$ STATE                   &lt;chr&gt; \"United States\", \"Alabama\", \"Alaska\", \"Arizona…\n$ STATE_ABV               &lt;chr&gt; NA, \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", …\n$ TOTAL_BALLOTS_COUNTED   &lt;dbl&gt; 112030874, 1424087, 267047, 2592313, 914227, 1…\n$ VOTE_FOR_HIGHEST_OFFICE &lt;chr&gt; NA, \"https://www.eac.gov/sites/default/files/2…\n$ VAP                     &lt;dbl&gt; 260725069, 3956111, 556592, 5796801, 2347291, …\n$ NONCITIZEN_PCT          &lt;chr&gt; \"7.50%\", \"2.54%\", \"3.56%\", \"7.78%\", \"3.76%\", \"…\n$ INELIGIBLE_PRISON       &lt;dbl&gt; 1175823, 25403, 4778, 31441, 17331, 97608, 163…\n$ INELIGIBLE_PROBATION    &lt;dbl&gt; 1074600, 27469, 1872, 47515, 28009, 0, 0, 0, 7…\n$ INELIGIBLE_PAROLE       &lt;dbl&gt; 412595, 7815, 865, 7022, 23829, 0, 0, 0, 344, …\n$ INELIGIBLE_FELONS_TOTAL &lt;dbl&gt; 2663018, 60687, 7515, 85978, 69169, 97608, 163…\n$ ELIGIBLE_OVERSEAS       &lt;dbl&gt; 4400000, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ VEP                     &lt;dbl&gt; 242907672, 3794939, 529263, 5259832, 2189865, …\n$ VEP_TURNOUT_RATE        &lt;chr&gt; \"46.12%\", \"37.53%\", \"50.46%\", \"49.29%\", \"41.75…\n$ VAP_TURNOUT_RATE        &lt;chr&gt; \"42.97%\", \"36.00%\", \"47.98%\", \"44.72%\", \"38.95…\nCleaning up into the data file used for class:\ndf_turnout &lt;- df_turnout_raw |&gt;\n  # Only want national level data\n  filter(STATE == \"United States\") |&gt;\n  # Grab and rename the columns we want\n  mutate(\n    year = YEAR,\n    voting_age_pop = VAP,\n    voting_eligible_pop = VEP,\n    ballots_counted = TOTAL_BALLOTS_COUNTED,\n    highest_office = VOTE_FOR_HIGHEST_OFFICE,\n    noncitizen_pct = NONCITIZEN_PCT,\n    ineligible_felons = INELIGIBLE_FELONS_TOTAL,\n    eligible_overseas = ELIGIBLE_OVERSEAS,\n    .keep = \"none\",\n  ) |&gt;\n  # Clean up highest_office and noncitizen_pct columns to be numeric\n  mutate(\n    noncitizen_pct = str_replace(noncitizen_pct, \"\\\\%\", \"\"),\n    noncitizen_pct = as.numeric(noncitizen_pct) / 100,\n    highest_office = str_replace_all(highest_office, \",\", \"\"),\n    highest_office = as.numeric(highest_office),\n  ) |&gt;\n  # Calculate number of noncitizens\n  mutate(\n    ineligible_noncitizens = noncitizen_pct * voting_age_pop,\n  ) |&gt;\n  select(-noncitizen_pct) |&gt;\n  # For vote total, use ballots counted where available, otherwise just use\n  # votes for highest office\n  mutate(\n    votes_counted = if_else(\n      !is.na(ballots_counted),\n      ballots_counted,\n      highest_office\n    )\n  ) |&gt;\n  # Convert population counts to millions\n  mutate(across(-year, \\(x) x / 1e6)) |&gt;\n  # Remove columns no longer needed\n  select(\n    year, votes_counted, voting_age_pop, voting_eligible_pop,\n    ineligible_felons, ineligible_noncitizens, eligible_overseas\n  ) |&gt;\n  # Order from earliest to latest\n  arrange(year)\n\nglimpse(df_turnout)\n\nRows: 22\nColumns: 7\n$ year                   &lt;dbl&gt; 1980, 1982, 1984, 1986, 1988, 1990, 1992, 1994,…\n$ votes_counted          &lt;dbl&gt; 86.51522, 67.61558, 92.65268, 64.99113, 91.5946…\n$ voting_age_pop         &lt;dbl&gt; 164.4455, 166.0276, 173.9946, 177.9223, 181.955…\n$ voting_eligible_pop    &lt;dbl&gt; 159.6909, 160.4088, 167.7085, 170.4089, 173.609…\n$ ineligible_felons      &lt;dbl&gt; 0.801977, 0.959637, 1.165246, 1.367117, 1.59397…\n$ ineligible_noncitizens &lt;dbl&gt; 5.755592, 6.641105, 7.481768, 8.362350, 9.27973…\n$ eligible_overseas      &lt;dbl&gt; 1.803021, 1.981895, 2.360867, 2.216053, 2.52737…\nDouble check that the manually calculated voting eligible calculation lines up with the one reported in the data frame:\ndf_turnout |&gt;\n  mutate(\n    vep_manual = voting_age_pop - ineligible_felons -\n      ineligible_noncitizens + eligible_overseas,\n    vep_difference = abs(voting_eligible_pop - vep_manual) / voting_eligible_pop,\n  ) |&gt;\n  select(year, voting_eligible_pop, vep_manual, vep_difference) |&gt;\n  print(n = Inf)\n\n# A tibble: 22 × 4\n    year voting_eligible_pop vep_manual vep_difference\n   &lt;dbl&gt;               &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1  1980                160.       160.       2.35e- 9\n 2  1982                160.       160.       1.99e- 9\n 3  1984                168.       168.       1.37e- 9\n 4  1986                170.       170.       2.88e- 9\n 5  1988                174.       174.       1.82e- 9\n 6  1990                177.       177.       1.44e- 9\n 7  1992                180.       180.       2.12e- 9\n 8  1994                183.       183.       2.19e- 9\n 9  1996                186.       186.       1.91e- 9\n10  1998                190.       190.       1.50e- 9\n11  2000                194.       194.       2.14e- 9\n12  2002                198.       198.       4.03e-10\n13  2004                203.       203.       2.18e- 9\n14  2006                207.       207.       1.52e- 9\n15  2008                213.       213.       2.25e- 9\n16  2010                222.       222.       1.94e- 4\n17  2012                222.       222.       2.66e- 9\n18  2014                227.       227.       7.00e-10\n19  2016                231.       231.       7.06e-10\n20  2018                237.       237.       4.22e-10\n21  2020                242.       242.       6.20e-10\n22  2022                243.       243.       4.84e- 9\nSave cleaned data to turnout.csv:\nwrite_csv(df_turnout, \"data/turnout.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "data_sources.html#international-crises-crises.csv",
    "href": "data_sources.html#international-crises-crises.csv",
    "title": "Appendix B — Data Sources",
    "section": "B.2 International crises: crises.csv",
    "text": "B.2 International crises: crises.csv\nThis file contains the International Crisis Behavior actor-level data, version 16. There’s purposely no additional cleaning since it’s used in the data wrangling lecture.\nThe raw data is stored via a Box link that doesn’t work with download.file(), and my efforts to get ChatGPT to help me get to the underlying data were unsuccessful.\n\n# Read in raw data\ncrises_file &lt;- \"_raw/icb2v16.csv\"\nif (!file.exists(crises_file))\n  stop(\"Need to download data manually from ICB website\")\ndf_crises_raw &lt;- read_csv(crises_file)\n\nglimpse(df_crises_raw)\n\nRows: 1,131\nColumns: 95\n$ icb2     &lt;chr&gt; \"ICB2\", \"ICB2\", \"ICB2\", \"ICB2\", \"ICB2\", \"ICB2\", \"ICB2\", \"ICB2…\n$ crisno   &lt;dbl&gt; 1, 2, 2, 3, 4, 4, 4, 4, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9, 9, 1…\n$ cracno   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ cracid   &lt;dbl&gt; 365, 93, 94, 365, 365, 366, 368, 367, 315, 290, 310, 315, 310…\n$ actor    &lt;chr&gt; \"RUS\", \"NIC\", \"COS\", \"RUS\", \"RUS\", \"EST\", \"LIT\", \"LAT\", \"CZE\"…\n$ systrgyr &lt;dbl&gt; 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1918, 1919, 1919, 1…\n$ systrgmo &lt;dbl&gt; 5, 5, 5, 6, 11, 11, 11, 11, 1, 1, 3, 3, 3, 3, 3, 3, 4, 4, 4, …\n$ systrgda &lt;dbl&gt; NA, 25, 25, 23, 18, 18, 18, 18, 15, 15, 20, 20, 20, 20, 66, N…\n$ crisname &lt;chr&gt; \"RUSSIAN CIVIL WAR I\", \"COSTA RICAN COUP\", \"COSTA RICAN COUP\"…\n$ triggr   &lt;dbl&gt; 9, 7, 4, 7, 6, 9, 9, 9, 2, 7, 2, 9, 2, 9, 7, 7, 2, 7, 9, 9, 2…\n$ yrtrig   &lt;dbl&gt; 1918, 1918, 1919, 1918, 1918, 1918, 1918, 1918, 1919, 1919, 1…\n$ motrig   &lt;dbl&gt; 5, 5, 1, 6, 11, 11, 12, 12, 1, 1, 3, 5, 6, 7, 3, 5, 4, 5, 4, …\n$ datrig   &lt;dbl&gt; NA, 25, 25, 23, 18, 22, NA, NA, 15, 23, 20, 11, 8, 20, 66, 15…\n$ trigent  &lt;dbl&gt; 996, 94, 996, 997, 366, 365, 365, 365, 290, 315, 997, 310, 22…\n$ trigloc  &lt;dbl&gt; 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ southv   &lt;dbl&gt; 220, 94, 93, 200, 366, 365, 365, 365, 290, 315, 360, 310, 220…\n$ southpow &lt;dbl&gt; 3, 1, 1, 3, 1, 3, 3, 3, 2, 1, 1, 1, 3, 1, 3, 2, 3, 1, 1, 3, 1…\n$ sizedu   &lt;dbl&gt; 1, NA, NA, 1, 1, NA, NA, NA, NA, NA, 3, 3, 3, NA, 3, NA, NA, …\n$ strcdu   &lt;dbl&gt; 1, 1, NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, NA, 1, 1, NA, …\n$ comlev   &lt;dbl&gt; 7, 1, 1, 7, 8, 8, 8, 8, 3, 3, 1, NA, 1, NA, NA, 3, 1, 2, 3, 3…\n$ majres   &lt;dbl&gt; 8, 3, 6, 8, 8, 9, 9, 9, 8, 8, 8, 8, 1, 8, 6, 8, 8, 8, 8, 8, 8…\n$ yerres   &lt;dbl&gt; 1918, 1918, 1919, 1918, 1918, 1918, 1918, 1918, 1919, 1919, 1…\n$ monres   &lt;dbl&gt; 5, 5, 1, 7, 11, 11, 12, 12, 1, 1, 3, 5, 6, 7, 5, 5, 5, 5, 6, …\n$ dayres   &lt;dbl&gt; 28, 30, 28, 1, 22, 22, NA, NA, 23, 23, 28, 11, 16, 24, 15, NA…\n$ trgresra &lt;dbl&gt; 14, 6, 4, 9, 5, 1, NA, NA, 9, 1, 9, 1, 9, 5, 76, NA, 19, 6, 4…\n$ crismg   &lt;dbl&gt; 8, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 8, 7, 7, 7, 7, 7, 7, 7…\n$ cenvio   &lt;dbl&gt; 4, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 2, 3, 3, 3, 3, 3, 3…\n$ sevvio   &lt;dbl&gt; 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 4, 4, 1, 4, 2, 2, 4, 4, 3, 3, 3…\n$ usinv    &lt;dbl&gt; 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 1, 1, 3, 3, 1…\n$ usfavr   &lt;dbl&gt; 3, 1, 3, 3, 3, 1, 1, 1, 3, 3, 5, 5, 5, 5, 1, 3, 5, 5, NA, 3, …\n$ suinv    &lt;dbl&gt; 9, 1, 1, 9, 9, 8, 8, 8, 1, 1, 3, 3, 1, 3, 1, 1, 5, 5, 9, 8, 9…\n$ sufavr   &lt;dbl&gt; 8, 5, 5, 8, 8, 3, 3, 3, 5, 5, 1, 3, 5, 3, 5, 5, 2, 2, 8, 3, 8…\n$ gbinv    &lt;dbl&gt; 7, 1, 1, 8, 3, 6, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 8, 9, 7, 7, 1…\n$ gbfavr   &lt;dbl&gt; NA, 5, 5, NA, NA, NA, NA, NA, 3, 3, NA, NA, NA, NA, 1, NA, 3,…\n$ frinv    &lt;dbl&gt; 2, 1, 1, 8, 3, 1, 3, 3, 3, 3, 8, 8, 3, 1, 3, 3, 1, 1, 3, 3, 1…\n$ frfavr   &lt;dbl&gt; NA, 5, 5, NA, NA, NA, NA, NA, 3, 3, NA, NA, NA, NA, 1, NA, 5,…\n$ itinv    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 7, 7, 1, 1, 8, 9, 1, 1, 1, 1, 1…\n$ itfavr   &lt;dbl&gt; NA, 5, 5, NA, NA, NA, NA, NA, 3, 3, NA, NA, NA, NA, 3, NA, 5,…\n$ grinv    &lt;dbl&gt; 1, 1, 1, 1, 8, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1…\n$ grfavr   &lt;dbl&gt; NA, 5, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 5, NA, …\n$ jpinv    &lt;dbl&gt; 7, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ jpfavr   &lt;dbl&gt; NA, 5, 5, NA, NA, NA, NA, NA, 3, 3, NA, NA, NA, NA, 5, NA, 5,…\n$ globorg  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2…\n$ globact  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2…\n$ globfavr &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, NA, …\n$ regorg   &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ regact   &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ rofavr   &lt;dbl&gt; 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ outcom   &lt;dbl&gt; 1, 1, 4, 1, 4, 1, 1, 1, 2, 2, 4, 1, 4, 1, 2, 2, 2, 2, 2, 2, 4…\n$ outfor   &lt;dbl&gt; 6, 4, 4, 6, 9, 8, 8, 8, 9, 9, 6, 1, 7, 4, 1, 1, 1, 1, 1, 1, 1…\n$ outevl   &lt;dbl&gt; 2, 2, 3, 2, 3, 2, 2, 2, 4, 4, 3, 2, 3, 2, 1, 1, 1, 1, 1, 3, 3…\n$ outesr   &lt;dbl&gt; 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2…\n$ yrterm   &lt;dbl&gt; 1920, 1918, 1919, 1919, 1920, 1920, 1920, 1920, 1920, 1920, 1…\n$ moterm   &lt;dbl&gt; 4, 12, 9, 9, 8, 2, 7, 8, 7, 7, 8, 6, 6, 8, 7, 7, 8, 8, 10, 10…\n$ daterm   &lt;dbl&gt; 1, 15, 3, 27, 11, 2, 12, 11, 28, 28, 3, 24, 16, 3, 29, 29, 8,…\n$ trgterra &lt;dbl&gt; 686, 205, 222, 462, 632, 438, 574, 603, 560, 552, 137, 45, 9,…\n$ resterra &lt;dbl&gt; 673, 199, 218, 453, 627, 438, 574, 603, 551, 552, 128, 45, 1,…\n$ actloc   &lt;dbl&gt; 30, 42, 42, 30, 30, 34, 34, 34, 31, 31, 31, 31, 31, 31, 35, 3…\n$ geog     &lt;dbl&gt; 30, 42, 42, 30, 34, 34, 34, 34, 31, 31, 31, 31, 31, 31, 10, 1…\n$ cractloc &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 3, 2, 4, 1, 1, 1, 1, 1…\n$ noactr   &lt;dbl&gt; 7, 5, 6, 5, 8, 8, 8, 8, 3, 3, 7, 7, 3, 7, 5, 5, 3, 3, 5, 6, 3…\n$ stainsys &lt;dbl&gt; 47, 47, 49, 47, 47, 47, 47, 47, 49, 49, 49, 49, 49, 49, 49, 4…\n$ period   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ syslev   &lt;dbl&gt; 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ pc       &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1…\n$ pcid     &lt;dbl&gt; 27, 6, 6, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 1…\n$ viol     &lt;dbl&gt; 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 1, 4, 2, 2, 4, 4, 3, 3, 3…\n$ iwc      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 3, 7, 1, 1, 1, 1, 1, 1, 1…\n$ powdis   &lt;dbl&gt; NA, 1, -1, NA, 12, -12, -12, -12, -1, 1, NA, NA, NA, NA, -1, …\n$ gpinv    &lt;dbl&gt; 7, 4, 4, 7, 7, 7, 7, 7, 3, 3, 5, 4, 4, 4, 6, 6, 6, 6, 7, 7, 7…\n$ powinv   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ age      &lt;dbl&gt; 1, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 1, 3, 1…\n$ territ   &lt;dbl&gt; 3, 1, 1, 3, 3, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 3, 2, 3…\n$ regime   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 1, 4, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2…\n$ durreg   &lt;dbl&gt; 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 1, 1…\n$ allycap  &lt;dbl&gt; 4, 2, 1, 4, 4, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 4, 4, 2, 4…\n$ globmemb &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4…\n$ nuclear  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ powsta   &lt;dbl&gt; 3, 1, 1, 3, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 3, 1, 3, 3, 1, 3…\n$ issue    &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1…\n$ chissu   &lt;dbl&gt; 4, 6, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6…\n$ gravty   &lt;dbl&gt; 2, 1, 2, 2, 3, 6, 6, 6, 3, 3, 6, 3, 5, 3, 3, 4, 2, 4, 3, 3, 3…\n$ pethin   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ col      &lt;dbl&gt; 1, NA, NA, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, NA, NA, 1, NA, 1, 1,…\n$ unemp    &lt;dbl&gt; NA, NA, NA, NA, NA, 4, 4, 4, 4, 4, 4, 4, 4, NA, NA, NA, NA, 3…\n$ inflat   &lt;dbl&gt; 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, NA, NA, 1, NA, NA, 1, …\n$ foodpr   &lt;dbl&gt; 1, NA, NA, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, NA, NA, 1, NA, 1, 1,…\n$ labstr   &lt;dbl&gt; 1, NA, NA, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, NA, NA, 1, NA, 1, 1,…\n$ short    &lt;dbl&gt; 1, NA, NA, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, NA, NA, 2, NA, 1, 1,…\n$ econdt   &lt;dbl&gt; 1, NA, NA, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, NA, NA, 1, NA, 1, 1,…\n$ regrep   &lt;dbl&gt; NA, 1, 1, NA, NA, 4, 4, 4, 4, 4, 4, 4, 4, NA, NA, NA, 1, 1, 1…\n$ socunr   &lt;dbl&gt; 1, NA, NA, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, NA, NA, NA, 1, 1, 1,…\n$ massvl   &lt;dbl&gt; 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, NA, NA, 2, NA, 2, 1, 1…\n$ gvinst   &lt;dbl&gt; 1, 2, NA, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, NA, NA, NA, NA, 2, 1,…\n$ sourdt   &lt;dbl&gt; 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, NA, 3, 1, 1, 1, 1, …\n\n\nSave to crises.csv:\n\nwrite_csv(df_crises_raw, \"data/crises.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "data_sources.html#military-spending-and-personnel-military.csv",
    "href": "data_sources.html#military-spending-and-personnel-military.csv",
    "title": "Appendix B — Data Sources",
    "section": "B.3 Military spending and personnel: military.csv",
    "text": "B.3 Military spending and personnel: military.csv\nThis file contains data from the Correlates of War project’s dataset on National Material Capabilities, version 6.0.\nObtain the raw data by extracting from the zip on the COW website:\n\n# Download zip file containing raw data\n#\n# This is convoluted because the csv is inside a zip within the zip\nmilitary_url &lt;- \"https://correlatesofwar.org/wp-content/uploads/NMC_Documentation-6.0.zip\"\nmilitary_file &lt;- \"_raw/NMC-60-abridged.csv\"\nif (!file.exists(military_file)) {\n  military_zip_outer &lt;- tempfile(fileext = \".zip\")\n  download.file(url = military_url, destfile = military_zip_outer)\n  military_zip_inner &lt;- archive_read(military_zip_outer, \"NMC-60-abridged.zip\")\n  military_csv &lt;- read_csv(archive_read(military_zip_inner, \"NMC-60-abridged.csv\"))\n  write_csv(military_csv, military_file)\n}\n\n# Read in raw data\ndf_military_raw &lt;- read_csv(military_file)\n\nglimpse(df_military_raw)\n\nRows: 15,951\nColumns: 11\n$ stateabb &lt;chr&gt; \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\"…\n$ ccode    &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ year     &lt;dbl&gt; 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1…\n$ milex    &lt;dbl&gt; 3823, 2466, 1910, 2301, 1556, 1612, 1079, 1170, 1261, 1336, 1…\n$ milper   &lt;dbl&gt; 17, 15, 14, 13, 15, 11, 10, 11, 11, 11, 12, 12, 11, 12, 12, 1…\n$ irst     &lt;dbl&gt; 80, 80, 90, 90, 110, 100, 100, 110, 110, 120, 120, 130, 130, …\n$ pec      &lt;dbl&gt; 254, 277, 302, 293, 303, 321, 332, 345, 390, 424, 502, 556, 6…\n$ tpop     &lt;dbl&gt; 8659, 8899, 9139, 9379, 9618, 9939, 10268, 10596, 10924, 1125…\n$ upop     &lt;dbl&gt; 101, 106, 112, 118, 124, 130, 136, 143, 151, 158, 166, 175, 1…\n$ cinc     &lt;dbl&gt; 0.03969749, 0.03581661, 0.03612655, 0.03713325, 0.03708687, 0…\n$ version  &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2…\n\n\nConvert to “long” format containing only spending and personnel, for pedagogical purposes:\n\ndf_military &lt;- df_military_raw |&gt;\n  select(ccode, stateabb, year, spending = milex, personnel = milper) |&gt;\n  pivot_longer(\n    cols = c(spending, personnel),\n    names_to = \"mil_indicator\",\n    values_to = \"amount\"\n  )\n\nglimpse(df_military)\n\nRows: 31,902\nColumns: 5\n$ ccode         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ stateabb      &lt;chr&gt; \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", …\n$ year          &lt;dbl&gt; 1816, 1816, 1817, 1817, 1818, 1818, 1819, 1819, 1820, 18…\n$ mil_indicator &lt;chr&gt; \"spending\", \"personnel\", \"spending\", \"personnel\", \"spend…\n$ amount        &lt;dbl&gt; 3823, 17, 2466, 15, 1910, 14, 2301, 13, 1556, 15, 1612, …\n\n\nSave to military.csv:\n\nwrite_csv(df_military, \"data/military.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "data_sources.html#county-level-presidential-election-returns-20002024-county_pres.csv",
    "href": "data_sources.html#county-level-presidential-election-returns-20002024-county_pres.csv",
    "title": "Appendix B — Data Sources",
    "section": "B.4 County-level presidential election returns, 2000–2024: county_pres.csv",
    "text": "B.4 County-level presidential election returns, 2000–2024: county_pres.csv\nThis file contains data from the MIT Election Lab dataset on County Presidential Election returns.\nObtain raw data from Harvard Dataverse:\n\n# Download raw data from Harvard Dataverse repository\ncounty_pres_file &lt;- \"_raw/countypres_2000-2024.csv\"\nif (!file.exists(county_pres_file)) {\n  df_county_pres_dataverse &lt;- get_dataframe_by_name(\n    filename = \"countypres_2000-2024.tab\",\n    dataset = \"10.7910/DVN/VOQCHQ\",\n    server = \"dataverse.harvard.edu\",\n    version = \"15.0\"\n  )\n  write_csv(df_county_pres_dataverse, county_pres_file)\n}\n\ndf_county_pres_raw &lt;- read_csv(county_pres_file)\n\nglimpse(df_county_pres_raw)\n\nRows: 94,409\nColumns: 12\n$ year           &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2…\n$ state          &lt;chr&gt; \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", \"ALABAMA\", …\n$ state_po       &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"…\n$ county_name    &lt;chr&gt; \"AUTAUGA\", \"AUTAUGA\", \"AUTAUGA\", \"AUTAUGA\", \"BALDWIN\", …\n$ county_fips    &lt;chr&gt; \"01001\", \"01001\", \"01001\", \"01001\", \"01003\", \"01003\", \"…\n$ office         &lt;chr&gt; \"US PRESIDENT\", \"US PRESIDENT\", \"US PRESIDENT\", \"US PRE…\n$ candidate      &lt;chr&gt; \"AL GORE\", \"GEORGE W. BUSH\", \"OTHER\", \"RALPH NADER\", \"A…\n$ party          &lt;chr&gt; \"DEMOCRAT\", \"REPUBLICAN\", \"OTHER\", \"GREEN\", \"DEMOCRAT\",…\n$ candidatevotes &lt;dbl&gt; 4942, 11993, 113, 160, 13997, 40872, 578, 1033, 5188, 5…\n$ totalvotes     &lt;dbl&gt; 17208, 17208, 17208, 17208, 56480, 56480, 56480, 56480,…\n$ version        &lt;dbl&gt; 20250712, 20250712, 20250712, 20250712, 20250712, 20250…\n$ mode           &lt;chr&gt; \"TOTAL\", \"TOTAL\", \"TOTAL\", \"TOTAL\", \"TOTAL\", \"TOTAL\", \"…\n\n\nClean data to have one row per county-year:\n\ndf_county_pres &lt;- df_county_pres_raw |&gt;\n  filter(!is.na(party), totalvotes &gt; 0) |&gt;\n  rename(county = county_name) |&gt;\n  group_by(year, state, county) |&gt;\n  summarize(\n    county_fips = first(county_fips),\n    total_votes = first(totalvotes),\n    dem_votes = sum(candidatevotes[party == \"DEMOCRAT\"]),\n    rep_votes = sum(candidatevotes[party == \"REPUBLICAN\"]),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    region = fct_collapse(\n      state,\n      Northeast = c(\n        \"CONNECTICUT\", \"MAINE\", \"MASSACHUSETTS\", \"NEW HAMPSHIRE\", \"RHODE ISLAND\",\n        \"VERMONT\", \"NEW JERSEY\", \"NEW YORK\", \"PENNSYLVANIA\"\n      ),\n      Midwest = c(\n        \"ILLINOIS\", \"INDIANA\", \"MICHIGAN\", \"OHIO\", \"WISCONSIN\", \"IOWA\", \"KANSAS\",\n        \"MINNESOTA\", \"MISSOURI\", \"NEBRASKA\", \"NORTH DAKOTA\", \"SOUTH DAKOTA\"\n      ),\n      South = c(\n        \"DELAWARE\", \"DISTRICT OF COLUMBIA\", \"FLORIDA\", \"GEORGIA\", \"MARYLAND\",\n        \"NORTH CAROLINA\", \"SOUTH CAROLINA\", \"VIRGINIA\", \"WEST VIRGINIA\", \"ALABAMA\",\n        \"KENTUCKY\", \"MISSISSIPPI\", \"TENNESSEE\", \"ARKANSAS\", \"LOUISIANA\", \"OKLAHOMA\",\n        \"TEXAS\"\n      ),\n      West = c(\n        \"ARIZONA\", \"COLORADO\", \"IDAHO\", \"MONTANA\", \"NEVADA\", \"NEW MEXICO\", \"UTAH\",\n        \"WYOMING\", \"ALASKA\", \"CALIFORNIA\", \"HAWAII\", \"OREGON\", \"WASHINGTON\"\n      ),\n      other_level = \"Unknown\"\n    ),\n    margin = dem_votes - rep_votes,\n    pct_margin = margin / total_votes,\n    competitiveness = case_when(\n      pct_margin &lt; -0.2 ~ -3,\n      pct_margin &lt; -0.1 ~ -2,\n      pct_margin &lt; -0.04 ~ -1,\n      pct_margin &lt; 0.04 ~ 0,\n      pct_margin &lt; 0.1 ~ 1,\n      pct_margin &lt; 0.2 ~ 2,\n      TRUE ~ 3\n    ),\n  ) |&gt;\n  group_by(state) |&gt;\n  mutate(\n    dem_win_state = as.numeric(sum(dem_votes) &gt; sum(rep_votes)),\n  ) |&gt;\n  ungroup() |&gt;\n  assert(not_na, everything()) |&gt;\n  select(year, state, region, everything())\n\nSave to county_pres.csv:\n\nwrite_csv(df_county_pres, \"data/county_pres.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "data_sources.html#world-development-indicators-2019-wdi.csv",
    "href": "data_sources.html#world-development-indicators-2019-wdi.csv",
    "title": "Appendix B — Data Sources",
    "section": "B.5 World Development Indicators, 2019: wdi.csv",
    "text": "B.5 World Development Indicators, 2019: wdi.csv\nThis file contains data from the World Bank’s World Development Indicators dataset.\nObtain raw data using the WDI package:\n\n# Download raw data via WDI package\nwdi_file &lt;- \"_raw/wdi_2019.csv\"\nif (!file.exists(wdi_file)) {\n  df_wdi_pkg &lt;- WDI(\n    country = \"all\",\n    indicator = c(\n      \"gdp_per_capita\" = \"NY.GDP.PCAP.CD\",\n      \"gdp_growth\" = \"NY.GDP.MKTP.KD.ZG\",\n      \"population\" = \"SP.POP.TOTL\",\n      \"inflation\" = \"FP.CPI.TOTL.ZG\",\n      \"unemployment\" = \"SL.UEM.TOTL.ZS\",\n      \"life_expectancy\" = \"SP.DYN.LE00.IN\"\n    ),\n    start = 2019, end = 2019,\n    extra = TRUE\n  )\n  write_csv(df_wdi_pkg, wdi_file)\n}\n\ndf_wdi_raw &lt;- read_csv(wdi_file)\n\nglimpse(df_wdi_raw)\n\nRows: 266\nColumns: 18\n$ country         &lt;chr&gt; \"Afghanistan\", \"Africa Eastern and Southern\", \"Africa …\n$ iso2c           &lt;chr&gt; \"AF\", \"ZH\", \"ZI\", \"AL\", \"DZ\", \"AS\", \"AD\", \"AO\", \"AG\", …\n$ iso3c           &lt;chr&gt; \"AFG\", \"AFE\", \"AFW\", \"ALB\", \"DZA\", \"ASM\", \"AND\", \"AGO\"…\n$ year            &lt;dbl&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, …\n$ status          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ lastupdated     &lt;date&gt; 2025-07-01, 2025-07-01, 2025-07-01, 2025-07-01, 2025-…\n$ gdp_per_capita  &lt;dbl&gt; 496.6025, 1493.8179, 1798.3407, 5460.4305, 4468.4534, …\n$ gdp_growth      &lt;dbl&gt; 3.9116034, 2.2003404, 3.2821630, 2.0625779, 0.9000000,…\n$ population      &lt;dbl&gt; 37856121, 675950189, 463365429, 2854191, 43294546, 502…\n$ inflation       &lt;dbl&gt; 2.3023725, 4.6449672, 1.9830923, 1.4110908, 1.9517682,…\n$ unemployment    &lt;dbl&gt; 11.185000, 7.584419, 4.395271, 11.466000, 12.259000, N…\n$ life_expectancy &lt;dbl&gt; 62.94100, 63.85726, 57.14985, 79.46700, 75.68200, 72.7…\n$ region          &lt;chr&gt; \"South Asia\", \"Aggregates\", \"Aggregates\", \"Europe & Ce…\n$ capital         &lt;chr&gt; \"Kabul\", NA, NA, \"Tirane\", \"Algiers\", \"Pago Pago\", \"An…\n$ longitude       &lt;dbl&gt; 69.17610, NA, NA, 19.81720, 3.05097, -170.69100, 1.521…\n$ latitude        &lt;dbl&gt; 34.52280, NA, NA, 41.33170, 36.73970, -14.28460, 42.50…\n$ income          &lt;chr&gt; \"Low income\", \"Aggregates\", \"Aggregates\", \"Upper middl…\n$ lending         &lt;chr&gt; \"IDA\", \"Aggregates\", \"Aggregates\", \"IBRD\", \"IBRD\", \"No…\n\n\nMinor cleaning to remove unwanted rows and columns:\n\ndf_wdi &lt;- df_wdi_raw |&gt;\n  as_tibble() |&gt;\n  select(-iso2c, -status, -lastupdated, -capital, -longitude, -latitude) |&gt;\n  filter(region != \"Aggregates\") |&gt;\n  filter(income != \"Not classified\") |&gt;\n  mutate(income = case_match(\n    income,\n    \"Low income\" ~ \"1. Low\",\n    \"Lower middle income\" ~ \"2. Lower-middle\",\n    \"Upper middle income\" ~ \"3. Upper-middle\",\n    \"High income\" ~ \"4. High\"\n  ))\n\n\nglimpse(df_wdi)\n\nRows: 215\nColumns: 12\n$ country         &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"American Samoa\",…\n$ iso3c           &lt;chr&gt; \"AFG\", \"ALB\", \"DZA\", \"ASM\", \"AND\", \"AGO\", \"ATG\", \"ARG\"…\n$ year            &lt;dbl&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, …\n$ gdp_per_capita  &lt;dbl&gt; 496.6025, 5460.4305, 4468.4534, 12886.1360, 41257.8046…\n$ gdp_growth      &lt;dbl&gt; 3.9116034, 2.0625779, 0.9000000, -0.4878049, 2.0155476…\n$ population      &lt;dbl&gt; 37856121, 2854191, 43294546, 50209, 76474, 32375632, 9…\n$ inflation       &lt;dbl&gt; 2.3023725, 1.4110908, 1.9517682, NA, NA, 17.0809541, 1…\n$ unemployment    &lt;dbl&gt; 11.185, 11.466, 12.259, NA, NA, 16.497, NA, 9.843, 18.…\n$ life_expectancy &lt;dbl&gt; 62.94100, 79.46700, 75.68200, 72.75100, 84.09800, 63.0…\n$ region          &lt;chr&gt; \"South Asia\", \"Europe & Central Asia\", \"Middle East & …\n$ income          &lt;chr&gt; \"1. Low\", \"3. Upper-middle\", \"3. Upper-middle\", \"4. Hi…\n$ lending         &lt;chr&gt; \"IDA\", \"IBRD\", \"IBRD\", \"Not classified\", \"Not classifi…\n\n\nSave to wdi.csv:\n\nwrite_csv(df_wdi, \"data/wdi.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "data_sources.html#subset-of-2020-anes-survey-anes_2020.csv",
    "href": "data_sources.html#subset-of-2020-anes-survey-anes_2020.csv",
    "title": "Appendix B — Data Sources",
    "section": "B.6 Subset of 2020 ANES survey: anes_2020.csv",
    "text": "B.6 Subset of 2020 ANES survey: anes_2020.csv\nThis file contains a small number of variables from the 2020 wave of the American National Election Studies Time Series Study.\nDespite the direct download link on ANES’s website, they’ve got it locked down to prevent programmatic access from utilities like download.file(). (Not running glimpse() this time because the data has 1700+ columns.)\n\nanes_file &lt;- \"_raw/anes_timeseries_2020_csv_20220210.csv\"\nif (!file.exists(anes_file))\n  stop(\"Need to download data manually from ANES website\")\n\ndf_anes_raw &lt;- read_csv(anes_file)\n\nExtract the few columns we care about and convert numeric codes to understandable values:\n\ndf_anes &lt;- df_anes_raw |&gt;\n  mutate(\n    id = row_number(),\n    state = case_match(\n      V201014b,\n      1 ~ \"Alabama\",\n      2 ~ \"Alaska\",\n      4 ~ \"Arizona\",\n      5 ~ \"Arkansas\",\n      6 ~ \"California\",\n      8 ~ \"Colorado\",\n      9 ~ \"Connecticut\",\n      10 ~ \"Delaware\",\n      11 ~ \"District of Columbia\",\n      12 ~ \"Florida\",\n      13 ~ \"Georgia\",\n      15 ~ \"Hawaii\",\n      16 ~ \"Idaho\",\n      17 ~ \"Illinois\",\n      18 ~ \"Indiana\",\n      19 ~ \"Iowa\",\n      20 ~ \"Kansas\",\n      21 ~ \"Kentucky\",\n      22 ~ \"Louisiana\",\n      23 ~ \"Maine\",\n      24 ~ \"Maryland\",\n      25 ~ \"Massachusetts\",\n      26 ~ \"Michigan\",\n      27 ~ \"Minnesota\",\n      28 ~ \"Mississippi\",\n      29 ~ \"Missouri\",\n      30 ~ \"Montana\",\n      31 ~ \"Nebraska\",\n      32 ~ \"Nevada\",\n      33 ~ \"New Hampshire\",\n      34 ~ \"New Jersey\",\n      35 ~ \"New Mexico\",\n      36 ~ \"New York\",\n      37 ~ \"North Carolina\",\n      38 ~ \"North Dakota\",\n      39 ~ \"Ohio\",\n      40 ~ \"Oklahoma\",\n      41 ~ \"Oregon\",\n      42 ~ \"Pennsylvania\",\n      44 ~ \"Rhode Island\",\n      45 ~ \"South Carolina\",\n      46 ~ \"South Dakota\",\n      47 ~ \"Tennessee\",\n      48 ~ \"Texas\",\n      49 ~ \"Utah\",\n      50 ~ \"Vermont\",\n      51 ~ \"Virginia\",\n      53 ~ \"Washington\",\n      54 ~ \"West Virginia\",\n      55 ~ \"Wisconsin\",\n      56 ~ \"Wyoming\"\n    ),\n    female = case_match(\n      V201600,\n      1 ~ 0,\n      2 ~ 1\n    ),\n    lgbt = case_match(\n      V201601,\n      1 ~ 0,\n      2:4 ~ 1\n    ),\n    race = case_match(\n      V201549x,\n      1 ~ \"White\",\n      2 ~ \"Black\",\n      3 ~ \"Hispanic\",\n      4 ~ \"Asian\",\n      5 ~ \"Native American\",\n      6 ~ \"Multiracial\"\n    ),\n    age = if_else(V201507x &gt; 0, V201507x, NA),\n    education = case_match(\n      V201511x,\n      1 ~ \"Less than high school\",\n      2 ~ \"High school\",\n      3 ~ \"Some college\",\n      4 ~ \"Bachelor's degree\",\n      5 ~ \"Graduate degree\"\n    ),\n    employed = case_match(\n      V201517,\n      1 ~ 1,\n      2 ~ 0\n    ),\n    hours_worked = case_when(\n      V201527 == -1 ~ 0,\n      V201527 &gt; 0 ~ V201527,\n      TRUE ~ NA\n    ),\n    watch_tucker = case_match(\n      V201630c,\n      c(-1, 0) ~ 0,\n      1 ~ 1\n    ),\n    watch_maddow = case_match(\n      V201630d,\n      c(-1, 0) ~ 0,\n      1 ~ 1\n    ),\n    therm_biden = if_else(V201151 %in% 0:100, V201151, NA),\n    therm_trump = if_else(V201152 %in% 0:100, V201152, NA),\n    therm_harris = if_else(V201153 %in% 0:100, V201153, NA),\n    therm_pence = if_else(V201154 %in% 0:100, V201154, NA),\n    therm_obama = if_else(V201155 %in% 0:100, V201155, NA),\n    therm_dem_party = if_else(V201156 %in% 0:100, V201156, NA),\n    therm_rep_party = if_else(V201157 %in% 0:100, V201157, NA),\n    therm_feminists = if_else(V202160 %in% 0:100, V202160, NA),\n    therm_liberals = if_else(V202161 %in% 0:100, V202161, NA),\n    therm_labor_unions = if_else(V202162 %in% 0:100, V202162, NA),\n    therm_big_business = if_else(V202163 %in% 0:100, V202163, NA),\n    therm_conservatives = if_else(V202164 %in% 0:100, V202164, NA),\n    therm_supreme_court = if_else(V202165 %in% 0:100, V202165, NA),\n    therm_congress = if_else(V202167 %in% 0:100, V202167, NA),\n    therm_police = if_else(V202171 %in% 0:100, V202171, NA),\n    therm_scientists = if_else(V202173 %in% 0:100, V202173, NA),\n    contributed_to_party = case_match(\n      V202019,\n      1 ~ 1,\n      2 ~ 0\n    ),\n    voted = case_match(\n      V202068x,\n      0:1 ~ 0,\n      2 ~ 1\n    ),\n    voted_for_biden = if_else(V202073 &lt; 0, NA, V202073),\n    voted_for_biden = case_match(\n      voted_for_biden,\n      1 ~ 1,\n      2:8 ~ 0\n    ),\n    voted_for_trump = if_else(V202073 &lt; 0, NA, V202073),\n    voted_for_trump = case_match(\n      voted_for_trump,\n      2 ~ 1,\n      c(1, 3:8) ~ 0\n    ),\n    .keep = \"none\"\n  )\n\nglimpse(df_anes)\n\nRows: 8,280\nColumns: 31\n$ id                   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ state                &lt;chr&gt; \"Oklahoma\", \"Idaho\", \"Virginia\", \"California\", \"C…\n$ female               &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0…\n$ lgbt                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ race                 &lt;chr&gt; \"Hispanic\", \"Asian\", \"White\", \"Asian\", \"Native Am…\n$ age                  &lt;dbl&gt; 46, 37, 40, 41, 72, 71, 37, 45, 70, 43, 37, 55, 3…\n$ education            &lt;chr&gt; \"Bachelor's degree\", \"Some college\", \"High school…\n$ employed             &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1…\n$ hours_worked         &lt;dbl&gt; 40, 40, 0, 40, 0, 0, 30, 40, 0, 30, 25, 50, 0, 50…\n$ watch_tucker         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ watch_maddow         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ therm_biden          &lt;dbl&gt; 0, 0, 65, 70, 15, 85, 50, 50, 85, 85, 100, 60, 0,…\n$ therm_trump          &lt;dbl&gt; 100, 0, 0, 15, 85, 0, 75, 100, 0, 0, 0, 0, 100, 0…\n$ therm_harris         &lt;dbl&gt; 0, 0, 65, 85, 15, 85, 15, 50, 85, 50, 100, 85, 0,…\n$ therm_pence          &lt;dbl&gt; 85, 0, 0, 15, 90, 0, 75, 50, 0, 50, 0, 50, 0, 0, …\n$ therm_obama          &lt;dbl&gt; 0, 50, 90, 85, 10, 60, 15, 50, 60, 100, 100, 85, …\n$ therm_dem_party      &lt;dbl&gt; 0, 0, 60, 50, 20, 85, 15, 50, NA, 60, 100, 50, 0,…\n$ therm_rep_party      &lt;dbl&gt; 85, 50, 0, 70, 70, 15, 75, 100, NA, 50, 0, 30, 85…\n$ therm_feminists      &lt;dbl&gt; 65, 100, 75, 70, 30, 60, 60, 100, 50, 50, 0, 70, …\n$ therm_liberals       &lt;dbl&gt; 30, 0, 75, 70, 10, 70, 0, NA, 30, 50, 50, 70, 0, …\n$ therm_labor_unions   &lt;dbl&gt; 30, 70, 75, 70, 50, 50, 50, 0, 30, 50, 50, 50, 50…\n$ therm_big_business   &lt;dbl&gt; 70, 50, 0, 85, 0, 40, 50, 0, 50, 15, 50, 60, 0, 0…\n$ therm_conservatives  &lt;dbl&gt; 85, 15, 0, 70, 60, 40, 60, NA, 50, 50, 50, 40, 85…\n$ therm_supreme_court  &lt;dbl&gt; 100, 50, 25, 85, 60, 60, 70, 50, 50, 50, 40, 50, …\n$ therm_congress       &lt;dbl&gt; 40, 15, 0, 100, 10, 85, 50, 50, 50, 40, 50, 50, 0…\n$ therm_police         &lt;dbl&gt; 85, 90, 40, 100, 70, 70, 60, 100, 60, 70, 50, 70,…\n$ therm_scientists     &lt;dbl&gt; 100, 70, 100, 85, 60, 85, 85, NA, 60, 50, 85, 100…\n$ contributed_to_party &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ voted                &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1…\n$ voted_for_biden      &lt;dbl&gt; NA, 0, 1, 1, 0, 1, 0, NA, NA, 1, 1, 1, 0, NA, NA,…\n$ voted_for_trump      &lt;dbl&gt; NA, 0, 0, 0, 1, 0, 1, NA, NA, 0, 0, 0, 1, NA, NA,…\n\n\nSave to anes_2020.csv:\n\nwrite_csv(df_anes, \"data/anes_2020.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "data_sources.html#federalist-papers-corpus-fed_papers.csv",
    "href": "data_sources.html#federalist-papers-corpus-fed_papers.csv",
    "title": "Appendix B — Data Sources",
    "section": "B.7 Federalist Papers corpus: fed_papers.csv",
    "text": "B.7 Federalist Papers corpus: fed_papers.csv\nThis file contains the full text of each of the Federalist Papers, per the public domain text archived by Project Gutenberg.\nObtain the raw text from Project Gutenberg:\n\nfed_papers_url &lt;- \"https://www.gutenberg.org/files/18/18-0.txt\"\nfed_papers_file &lt;- \"_raw/fed_papers.txt\"\n\nif (!file.exists(fed_papers_file))\n  download.file(url = fed_papers_url, destfile = fed_papers_file)\n\nfed_papers_raw &lt;- readLines(fed_papers_file)\n\nParse text and assemble into data frame:\n\n## Eliminate table of contents and other non-text content\nfed_papers &lt;- fed_papers_raw |&gt;\n  tail(-98) |&gt;\n  head(-2)\n\n## Combine into single string\nfed_papers &lt;- str_c(fed_papers, collapse = \"\\n\")\n\n## Split into individual papers\nfed_papers &lt;- fed_papers |&gt;\n  str_split(\"THE FEDERALIST.\\n\") |&gt;\n  unlist()\n\n## Eliminate the empty first entry, as well as the duplicate of #70\nfed_papers &lt;- fed_papers[-1]\nfed_papers &lt;- fed_papers[-70]\n\n## Extract author(s) of each paper\nauthor_id_regex &lt;- \"\\\\n\\\\n(HAMILTON|JAY|MADISON|HAMILTON AND MADISON|HAMILTON OR MADISON)\\\\n\\\\n\\\\n\"\npaper_author &lt;- fed_papers |&gt;\n  str_extract(author_id_regex) |&gt;\n  str_remove_all(\"\\\\n\") |&gt;\n  str_to_lower()\n\n## Start each paper text after author identifier\n##\n## This will keep our classifiers from \"peeking\" by directly using author info\npaper_text &lt;- fed_papers |&gt;\n  str_split_i(author_id_regex, i = 2)\n\n## Combine into a data frame\ndf_fed_papers &lt;- tibble(\n  paper_id = seq_along(fed_papers),\n  author = paper_author,\n  text = paper_text\n)\n\nglimpse(df_fed_papers)\n\nRows: 85\nColumns: 3\n$ paper_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ author   &lt;chr&gt; \"hamilton\", \"jay\", \"jay\", \"jay\", \"jay\", \"hamilton\", \"hamilton…\n$ text     &lt;chr&gt; \"To the People of the State of New York:\\n\\nAfter an unequivo…\n\n\nSave to fed_papers.csv:\n\nwrite_csv(df_fed_papers, \"data/fed_papers.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  }
]